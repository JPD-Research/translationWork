{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "15ad6f5e961949f5",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Text translation using transformer architectures\n",
    " \n",
    "Based on Tensorflow tutorial [Neural machine translation with a Transformer and Keras](https://www.tensorflow.org/text/tutorials/transformer)\n",
    "\n",
    "Training a sequence-to-sequence (seq2seq) model similar to from [Effective Approaches to Attention-based Neural Machine Translation](https://arxiv.org/abs/1508.04025v5)\n",
    "\n",
    "- not the most current architecture, but a good learning step for seq2seq models and attention before moving to Transformers\n",
    "- will allow direct Spanish-English translation\n",
    "- exports model as a tf.saved_model for use in other environments\n",
    "- attention plot is more interesting than the resulting output - shows what parts of the input sentence \"has the model's attention\" while translating\n",
    "\n",
    "With modifications described along the way...this does not currently leverage any available GPU, but will by the time we're done...\n",
    "\n",
    "## Step 0 - Environment Setup \n",
    "\n",
    "assumes tensorflow-text >= 2.10, matplotlib and einops are installed in the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5ac1f76023a6f70",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
