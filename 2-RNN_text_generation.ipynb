{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JPD-Research/translationWork/blob/master/2-RNN_text_generation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2113b7af313f668e",
      "metadata": {
        "id": "2113b7af313f668e"
      },
      "source": [
        "# Text Generation using Recurrent Neural Networks\n",
        "\n",
        "Based on Tensorflow tutorial [Text generation with an RNN](https://www.tensorflow.org/text/tutorials/text_generation)\n",
        "\n",
        "Text generation using Shakespeare dataset from [The Unreasonable Effectiveness of Recurrent Neural Networks](http://karpathy.github.io/2015/05/21/rnn-effectiveness/)\n",
        "\n",
        "With modifications described along the way...\n",
        "this runs on the GPU if available - about 55 min to train on CPU - <2 min on GPU\n",
        "\n",
        "## Step 0 - Environment Setup\n",
        "\n",
        "conda yaml for the environment compatible with all of these notebooks is included in the root of this repo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "bb007a00562a6500",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-12-27T19:21:18.687105800Z",
          "start_time": "2023-12-27T19:21:13.398239400Z"
        },
        "id": "bb007a00562a6500"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "import numpy as np\n",
        "import os\n",
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.15.0\n",
            "1 Physical GPUs, 1 Logical GPUs\n"
          ]
        }
      ],
      "source": [
        "print(tf.__version__)\n",
        "\n",
        "gpus = tf.config.list_physical_devices('GPU')\n",
        "logical_gpus = []\n",
        "if gpus:\n",
        "    try:\n",
        "        for gpu in gpus:\n",
        "            tf.config.experimental.set_memory_growth(gpu, True)\n",
        "        logical_gpus = tf.config.list_logical_devices('GPU')\n",
        "    except RuntimeError as e:\n",
        "        # Virtual devices must be set before GPUs have been initialized\n",
        "        print(e)\n",
        "\n",
        "print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n"
      ],
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-12-27T19:21:19.554910800Z",
          "start_time": "2023-12-27T19:21:19.536903400Z"
        },
        "id": "69a203f23b1bdf8d",
        "outputId": "cce1b709-db78-47c4-dc33-db0a93e48926",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "69a203f23b1bdf8d"
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "fa1e278a6bb2ff21",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-12-27T19:21:19.591920800Z",
          "start_time": "2023-12-27T19:21:19.557907200Z"
        },
        "id": "fa1e278a6bb2ff21"
      },
      "outputs": [],
      "source": [
        "local_data_path_root = \".\"\n",
        "local_data_path = local_data_path_root+ \"/data\"\n",
        "local_vocab_path = local_data_path_root+ \"/vocab/\"\n",
        "local_model_path = local_data_path_root+ \"/models/\"\n",
        "\n",
        "# Directory where the checkpoints will be saved\n",
        "checkpoint_dir = local_data_path_root + '/training_checkpoints'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "1927d862267744c0",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-12-27T19:21:19.663429200Z",
          "start_time": "2023-12-27T19:21:19.571922300Z"
        },
        "id": "1927d862267744c0",
        "outputId": "1546fefa-d810-4c38-e4c5-8bbe6c84313a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt\n",
            "1115394/1115394 [==============================] - 0s 0us/step\n",
            "Length of text: 1115394 characters\n",
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You are all resolved rather to die than to famish?\n",
            "\n",
            "All:\n",
            "Resolved. resolved.\n",
            "\n",
            "First Citizen:\n",
            "First, you know Caius Marcius is chief enemy to the people.\n",
            "\n",
            "65 unique characters\n"
          ]
        }
      ],
      "source": [
        "path_to_file = tf.keras.utils.get_file('shakespeare.txt', 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt', cache_subdir=local_data_path)\n",
        "\n",
        "# Read, then decode\n",
        "text = open(path_to_file, 'rb').read().decode(encoding='utf-8')\n",
        "# length of text is the number of characters in it\n",
        "print(f'Length of text: {len(text)} characters')\n",
        "print(text[:250])\n",
        "\n",
        "vocab = sorted(set(text))\n",
        "print(f'{len(vocab)} unique characters')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e30eac3991c93dbd",
      "metadata": {
        "id": "e30eac3991c93dbd"
      },
      "source": [
        "get dataset and validate:"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ded04fc3f32bf47e",
      "metadata": {
        "collapsed": false,
        "id": "ded04fc3f32bf47e"
      },
      "source": [
        "## Step 1 - Preprocess the data\n",
        "with the dataset loaded, we need to vectorize it\n",
        "characters can be turned into numeric IDs, once the text is split into tokens\n",
        "\n",
        "and the tokens are turned into character IDs using a StringLookup layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "e04a1da10eed1caf",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-12-27T19:21:19.744946100Z",
          "start_time": "2023-12-27T19:21:19.666927900Z"
        },
        "id": "e04a1da10eed1caf",
        "outputId": "88280dee-98b4-4cc0-c881-1ab106e89eb3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<tf.RaggedTensor [[b'a', b'b', b'c', b'd', b'e', b'f', b'g'], [b'x', b'y', b'z']]>\n",
            "<tf.RaggedTensor [[40, 41, 42, 43, 44, 45, 46], [63, 64, 65]]>\n"
          ]
        }
      ],
      "source": [
        "example_texts = ['abcdefg', 'xyz']\n",
        "\n",
        "chars = tf.strings.unicode_split(example_texts, input_encoding='UTF-8')\n",
        "print(chars)\n",
        "\n",
        "ids_from_chars = tf.keras.layers.StringLookup(\n",
        "    vocabulary=list(vocab), mask_token=None)\n",
        "\n",
        "ids = ids_from_chars(chars)\n",
        "print(ids)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "379f715481536887",
      "metadata": {
        "collapsed": false,
        "id": "379f715481536887"
      },
      "source": [
        "for generation, this needs to be able to be inverted, recovering strings from IDs, using the same LookupLayer\n",
        "and these can be joined back into strings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "5dd54f052683de10",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-12-27T19:21:19.834470500Z",
          "start_time": "2023-12-27T19:21:19.744946100Z"
        },
        "id": "5dd54f052683de10",
        "outputId": "056fb27f-8fa3-4628-a712-eaf60f3c41c0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<tf.RaggedTensor [[b'a', b'b', b'c', b'd', b'e', b'f', b'g'], [b'x', b'y', b'z']]>\n",
            "[b'abcdefg' b'xyz']\n",
            "tf.Tensor([b'abcdefg' b'xyz'], shape=(2,), dtype=string)\n",
            "[b'abcdefg' b'xyz']\n"
          ]
        }
      ],
      "source": [
        "chars_from_ids = tf.keras.layers.StringLookup(\n",
        "    vocabulary=ids_from_chars.get_vocabulary(), invert=True, mask_token=None)\n",
        "chars = chars_from_ids(ids)\n",
        "print(chars)\n",
        "\n",
        "print(tf.strings.reduce_join(chars, axis=-1).numpy())\n",
        "\n",
        "def text_from_ids(ids):\n",
        "  return tf.strings.reduce_join(chars_from_ids(ids), axis=-1)\n",
        "\n",
        "print( text_from_ids(ids))\n",
        "print( text_from_ids(ids).numpy() )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "48756f5780bd892d",
      "metadata": {
        "collapsed": false,
        "id": "48756f5780bd892d"
      },
      "source": [
        "Task definition - to determine the next likely character, given a character or sequence\n",
        "RNNs maintain state dependent on prior seen elements, use that state to predict the next character\n",
        "\n",
        "Split set into training example sequences, each of seq_length length\n",
        "each sequence predicts the seq_length+1 character"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "ce577295113ca2df",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-12-27T19:21:20.164946500Z",
          "start_time": "2023-12-27T19:21:19.836977600Z"
        },
        "id": "ce577295113ca2df",
        "outputId": "5e81a87e-bd0a-4e03-b732-693d7d06ccff",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor([19 48 57 ... 46  9  1], shape=(1115394,), dtype=int64)\n",
            "F\n",
            "i\n",
            "r\n",
            "s\n",
            "t\n",
            " \n",
            "C\n",
            "i\n",
            "t\n",
            "i\n"
          ]
        }
      ],
      "source": [
        "all_ids = ids_from_chars(tf.strings.unicode_split(text, 'UTF-8'))\n",
        "print(all_ids)\n",
        "ids_dataset = tf.data.Dataset.from_tensor_slices(all_ids)\n",
        "for ids in ids_dataset.take(10):\n",
        "    print(chars_from_ids(ids).numpy().decode('utf-8'))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "6ffd09490941b024",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-12-27T19:21:20.176446300Z",
          "start_time": "2023-12-27T19:21:20.162444400Z"
        },
        "id": "6ffd09490941b024"
      },
      "outputs": [],
      "source": [
        "seq_length = 100"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6b6fc3dfab2f770f",
      "metadata": {
        "collapsed": false,
        "id": "6b6fc3dfab2f770f"
      },
      "source": [
        "use batch to generate appropriate sequences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "f813ebe9440fbe47",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-12-27T19:21:20.222449500Z",
          "start_time": "2023-12-27T19:21:20.180444800Z"
        },
        "id": "f813ebe9440fbe47",
        "outputId": "02533556-85e6-48aa-eb12-51612bda5760",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(\n",
            "[b'F' b'i' b'r' b's' b't' b' ' b'C' b'i' b't' b'i' b'z' b'e' b'n' b':'\n",
            " b'\\n' b'B' b'e' b'f' b'o' b'r' b'e' b' ' b'w' b'e' b' ' b'p' b'r' b'o'\n",
            " b'c' b'e' b'e' b'd' b' ' b'a' b'n' b'y' b' ' b'f' b'u' b'r' b't' b'h'\n",
            " b'e' b'r' b',' b' ' b'h' b'e' b'a' b'r' b' ' b'm' b'e' b' ' b's' b'p'\n",
            " b'e' b'a' b'k' b'.' b'\\n' b'\\n' b'A' b'l' b'l' b':' b'\\n' b'S' b'p' b'e'\n",
            " b'a' b'k' b',' b' ' b's' b'p' b'e' b'a' b'k' b'.' b'\\n' b'\\n' b'F' b'i'\n",
            " b'r' b's' b't' b' ' b'C' b'i' b't' b'i' b'z' b'e' b'n' b':' b'\\n' b'Y'\n",
            " b'o' b'u' b' '], shape=(101,), dtype=string)\n",
            "tf.Tensor(b'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou ', shape=(), dtype=string)\n",
            "tf.Tensor(b'are all resolved rather to die than to famish?\\n\\nAll:\\nResolved. resolved.\\n\\nFirst Citizen:\\nFirst, you k', shape=(), dtype=string)\n",
            "tf.Tensor(b\"now Caius Marcius is chief enemy to the people.\\n\\nAll:\\nWe know't, we know't.\\n\\nFirst Citizen:\\nLet us ki\", shape=(), dtype=string)\n",
            "tf.Tensor(b\"ll him, and we'll have corn at our own price.\\nIs't a verdict?\\n\\nAll:\\nNo more talking on't; let it be d\", shape=(), dtype=string)\n",
            "tf.Tensor(b'one: away, away!\\n\\nSecond Citizen:\\nOne word, good citizens.\\n\\nFirst Citizen:\\nWe are accounted poor citi', shape=(), dtype=string)\n"
          ]
        }
      ],
      "source": [
        "sequences = ids_dataset.batch(seq_length+1, drop_remainder=True)\n",
        "\n",
        "for seq in sequences.take(1):\n",
        "  print(chars_from_ids(seq))\n",
        "for seq in sequences.take(5):\n",
        "  print(text_from_ids(seq))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "63f29587c8c80c07",
      "metadata": {
        "collapsed": false,
        "id": "63f29587c8c80c07"
      },
      "source": [
        "these sequences need to be turned into input/label sets\n",
        "for each step, the input is the current character, and the label is the next character"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "e1ba20d5db37af4a",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-12-27T19:21:20.273995500Z",
          "start_time": "2023-12-27T19:21:20.224944900Z"
        },
        "id": "e1ba20d5db37af4a",
        "outputId": "3460f3a6-8f10-4dcc-adbb-d75f413affbc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(['t', 'e', 'n', 's', 'o', 'r', 'f', 'l', 'o'], ['e', 'n', 's', 'o', 'r', 'f', 'l', 'o', 'w'])\n"
          ]
        }
      ],
      "source": [
        "def split_input_target(sequence):\n",
        "    input_text = sequence[:-1]\n",
        "    target_text = sequence[1:]\n",
        "    return input_text, target_text\n",
        "\n",
        "print( split_input_target(list(\"tensorflow\")))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "32cd8928f818c438",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-12-27T19:21:20.322965500Z",
          "start_time": "2023-12-27T19:21:20.241943900Z"
        },
        "id": "32cd8928f818c438",
        "outputId": "8729da43-df71-4db7-b1fd-76150d5f84f3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input : b'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou'\n",
            "Target: b'irst Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n"
          ]
        }
      ],
      "source": [
        "dataset = sequences.map(split_input_target)\n",
        "\n",
        "for input_example, target_example in dataset.take(1):\n",
        "    print(\"Input :\", text_from_ids(input_example).numpy())\n",
        "    print(\"Target:\", text_from_ids(target_example).numpy())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5af8884960cb0015",
      "metadata": {
        "collapsed": false,
        "id": "5af8884960cb0015"
      },
      "source": [
        "with these sequences, we need to pack these into training batches we can use with the model\n",
        "note that not all data is pulled into memory at once using the batch and buffer size to manage this transition"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "93feefea03f62c09",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-12-27T19:21:20.385670300Z",
          "start_time": "2023-12-27T19:21:20.318965700Z"
        },
        "id": "93feefea03f62c09",
        "outputId": "a1126b8e-b51a-4913-83a0-ba59abbc982c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<_PrefetchDataset element_spec=(TensorSpec(shape=(64, 100), dtype=tf.int64, name=None), TensorSpec(shape=(64, 100), dtype=tf.int64, name=None))>\n"
          ]
        }
      ],
      "source": [
        "# Batch size\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "# Buffer size to shuffle the dataset\n",
        "# (TF data is designed to work with possibly infinite sequences,\n",
        "# so it doesn't attempt to shuffle the entire sequence in memory. Instead,\n",
        "# it maintains a buffer in which it shuffles elements).\n",
        "BUFFER_SIZE = 10000\n",
        "\n",
        "dataset = (\n",
        "    dataset\n",
        "    .shuffle(BUFFER_SIZE)\n",
        "    .batch(BATCH_SIZE, drop_remainder=True)\n",
        "    .prefetch(tf.data.experimental.AUTOTUNE))\n",
        "\n",
        "print(dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e8713c78a6632479",
      "metadata": {
        "collapsed": false,
        "id": "e8713c78a6632479"
      },
      "source": [
        "## Step 2 Model generation\n",
        "\n",
        "This Keras.Model implementation has three layers\n",
        "\n",
        "| layer name | function                             |\n",
        "|------------|--------------------------------------|\n",
        "| Embedding  | input layer                          |\n",
        "| GRU        | RNN with input size units=rnn_units  |\n",
        "| Dense      | output layer with vocab_size outputs |\n",
        " outputs are the log-likelihood of each character in the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "7b219313337c3c7d",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-12-27T19:21:20.387170100Z",
          "start_time": "2023-12-27T19:21:20.350086400Z"
        },
        "id": "7b219313337c3c7d"
      },
      "outputs": [],
      "source": [
        "# Length of the vocabulary in StringLookup Layer\n",
        "vocab_size = len(ids_from_chars.get_vocabulary())\n",
        "\n",
        "# The embedding dimension\n",
        "embedding_dim = 256\n",
        "\n",
        "# Number of RNN units\n",
        "rnn_units = 1024"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "b7b6f7029d1dd3ac",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-12-27T19:21:20.414171100Z",
          "start_time": "2023-12-27T19:21:20.369142400Z"
        },
        "id": "b7b6f7029d1dd3ac"
      },
      "outputs": [],
      "source": [
        "class MyModel(tf.keras.Model):\n",
        "  def __init__(self, vocab_size, embedding_dim, rnn_units):\n",
        "    super().__init__(self)\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "    self.gru = tf.keras.layers.GRU(rnn_units,\n",
        "                                   return_sequences=True,\n",
        "                                   return_state=True)\n",
        "    self.dense = tf.keras.layers.Dense(vocab_size)\n",
        "\n",
        "  def call(self, inputs, states=None, return_state=False, training=False):\n",
        "    x = inputs\n",
        "    x = self.embedding(x, training=training)\n",
        "    if states is None:\n",
        "      states = self.gru.get_initial_state(x)\n",
        "    x, states = self.gru(x, initial_state=states, training=training)\n",
        "    x = self.dense(x, training=training)\n",
        "\n",
        "    if return_state:\n",
        "      return x, states\n",
        "    else:\n",
        "      return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "10bbdceb577233a5",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-12-27T19:21:20.418169800Z",
          "start_time": "2023-12-27T19:21:20.381677200Z"
        },
        "id": "10bbdceb577233a5"
      },
      "outputs": [],
      "source": [
        "model = MyModel(\n",
        "    vocab_size=vocab_size,\n",
        "    embedding_dim=embedding_dim,\n",
        "    rnn_units=rnn_units)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cdb7e64aad89531b",
      "metadata": {
        "collapsed": false,
        "id": "cdb7e64aad89531b"
      },
      "source": [
        "## Testing the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "e7171fd6993fb1",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-12-27T19:21:27.851010500Z",
          "start_time": "2023-12-27T19:21:20.411171900Z"
        },
        "id": "e7171fd6993fb1",
        "outputId": "416e830a-8de2-4e59-f9f8-9cb4e26bc0a9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(64, 100, 66) # (batch_size, sequence_length, vocab_size)\n",
            "Model: \"my_model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       multiple                  16896     \n",
            "                                                                 \n",
            " gru (GRU)                   multiple                  3938304   \n",
            "                                                                 \n",
            " dense (Dense)               multiple                  67650     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 4022850 (15.35 MB)\n",
            "Trainable params: 4022850 (15.35 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "None\n"
          ]
        }
      ],
      "source": [
        "for input_example_batch, target_example_batch in dataset.take(1):\n",
        "    example_batch_predictions = model(input_example_batch)\n",
        "    print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")\n",
        "\n",
        "print(model.summary())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eaa784675224a1f4",
      "metadata": {
        "collapsed": false,
        "id": "eaa784675224a1f4"
      },
      "source": [
        "to actually predict, you need to process the logits returned over the character vocabulary\n",
        "for the first example (encoded and decoded):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "ef0efa1f97edd01d",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-12-27T19:21:27.890072600Z",
          "start_time": "2023-12-27T19:21:27.850510500Z"
        },
        "id": "ef0efa1f97edd01d",
        "outputId": "2b90a44f-fe46-42c9-a668-9adc2d4ac1f9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[46 64 22 62 55 62 21 25 63 53 51 43 26 16 58 29 54 53 32 59 29  3 35  7\n",
            " 28 15 11 41 61 32 50 35 53 62 48 21 53 21 51 31 43 14 27 55 63 30 65 46\n",
            "  0 56  5  8  4 16  2 34 41 25 24 59 26 31 29 17 16 22 35 28  8 52 18 61\n",
            " 38  2 37 31 39 30 43 38 36  1 57 11 22 54 30 52 21 30 11 40 51 41 15 12\n",
            " 57 32 61 21]\n",
            "Input:\n",
            " b'g,\\nAs little joy may you suppose in me.\\nThat I enjoy, being the queen thereof.\\n\\nQUEEN MARGARET:\\nA li'\n",
            "\n",
            "Next Char Predictions:\n",
            " b'gyIwpwHLxnldMCsPonStP!V,OB:bvSkVnwiHnHlRdANpxQzg[UNK]q&-$C UbLKtMRPDCIVO-mEvY XRZQdYW\\nr:IoQmHQ:albB;rSvH'\n"
          ]
        }
      ],
      "source": [
        "sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)\n",
        "sampled_indices = tf.squeeze(sampled_indices, axis=-1).numpy()\n",
        "print(sampled_indices)\n",
        "print(\"Input:\\n\", text_from_ids(input_example_batch[0]).numpy())\n",
        "print()\n",
        "print(\"Next Char Predictions:\\n\", text_from_ids(sampled_indices).numpy())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e979f764cb424f57",
      "metadata": {
        "collapsed": false,
        "id": "e979f764cb424f57"
      },
      "source": [
        "## Step 3 - Model Training\n",
        "\n",
        "We've turned prediction into a simple classification problem - given the prior state, predict the class of the next character\n",
        "We need an optimizer and loss function\n",
        "- the crossentropy loss function is reasonable in this case?\n",
        "- use the 'Adam' optimizer\n",
        "- use Compile to comfigure the training process"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "4b97396b8948378f",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-12-27T19:21:27.938056700Z",
          "start_time": "2023-12-27T19:21:27.877537600Z"
        },
        "id": "4b97396b8948378f",
        "outputId": "b7980c76-8236-4b0d-c5fa-0e19fc70e6d9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction shape:  (64, 100, 66)  # (batch_size, sequence_length, vocab_size)\n",
            "Mean loss:         tf.Tensor(4.189856, shape=(), dtype=float32)\n"
          ]
        }
      ],
      "source": [
        "loss = tf.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "\n",
        "example_batch_mean_loss = loss(target_example_batch, example_batch_predictions)\n",
        "print(\"Prediction shape: \", example_batch_predictions.shape, \" # (batch_size, sequence_length, vocab_size)\")\n",
        "print(\"Mean loss:        \", example_batch_mean_loss)\n",
        "\n",
        "model.compile(optimizer='adam', loss=loss)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ead04c6b21128780",
      "metadata": {
        "collapsed": false,
        "id": "ead04c6b21128780"
      },
      "source": [
        "prior to training, the mean loss should be equal to the vocabulary size - even higher values means the model us certain of its wrong answers..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "da245f657adda1ec",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-12-27T19:21:27.991632900Z",
          "start_time": "2023-12-27T19:21:27.924533800Z"
        },
        "id": "da245f657adda1ec",
        "outputId": "51438567-b500-43bc-c41c-9313177a3278",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "66.01329\n"
          ]
        }
      ],
      "source": [
        "print(tf.exp(example_batch_mean_loss).numpy())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e0ece1323f350fc1",
      "metadata": {
        "collapsed": false,
        "id": "e0ece1323f350fc1"
      },
      "source": [
        "configure checkpoints, and train the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "45f9f66163cab996",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-12-27T19:21:27.993600300Z",
          "start_time": "2023-12-27T19:21:27.956045600Z"
        },
        "id": "45f9f66163cab996"
      },
      "outputs": [],
      "source": [
        "# Name of the checkpoint files\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
        "\n",
        "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_prefix,\n",
        "    save_weights_only=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "5ba8df5f55990c0",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-12-27T19:23:14.484173900Z",
          "start_time": "2023-12-27T19:21:27.970594500Z"
        },
        "id": "5ba8df5f55990c0",
        "outputId": "4be3b5c9-ca67-4297-d79d-c7a77463bd10",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "172/172 [==============================] - 14s 57ms/step - loss: 2.7256\n",
            "Epoch 2/20\n",
            "172/172 [==============================] - 11s 57ms/step - loss: 1.9898\n",
            "Epoch 3/20\n",
            "172/172 [==============================] - 12s 58ms/step - loss: 1.7124\n",
            "Epoch 4/20\n",
            "172/172 [==============================] - 12s 61ms/step - loss: 1.5523\n",
            "Epoch 5/20\n",
            "172/172 [==============================] - 12s 62ms/step - loss: 1.4537\n",
            "Epoch 6/20\n",
            "172/172 [==============================] - 12s 62ms/step - loss: 1.3856\n",
            "Epoch 7/20\n",
            "172/172 [==============================] - 13s 62ms/step - loss: 1.3327\n",
            "Epoch 8/20\n",
            "172/172 [==============================] - 12s 60ms/step - loss: 1.2894\n",
            "Epoch 9/20\n",
            "172/172 [==============================] - 12s 59ms/step - loss: 1.2481\n",
            "Epoch 10/20\n",
            "172/172 [==============================] - 13s 59ms/step - loss: 1.2086\n",
            "Epoch 11/20\n",
            "172/172 [==============================] - 12s 61ms/step - loss: 1.1703\n",
            "Epoch 12/20\n",
            "172/172 [==============================] - 12s 62ms/step - loss: 1.1287\n",
            "Epoch 13/20\n",
            "172/172 [==============================] - 12s 62ms/step - loss: 1.0868\n",
            "Epoch 14/20\n",
            "172/172 [==============================] - 12s 61ms/step - loss: 1.0414\n",
            "Epoch 15/20\n",
            "172/172 [==============================] - 12s 60ms/step - loss: 0.9953\n",
            "Epoch 16/20\n",
            "172/172 [==============================] - 12s 59ms/step - loss: 0.9440\n",
            "Epoch 17/20\n",
            "172/172 [==============================] - 12s 60ms/step - loss: 0.8931\n",
            "Epoch 18/20\n",
            "172/172 [==============================] - 12s 60ms/step - loss: 0.8413\n",
            "Epoch 19/20\n",
            "172/172 [==============================] - 13s 62ms/step - loss: 0.7898\n",
            "Epoch 20/20\n",
            "172/172 [==============================] - 13s 62ms/step - loss: 0.7409\n",
            "<keras.src.callbacks.History object at 0x784f7670e4d0>\n"
          ]
        }
      ],
      "source": [
        "EPOCHS = 20\n",
        "history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])\n",
        "print(history)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ca34575165428d3e",
      "metadata": {
        "collapsed": false,
        "id": "ca34575165428d3e"
      },
      "source": [
        "## Step 4 Text Generation\n",
        "\n",
        "can use a loop that passes in a propmpt, adds the generated output and passes that back in\n",
        "\n",
        "We'll define a class that takes the model and char <--> id functions as inputs\n",
        "- this class masks the input such that [UNK] won't be generated\n",
        "- then the function that\n",
        "    - tokenizees the current inputs\n",
        "    - runs the model on the input\n",
        "    - gets the last prediction\n",
        "    - samples the last prediction\n",
        "    - char-izes the resulting output\n",
        "    - returns the updated result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "129ec09c044c857a",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-12-27T19:23:14.504673700Z",
          "start_time": "2023-12-27T19:23:14.491673600Z"
        },
        "id": "129ec09c044c857a"
      },
      "outputs": [],
      "source": [
        "class OneStep(tf.keras.Model):\n",
        "  def __init__(self, model, chars_from_ids, ids_from_chars, temperature=1.0):\n",
        "    super().__init__()\n",
        "    self.temperature = temperature\n",
        "    self.model = model\n",
        "    self.chars_from_ids = chars_from_ids\n",
        "    self.ids_from_chars = ids_from_chars\n",
        "\n",
        "    # Create a mask to prevent \"[UNK]\" from being generated.\n",
        "    skip_ids = self.ids_from_chars(['[UNK]'])[:, None]\n",
        "    sparse_mask = tf.SparseTensor(\n",
        "        # Put a -inf at each bad index.\n",
        "        values=[-float('inf')]*len(skip_ids),\n",
        "        indices=skip_ids,\n",
        "        # Match the shape to the vocabulary\n",
        "        dense_shape=[len(ids_from_chars.get_vocabulary())])\n",
        "    self.prediction_mask = tf.sparse.to_dense(sparse_mask)\n",
        "\n",
        "  @tf.function\n",
        "  def generate_one_step(self, inputs, states=None):\n",
        "    # Convert strings to token IDs.\n",
        "    input_chars = tf.strings.unicode_split(inputs, 'UTF-8')\n",
        "    input_ids = self.ids_from_chars(input_chars).to_tensor()\n",
        "\n",
        "    # Run the model.\n",
        "    # predicted_logits.shape is [batch, char, next_char_logits]\n",
        "    predicted_logits, states = self.model(inputs=input_ids, states=states,\n",
        "                                          return_state=True)\n",
        "    # Only use the last prediction.\n",
        "    predicted_logits = predicted_logits[:, -1, :]\n",
        "    predicted_logits = predicted_logits/self.temperature\n",
        "    # Apply the prediction mask: prevent \"[UNK]\" from being generated.\n",
        "    predicted_logits = predicted_logits + self.prediction_mask\n",
        "\n",
        "    # Sample the output logits to generate token IDs.\n",
        "    predicted_ids = tf.random.categorical(predicted_logits, num_samples=1)\n",
        "    predicted_ids = tf.squeeze(predicted_ids, axis=-1)\n",
        "\n",
        "    # Convert from token ids to characters\n",
        "    predicted_chars = self.chars_from_ids(predicted_ids)\n",
        "\n",
        "    # Return the characters and model state.\n",
        "    return predicted_chars, states"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "616ccba8734af7e3",
      "metadata": {
        "collapsed": false,
        "id": "616ccba8734af7e3"
      },
      "source": [
        "now define an instance of our generator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "b9c65170b9cae4e0",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-12-27T19:23:14.535185100Z",
          "start_time": "2023-12-27T19:23:14.502674Z"
        },
        "id": "b9c65170b9cae4e0"
      },
      "outputs": [],
      "source": [
        "one_step_model = OneStep(model, chars_from_ids, ids_from_chars)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9ebc75c68a1635eb",
      "metadata": {
        "collapsed": false,
        "id": "9ebc75c68a1635eb"
      },
      "source": [
        "the generation loop:\n",
        "- defines a constant text of \"ROMEO:\"\n",
        "- initialize our result (state) to this constant\n",
        "- iterates 1000 times\n",
        "    - calling our function to get the next char (and states)\n",
        "    - appends the next char to \"result\"\n",
        "- finally, the output result characters are joined together\n",
        "- and the result (and runtime) are printed out  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "fe91809392697f1c",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-12-27T19:23:17.816131800Z",
          "start_time": "2023-12-27T19:23:14.534188700Z"
        },
        "id": "fe91809392697f1c",
        "outputId": "9b2d0abb-47db-41d4-c1f9-78260848d3bc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "________________________________________________________________________________\n",
            "\n",
            "ROMEO:\n",
            "You make such purpose, sir, for no more due scrace;\n",
            "Since our steeps with twenty gods,\n",
            "For mine arms, our beauty: good enteed, meater, prentinant\n",
            "Decound his master, for some deal of sorrow.\n",
            "\n",
            "ROMEO:\n",
            "I pray when you have done, whose love, think'st\n",
            "Require's subs dislaint the afflicts of thy way.\n",
            "\n",
            "HENRY PERCY:\n",
            "I know not where flowers out of breath,\n",
            "Even to the poor son RomE:\n",
            "A mainted blacks from the records.\n",
            "Ir wish! deserves God's gate,\n",
            "Peterachs with the deep struck queen's;\n",
            "And so part therebin the badments by you,\n",
            "Dear Jove lies where once alard millo,\n",
            "Whose courage their tedious untand and enforce.\n",
            "When did shoke received the grave we'll swear.\n",
            "Hark, how the more home?\n",
            "\n",
            "MENENIUS:\n",
            "This is strange?\n",
            "When he disdain,\n",
            "As far scorn like securaties, hast\n",
            "Hard's saift, he is for me; we will have me\n",
            "As was your body to summons for their own.\n",
            "Lord Oxford, to Appare he hurl down with thy\n",
            "bonested black favours by\n",
            "A prefermst perpoition. Constable, for I it lead\n",
            "Some conceit of damned Richmo \n",
            "\n",
            "________________________________________________________________________________\n",
            "\n",
            "Run time: 3.4538521766662598\n"
          ]
        }
      ],
      "source": [
        "start = time.time()\n",
        "states = None\n",
        "next_char = tf.constant(['ROMEO:'])\n",
        "result = [next_char]\n",
        "\n",
        "for n in range(1000):\n",
        "  next_char, states = one_step_model.generate_one_step(next_char, states=states)\n",
        "  result.append(next_char)\n",
        "\n",
        "result = tf.strings.join(result)\n",
        "end = time.time()\n",
        "print('_'*80 + '\\n\\n' + result[0].numpy().decode('utf-8'), '\\n\\n' + '_'*80)\n",
        "print('\\nRun time:', end - start)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7c89dd896a5f29a7",
      "metadata": {
        "collapsed": false,
        "id": "7c89dd896a5f29a7"
      },
      "source": [
        "### things to note:\n",
        "- this model is character-based, so it doesn't really know about words,\n",
        "- it does know when to capitalize, insert paragraph breaks, emulate the Shakespearean vocab from the vocabulary\n",
        "- it has not yet learned to generate coherent sentences\n",
        "    - larger number of epochs may improve this\n",
        "\n",
        "### things to experiment with\n",
        "- the \"temperature\" parameter can be used to make more/less random predictions\n",
        "- starting with a different seed string will change the output\n",
        "- adding another RNN layer can improve its overall accuracy\n",
        "- generation speed can be improved by batching (below code runs in similar time to above code)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "7bae3e60975dd545",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-12-27T19:23:21.351237400Z",
          "start_time": "2023-12-27T19:23:17.823639900Z"
        },
        "id": "7bae3e60975dd545",
        "outputId": "86c23ed6-d195-4228-f064-741013635596",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(\n",
            "[b\"ROMEO:\\nNot Romeo, that sad eye; or; what offence nay,\\nIf thou hadst for breathe upon the shewer duty,\\nAnd to misuse his body teaching to blood,\\nWited joys, and to be sad, so many sorrow\\nAnd her wants character to his country.\\n\\nCORIOLANUS:\\nI follow'd I give a man,\\nWhom I, without his foot king'd misery,\\nWhose infastised waste gods;\\nHumour it to make for clock.\\nDoth we mis start in post to enter in.\\n\\nCLIFFORD:\\nMy lord, here comes the fatal scorn:\\nWomen hath held it grows him do and hard.\\n\\nRATCLIFF:\\nDidst title Benvolio; O usuppured!\\n\\nGONZALO:\\nWhat stay, is my poor fortune!\\n\\nQUEEN:\\nThat am I, sir, to save my letter,\\nFor his own way; whose alive us on our voice,\\nInto their power with bawdy upon her: say\\nMy eyes about, all imposing to the Duke of Lancaster.\\nYou, my good lord, dismant that may strike.\\n\\nBUSKE:\\nWhy, mass I plain my father Warwick?\\nCome, come, you bring not signifies!\\nThis is that hand, farest thou, King Richard's hand.\\n\\nPAULINA:\\nWell, being thus forward to the\\nextremest slaughter Cl\"\n",
            " b\"ROMEO:\\nGrumio! when 'tes you on the way?\\n\\nCLIFFORD:\\nWhy, that dear saints draving a went: his mother\\nso.\\n\\nProvost:\\nHow joy!\\n\\nJULIET:\\nO, but which had you not; if I loved mischance:\\nThen thou, unload to the moon of my poor confemenaly.\\n\\nGLOUCESTER:\\nStay, your chaps: hear me speak.\\n\\nBRUTUS:\\nCome, I say, I pray you and I\\nprove sheep so from a grain Neven a dog I had.\\n\\nKING LEWIS XI:\\nWell marry le; her duty and the cunning:\\nBe strange Richarm, he had not answered.\\nMy brother, hath devour usurp'd,\\nThat know not what I swound; it may not so,\\nErrow and with him give distorce with me;\\nBut or bild to make dusphs the deepes of land,\\nCome help that will stand to change, we'll to die.\\n\\nGREGORO:\\nAm I but then?\\n\\nFirst Senator:\\nI pray the mother of our outrage, nor scorn,\\nI'll gladly desire to live that valuable,\\nBut with thy bolder, there might ravish book\\nindeed, that title in your time,\\nIf robbery me, good friend here,\\nTo make her hand, whomabound your grace.\\n\\nGLOUCESTER:\\nAnd in thy kingdom stands your \"\n",
            " b\"ROMEO:\\nThis way, but something.\\n\\nSICINIUS:\\nAs the main.\\n\\nThird Conspirator:\\nTherefore disdain the clouds o'erphore home: you are darcely too,\\nHalile them from the tears that bledding in\\nSpigin ourselves with man, seeing him;\\nAnd in regiven unlike, and the blood\\nWhose hate in safes of them and spirits. Let no more noble\\nWouldst have my wife. Do I sugh prantellisy\\nWell see the axe conserved ears his noble\\nDotted for, how to turn his hands aspecially to be\\nWith your command; the town of Edward's prince,\\nAnd his power heart; and with two liest,\\nHis dagger's poll, a faithless black dreeds,\\nHaving but a thousand father of the dianest.\\n\\nKING RICHARD III:\\nAnd you shall be that disgaised unto mine that I may.\\nI must to God, he was not bid them not;\\nThou, or fortune at once comfort me.\\nWhat doth he soon met? what is at natual?\\n\\nCORIOLANUS:\\nAy, or two sick mortal-glass,\\nIn me regare me leave to see: her womb I see\\nA mother's dear-loved, as far as dayly subjects,\\nEre our boy prosperity, and that's more \"\n",
            " b'ROMEO:\\nI cloudy Edward for brothers that surfeis up;\\nAskines am nothing; but not withoutly I but hear his help\\nThat doth madam lives aboding about.\\n\\nCitizens:\\nGo; get you green, sir, boing to years, and finds for mad.\\n\\nPETRUCHIO:\\nMy heart the dignity is repide.\\n\\nMERCUTIO:\\nCome, come, Katharina,\\nAn asses some friend open of more than this son,\\nFor that in goodness have done with direst;\\nWhy brought since the boar is offended the night\\nInto his traitoring cheers the tidings of my shroud.\\n\\nHASTINGS:\\nIs not the deep dear King of kings and gentleness,\\nCorrocting them, when from her actions\\nthey are, hindows.\\n\\nKING RICHARD II:\\nHow camest thou home a boar will ear\\nIn bolth. Now one three proclamation of\\nRome, ere thou incapation beward.\\n\\nDORCAS:\\nIf go, being squired for and by the action.\\n3 KING HENRY VI\\n\\nKING LEWIS XI:\\nWe are but set and bring them women? Heaven keep it then sepulchre?\\n\\nSIR STEPHEN SCROOP:\\nMy sons!\\n\\nMISTRESS OVERDONE:\\nYour horse deparmed well; but here I see my standing here.\\nGood'\n",
            " b\"ROMEO:\\nThis man I seek water thou decrees\\nAll that he shall have with me but sensible.\\n\\nLUCIO:\\n'Twas not your husband in it so; and so, for well\\nRe--as no matters; that goes bower the adward be\\nA. ANNoble's foot in blood to brow\\nI take my daughter's bosom!\\n\\nRICHARD:\\nThou shalt be made, but not my sweet'st infected\\nBy honour follows thence; for this is all,\\nAnd is no lingerle start himself:\\nbeing now apparel fit thy back of corse dewards\\nWhen he shall marry her.\\n\\nBUCKINGHAM:\\nWhy, marry, what of him? ha? lie\\nthy mutinies. For your powers draw not your bosom;\\nAnd now my gracious lord, whose very prettined he\\nwas dream'd a wooth and conduct.\\n\\nVINCENTIO:\\nBut Warwick!\\n\\nISABELLA:\\nI have confusion'd in thy mistress,\\nSince that our funt in this goose did stand.\\n\\nLADY GREY:\\nTherefore, I know not how much to me.\\n\\nMENENIUS:\\nNo, 'tis namely born.\\n\\nClown:\\nHow now you have a staigo so?\\n\\nCitizens:\\nCome, come! what means this gold in flat;\\nbrief our boor, with a frail nor farther true madest,\\nHis goods throu\"], shape=(5,), dtype=string) \n",
            "\n",
            "________________________________________________________________________________\n",
            "\n",
            "Run time: 4.288720369338989\n"
          ]
        }
      ],
      "source": [
        "start = time.time()\n",
        "states = None\n",
        "next_char = tf.constant(['ROMEO:', 'ROMEO:', 'ROMEO:', 'ROMEO:', 'ROMEO:'])\n",
        "result = [next_char]\n",
        "\n",
        "for n in range(1000):\n",
        "  next_char, states = one_step_model.generate_one_step(next_char, states=states)\n",
        "  result.append(next_char)\n",
        "\n",
        "result = tf.strings.join(result)\n",
        "end = time.time()\n",
        "print(result, '\\n\\n' + '_'*80)\n",
        "print('\\nRun time:', end - start)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7c1fc7bac977c36f",
      "metadata": {
        "collapsed": false,
        "id": "7c1fc7bac977c36f"
      },
      "source": [
        "## Step 5 - Saving the trained generator model\n",
        "\n",
        "so it can be used as a tf.saved_model\n",
        "Note that you should not call the \"save\" step below if you just want to load/run the saved version - skip that cell..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "8e2576efc2be427a",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-12-27T19:23:26.791321300Z",
          "start_time": "2023-12-27T19:23:21.355229300Z"
        },
        "id": "8e2576efc2be427a",
        "outputId": "a4b4717c-242f-46e7-cecc-44b688f706fc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Skipping full serialization of Keras layer <__main__.OneStep object at 0x784f7670cd60>, because it is not built.\n",
            "WARNING:tensorflow:Model's `__init__()` arguments contain non-serializable objects. Please implement a `get_config()` method in the subclassed Model for proper saving and loading. Defaulting to empty config.\n",
            "WARNING:tensorflow:Model's `__init__()` arguments contain non-serializable objects. Please implement a `get_config()` method in the subclassed Model for proper saving and loading. Defaulting to empty config.\n"
          ]
        }
      ],
      "source": [
        "tf.saved_model.save(one_step_model, local_model_path+'one_step')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "c77e8d372d5232f3",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-12-27T19:23:28.745757800Z",
          "start_time": "2023-12-27T19:23:26.794814Z"
        },
        "id": "c77e8d372d5232f3"
      },
      "outputs": [],
      "source": [
        "one_step_reloaded = tf.saved_model.load(local_model_path+'one_step')"
      ]
    },
    {
      "cell_type": "raw",
      "id": "77b234221625b70",
      "metadata": {
        "collapsed": false,
        "id": "77b234221625b70"
      },
      "source": [
        "this is actually a big deal - you include your wrapper and handling code around the tf supplied model...\n",
        "use the model just as you used the originally trained model (but just show 100 characters...):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "id": "a68ef82b0cf98885",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-12-27T19:23:29.847598200Z",
          "start_time": "2023-12-27T19:23:28.752765100Z"
        },
        "id": "a68ef82b0cf98885",
        "outputId": "09811542-ab1d-4ef8-b0ea-d810ccf0858c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ROMEO:\n",
            "The news do pity me.\n",
            "\n",
            "MONTAGUE:\n",
            "Thou wert sworn much else to make the last take my looks:\n",
            "Block, a \n"
          ]
        }
      ],
      "source": [
        "states = None\n",
        "next_char = tf.constant(['ROMEO:'])\n",
        "result = [next_char]\n",
        "\n",
        "for n in range(100):\n",
        "  next_char, states = one_step_reloaded.generate_one_step(next_char, states=states)\n",
        "  result.append(next_char)\n",
        "\n",
        "print(tf.strings.join(result)[0].numpy().decode(\"utf-8\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "14a069e5a68279f6",
      "metadata": {
        "collapsed": false,
        "id": "14a069e5a68279f6"
      },
      "source": [
        "## Step 6 - Still to do\n",
        "\n",
        "- better understand\n",
        "    - the practical difference between using an RNN vs an LSTM layer\n",
        "        - [Working with RNNs](https://www.tensorflow.org/guide/keras/working_with_rnns)\n",
        "    - the masking step\n",
        "    - what to expect in terms of improved performance by adding an additional (RNN?) layer\n",
        "- this should operate similarly if we replaced the IDs as character placeholders with word tokenizations, no?\n",
        "- enable GPU (local is completed - Rosie still to do, should just be config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "id": "cc7327a535e48494",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-12-27T19:23:29.886013600Z",
          "start_time": "2023-12-27T19:23:29.848599Z"
        },
        "id": "cc7327a535e48494"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}