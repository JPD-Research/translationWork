{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# image captioning using attention\n",
    "\n",
    "Based on Tensorflow tutorial [Image captioning with visual attention](https://www.tensorflow.org/text/tutorials/image_captioning)\n",
    "\n",
    "Transformer model similar to that described in  [Attention is all you need](https://arxiv.org/abs/1706.03762)\n",
    "\n",
    "- primary difference with model in seq2seq notebook is replacing the RNN layers with [self-attention](https://developers.google.com/machine-learning/glossary#self-attention) layers\n",
    "- self-attention allows broader transmission of information across input sequences\n",
    "\n",
    "1. prepare data\n",
    "2. implement components\n",
    "    1. positional embeddings\n",
    "    1. attention layers\n",
    "    1. encoder / decoder\n",
    "1. build/train transformer\n",
    "2. generate translations\n",
    "3. export the model \n",
    "\n",
    "With modifications described along the way...this does not currently leverage any available GPU, but will by the time we're done...\n",
    "\n",
    "## Step 0 - Environment Setup \n",
    "\n",
    "assumes tensorflow-text and tensorflow-datasets are installed in the environment\n",
    "conda yaml for the environment compatible with all of these notebooks is included in the root of this repo "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "210eb19d1c168334"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
