{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# image captioning using attention\n",
    "\n",
    "Based on Tensorflow tutorial [Image captioning with visual attention](https://www.tensorflow.org/text/tutorials/image_captioning)\n",
    "\n",
    "Similar to architecture described in  [Show, Attend and Tell: Neural Image Caption Generation with Visual Attention](https://arxiv.org/abs/1502.03044)\n",
    "\n",
    "Features are extracted from the image and passed to cross attention layers of the xfmr decoder\n",
    "\n",
    "With modifications described along the way...this does not currently leverage any available GPU, but will by the time we're done...\n",
    "\n",
    "## Step 0 - Environment Setup \n",
    "\n",
    "packages available in the yaml provided are sufficient for this work\n",
    "conda yaml for the environment compatible with all of these notebooks is included in the root of this repo "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "210eb19d1c168334"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "import concurrent.futures\n",
    "import collections\n",
    "import dataclasses\n",
    "import hashlib\n",
    "import itertools\n",
    "import json\n",
    "import math\n",
    "import os\n",
    "import pathlib\n",
    "import random\n",
    "import re\n",
    "import string\n",
    "import time\n",
    "import urllib.request\n",
    "\n",
    "import einops\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import requests\n",
    "import tqdm\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow_text as text\n",
    "import tensorflow_datasets as tfds"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-22T19:12:42.116620400Z",
     "start_time": "2023-12-22T19:12:30.444552400Z"
    }
   },
   "id": "80ee91c442d4328b"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Step 1 data handling\n",
    "\n",
    "download and prep dataset for training\n",
    "tokenizes input text, caches results of running images through a pretrained feature extractor"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "40ba0ac7281201f0"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "local_data_path_root = \"C:/LocalResearch/JPD-Research/translationWork\"\n",
    "local_data_path = local_data_path_root+ \"/data\"\n",
    "local_vocab_path = local_data_path_root+ \"/vocab/\"\n",
    "local_model_path = local_data_path_root+ \"/models/\"\n",
    "\n",
    "# Directory where the checkpoints will be saved\n",
    "checkpoint_dir = local_data_path_root + '/training_checkpoints'"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-22T19:14:44.943138200Z",
     "start_time": "2023-12-22T19:14:44.931615500Z"
    }
   },
   "id": "dafaf15e7d9f1592"
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "def flickr8k(path='flickr8k'):\n",
    "  path = pathlib.Path(path)\n",
    "  print(path)\n",
    "  print(local_data_path)\n",
    "  fullpath = local_data_path/path\n",
    "\n",
    "  if len(list(path.rglob('*'))) < 16197:\n",
    "    tf.keras.utils.get_file(\n",
    "        origin='https://github.com/jbrownlee/Datasets/releases/download/Flickr8k/Flickr8k_Dataset.zip',\n",
    "        cache_dir=local_data_path,\n",
    "        cache_subdir=path,\n",
    "        extract=True)\n",
    "    tf.keras.utils.get_file(\n",
    "        origin='https://github.com/jbrownlee/Datasets/releases/download/Flickr8k/Flickr8k_text.zip',\n",
    "        cache_dir=local_data_path,\n",
    "        cache_subdir=path,\n",
    "        extract=True)\n",
    "\n",
    "  \n",
    "  print(fullpath/\"Flickr8k.token.txt\")\n",
    "  captions = (fullpath/\"Flickr8k.token.txt\").read_text().splitlines()\n",
    "  captions = (line.split('\\t') for line in captions)\n",
    "  captions = ((fname.split('#')[0], caption) for (fname, caption) in captions)\n",
    "\n",
    "  cap_dict = collections.defaultdict(list)\n",
    "  for fname, cap in captions:\n",
    "    cap_dict[fname].append(cap)\n",
    "\n",
    "  train_files = (fullpath/'Flickr_8k.trainImages.txt').read_text().splitlines()\n",
    "  train_captions = [(str(fullpath/'Flicker8k_Dataset'/fname), cap_dict[fname]) for fname in train_files]\n",
    "\n",
    "  test_files = (fullpath/'Flickr_8k.testImages.txt').read_text().splitlines()\n",
    "  test_captions = [(str(fullpath/'Flicker8k_Dataset'/fname), cap_dict[fname]) for fname in test_files]\n",
    "\n",
    "  train_ds = tf.data.experimental.from_list(train_captions)\n",
    "  test_ds = tf.data.experimental.from_list(test_captions)\n",
    "\n",
    "  return train_ds, test_ds"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-22T19:49:31.019940200Z",
     "start_time": "2023-12-22T19:49:31.011451300Z"
    }
   },
   "id": "6313e6aa2e3b6298"
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [],
   "source": [
    "def conceptual_captions(*, data_dir=\"conceptual_captions\", num_train, num_val):\n",
    "  def iter_index(index_path):\n",
    "    with open(index_path) as f:\n",
    "      for line in f:\n",
    "        caption, url = line.strip().split('\\t')\n",
    "        yield caption, url\n",
    "\n",
    "  def download_image_urls(data_dir, urls):\n",
    "    ex = concurrent.futures.ThreadPoolExecutor(max_workers=100)\n",
    "    def save_image(url):\n",
    "      hash = hashlib.sha1(url.encode())\n",
    "      # Name the files after the hash of the URL.\n",
    "      file_path = data_dir/f'{hash.hexdigest()}.jpeg'\n",
    "      if file_path.exists():\n",
    "        # Only download each file once.\n",
    "        return file_path\n",
    "\n",
    "      try:\n",
    "        result = requests.get(url, timeout=5)\n",
    "      except Exception:\n",
    "        file_path = None\n",
    "      else:\n",
    "        file_path.write_bytes(result.content)\n",
    "      return file_path\n",
    "\n",
    "    result = []\n",
    "    out_paths = ex.map(save_image, urls)\n",
    "    for file_path in tqdm.tqdm(out_paths, total=len(urls)):\n",
    "      result.append(file_path)\n",
    "\n",
    "    return result\n",
    "\n",
    "  def ds_from_index_file(index_path, data_dir, count):\n",
    "    data_dir.mkdir(exist_ok=True)\n",
    "    index = list(itertools.islice(iter_index(index_path), count))\n",
    "    captions = [caption for caption, url in index]\n",
    "    urls = [url for caption, url in index]\n",
    "\n",
    "    paths = download_image_urls(data_dir, urls)\n",
    "\n",
    "    new_captions = []\n",
    "    new_paths = []\n",
    "    for cap, path in zip(captions, paths):\n",
    "      if path is None:\n",
    "        # Download failed, so skip this pair.\n",
    "        continue\n",
    "      new_captions.append(cap)\n",
    "      new_paths.append(path)\n",
    "\n",
    "    new_paths = [str(p) for p in new_paths]\n",
    "\n",
    "    ds = tf.data.Dataset.from_tensor_slices((new_paths, new_captions))\n",
    "    ds = ds.map(lambda path,cap: (path, cap[tf.newaxis])) # 1 caption per image\n",
    "    return ds\n",
    "\n",
    "  #data_dir = pathlib.Path(data_dir)\n",
    "  train_index_path = tf.keras.utils.get_file(\n",
    "    origin='https://storage.googleapis.com/gcc-data/Train/GCC-training.tsv',\n",
    "    cache_dir=local_data_path,\n",
    "    cache_subdir=data_dir\n",
    "    )\n",
    "\n",
    "  val_index_path = tf.keras.utils.get_file(\n",
    "    origin='https://storage.googleapis.com/gcc-data/Validation/GCC-1.1.0-Validation.tsv',\n",
    "    cache_dir=local_data_path,\n",
    "    cache_subdir=data_dir\n",
    "    )\n",
    "\n",
    "  fullpath = local_data_path+\"/\"+data_dir\n",
    "  train_raw = ds_from_index_file(train_index_path, data_dir=pathlib.Path(fullpath+'/train'), count=num_train)\n",
    "  test_raw = ds_from_index_file(val_index_path, data_dir=pathlib.Path(fullpath+'/val'), count=num_val)\n",
    "\n",
    "  return train_raw, test_raw"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-22T20:05:10.202629200Z",
     "start_time": "2023-12-22T20:05:10.172042800Z"
    }
   },
   "id": "7a8455f51e3d1493"
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "flickr8k\n",
      "C:/LocalResearch/JPD-Research/translationWork/data\n",
      "C:\\LocalResearch\\JPD-Research\\translationWork\\data\\flickr8k\\Flickr8k.token.txt\n"
     ]
    }
   ],
   "source": [
    "choose = 'flickr8k'\n",
    "\n",
    "if choose == 'flickr8k':\n",
    "  train_raw, test_raw = flickr8k()\n",
    "else:\n",
    "  train_raw, test_raw = conceptual_captions(num_train=10000, num_val=5000)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-22T20:12:34.211634400Z",
     "start_time": "2023-12-22T20:12:05.885977100Z"
    }
   },
   "id": "8ecce03698cde888"
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "outputs": [
    {
     "data": {
      "text/plain": "(TensorSpec(shape=(), dtype=tf.string, name=None),\n TensorSpec(shape=(5,), dtype=tf.string, name=None))"
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_raw.element_spec"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-22T20:12:48.286249Z",
     "start_time": "2023-12-22T20:12:48.280243600Z"
    }
   },
   "id": "5b51b2cde97533ad"
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(b'C:\\\\LocalResearch\\\\JPD-Research\\\\translationWork\\\\data\\\\flickr8k\\\\Flicker8k_Dataset\\\\2513260012_03d33305cf.jpg', shape=(), dtype=string)\n",
      "tf.Tensor(\n",
      "[b'A black dog is running after a white dog in the snow .'\n",
      " b'Black dog chasing brown dog through snow'\n",
      " b'Two dogs chase each other across the snowy ground .'\n",
      " b'Two dogs play together in the snow .'\n",
      " b'Two dogs running through a low lying body of water .'], shape=(5,), dtype=string)\n"
     ]
    }
   ],
   "source": [
    "for ex_path, ex_captions in train_raw.take(1):\n",
    "  print(ex_path)\n",
    "  print(ex_captions)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-22T20:12:53.050662800Z",
     "start_time": "2023-12-22T20:12:50.870567400Z"
    }
   },
   "id": "dc256207712d49ea"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
