{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# image captioning using attention\n",
    "\n",
    "Based on Tensorflow tutorial [Image captioning with visual attention](https://www.tensorflow.org/text/tutorials/image_captioning)\n",
    "\n",
    "Similar to architecture described in  [Show, Attend and Tell: Neural Image Caption Generation with Visual Attention](https://arxiv.org/abs/1502.03044)\n",
    "\n",
    "Features are extracted from the image and passed to cross attention layers of the xfmr decoder\n",
    "\n",
    "With modifications described along the way...this does not currently leverage any available GPU, but will by the time we're done...\n",
    "\n",
    "## Step 0 - Environment Setup \n",
    "\n",
    "packages available in the yaml provided are sufficient for this work\n",
    "conda yaml for the environment compatible with all of these notebooks is included in the root of this repo "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "210eb19d1c168334"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "import concurrent.futures\n",
    "import collections\n",
    "import dataclasses\n",
    "import hashlib\n",
    "import itertools\n",
    "import json\n",
    "import math\n",
    "import os\n",
    "import pathlib\n",
    "import random\n",
    "import re\n",
    "import string\n",
    "import time\n",
    "import urllib.request\n",
    "\n",
    "import einops\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import requests\n",
    "import tqdm\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow_text as text\n",
    "import tensorflow_datasets as tfds"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-22T19:12:42.116620400Z",
     "start_time": "2023-12-22T19:12:30.444552400Z"
    }
   },
   "id": "80ee91c442d4328b"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Step 1 data handling\n",
    "\n",
    "download and prep dataset for training\n",
    "tokenizes input text, caches results of running images through a pretrained feature extractor"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "40ba0ac7281201f0"
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "outputs": [],
   "source": [
    "local_data_path_root = \"C:/LocalResearch/JPD-Research/translationWork\"\n",
    "local_data_path = local_data_path_root+ \"/data\"\n",
    "local_vocab_path = local_data_path_root+ \"/vocab/\"\n",
    "local_model_path = local_data_path_root+ \"/models/\"\n",
    "local_data_cache_path = local_data_path+\"/captioning/\"\n",
    "\n",
    "# Directory where the checkpoints will be saved\n",
    "checkpoint_dir = local_data_path_root + '/training_checkpoints'"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-23T01:00:34.862246400Z",
     "start_time": "2023-12-23T01:00:34.856735100Z"
    }
   },
   "id": "dafaf15e7d9f1592"
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "def flickr8k(path='flickr8k'):\n",
    "  path = pathlib.Path(path)\n",
    "  print(path)\n",
    "  print(local_data_path)\n",
    "  fullpath = local_data_path/path\n",
    "\n",
    "  if len(list(path.rglob('*'))) < 16197:\n",
    "    tf.keras.utils.get_file(\n",
    "        origin='https://github.com/jbrownlee/Datasets/releases/download/Flickr8k/Flickr8k_Dataset.zip',\n",
    "        cache_dir=local_data_path,\n",
    "        cache_subdir=path,\n",
    "        extract=True)\n",
    "    tf.keras.utils.get_file(\n",
    "        origin='https://github.com/jbrownlee/Datasets/releases/download/Flickr8k/Flickr8k_text.zip',\n",
    "        cache_dir=local_data_path,\n",
    "        cache_subdir=path,\n",
    "        extract=True)\n",
    "\n",
    "  \n",
    "  print(fullpath/\"Flickr8k.token.txt\")\n",
    "  captions = (fullpath/\"Flickr8k.token.txt\").read_text().splitlines()\n",
    "  captions = (line.split('\\t') for line in captions)\n",
    "  captions = ((fname.split('#')[0], caption) for (fname, caption) in captions)\n",
    "\n",
    "  cap_dict = collections.defaultdict(list)\n",
    "  for fname, cap in captions:\n",
    "    cap_dict[fname].append(cap)\n",
    "\n",
    "  train_files = (fullpath/'Flickr_8k.trainImages.txt').read_text().splitlines()\n",
    "  train_captions = [(str(fullpath/'Flicker8k_Dataset'/fname), cap_dict[fname]) for fname in train_files]\n",
    "\n",
    "  test_files = (fullpath/'Flickr_8k.testImages.txt').read_text().splitlines()\n",
    "  test_captions = [(str(fullpath/'Flicker8k_Dataset'/fname), cap_dict[fname]) for fname in test_files]\n",
    "\n",
    "  train_ds = tf.data.experimental.from_list(train_captions)\n",
    "  test_ds = tf.data.experimental.from_list(test_captions)\n",
    "\n",
    "  return train_ds, test_ds"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-22T19:49:31.019940200Z",
     "start_time": "2023-12-22T19:49:31.011451300Z"
    }
   },
   "id": "6313e6aa2e3b6298"
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [],
   "source": [
    "def conceptual_captions(*, data_dir=\"conceptual_captions\", num_train, num_val):\n",
    "  def iter_index(index_path):\n",
    "    with open(index_path) as f:\n",
    "      for line in f:\n",
    "        caption, url = line.strip().split('\\t')\n",
    "        yield caption, url\n",
    "\n",
    "  def download_image_urls(data_dir, urls):\n",
    "    ex = concurrent.futures.ThreadPoolExecutor(max_workers=100)\n",
    "    def save_image(url):\n",
    "      hash = hashlib.sha1(url.encode())\n",
    "      # Name the files after the hash of the URL.\n",
    "      file_path = data_dir/f'{hash.hexdigest()}.jpeg'\n",
    "      if file_path.exists():\n",
    "        # Only download each file once.\n",
    "        return file_path\n",
    "\n",
    "      try:\n",
    "        result = requests.get(url, timeout=5)\n",
    "      except Exception:\n",
    "        file_path = None\n",
    "      else:\n",
    "        file_path.write_bytes(result.content)\n",
    "      return file_path\n",
    "\n",
    "    result = []\n",
    "    out_paths = ex.map(save_image, urls)\n",
    "    for file_path in tqdm.tqdm(out_paths, total=len(urls)):\n",
    "      result.append(file_path)\n",
    "\n",
    "    return result\n",
    "\n",
    "  def ds_from_index_file(index_path, data_dir, count):\n",
    "    data_dir.mkdir(exist_ok=True)\n",
    "    index = list(itertools.islice(iter_index(index_path), count))\n",
    "    captions = [caption for caption, url in index]\n",
    "    urls = [url for caption, url in index]\n",
    "\n",
    "    paths = download_image_urls(data_dir, urls)\n",
    "\n",
    "    new_captions = []\n",
    "    new_paths = []\n",
    "    for cap, path in zip(captions, paths):\n",
    "      if path is None:\n",
    "        # Download failed, so skip this pair.\n",
    "        continue\n",
    "      new_captions.append(cap)\n",
    "      new_paths.append(path)\n",
    "\n",
    "    new_paths = [str(p) for p in new_paths]\n",
    "\n",
    "    ds = tf.data.Dataset.from_tensor_slices((new_paths, new_captions))\n",
    "    ds = ds.map(lambda path,cap: (path, cap[tf.newaxis])) # 1 caption per image\n",
    "    return ds\n",
    "\n",
    "  #data_dir = pathlib.Path(data_dir)\n",
    "  train_index_path = tf.keras.utils.get_file(\n",
    "    origin='https://storage.googleapis.com/gcc-data/Train/GCC-training.tsv',\n",
    "    cache_dir=local_data_path,\n",
    "    cache_subdir=data_dir\n",
    "    )\n",
    "\n",
    "  val_index_path = tf.keras.utils.get_file(\n",
    "    origin='https://storage.googleapis.com/gcc-data/Validation/GCC-1.1.0-Validation.tsv',\n",
    "    cache_dir=local_data_path,\n",
    "    cache_subdir=data_dir\n",
    "    )\n",
    "\n",
    "  fullpath = local_data_path+\"/\"+data_dir\n",
    "  train_raw = ds_from_index_file(train_index_path, data_dir=pathlib.Path(fullpath+'/train'), count=num_train)\n",
    "  test_raw = ds_from_index_file(val_index_path, data_dir=pathlib.Path(fullpath+'/val'), count=num_val)\n",
    "\n",
    "  return train_raw, test_raw"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-22T20:05:10.202629200Z",
     "start_time": "2023-12-22T20:05:10.172042800Z"
    }
   },
   "id": "7a8455f51e3d1493"
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "flickr8k\n",
      "C:/LocalResearch/JPD-Research/translationWork/data\n",
      "C:\\LocalResearch\\JPD-Research\\translationWork\\data\\flickr8k\\Flickr8k.token.txt\n"
     ]
    }
   ],
   "source": [
    "choose = 'flickr8k'\n",
    "\n",
    "if choose == 'flickr8k':\n",
    "  train_raw, test_raw = flickr8k()\n",
    "else:\n",
    "  train_raw, test_raw = conceptual_captions(num_train=10000, num_val=5000)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-22T20:12:34.211634400Z",
     "start_time": "2023-12-22T20:12:05.885977100Z"
    }
   },
   "id": "8ecce03698cde888"
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "outputs": [
    {
     "data": {
      "text/plain": "(TensorSpec(shape=(), dtype=tf.string, name=None),\n TensorSpec(shape=(5,), dtype=tf.string, name=None))"
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_raw.element_spec"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-22T20:12:48.286249Z",
     "start_time": "2023-12-22T20:12:48.280243600Z"
    }
   },
   "id": "5b51b2cde97533ad"
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(b'C:\\\\LocalResearch\\\\JPD-Research\\\\translationWork\\\\data\\\\flickr8k\\\\Flicker8k_Dataset\\\\2513260012_03d33305cf.jpg', shape=(), dtype=string)\n",
      "tf.Tensor(\n",
      "[b'A black dog is running after a white dog in the snow .'\n",
      " b'Black dog chasing brown dog through snow'\n",
      " b'Two dogs chase each other across the snowy ground .'\n",
      " b'Two dogs play together in the snow .'\n",
      " b'Two dogs running through a low lying body of water .'], shape=(5,), dtype=string)\n"
     ]
    }
   ],
   "source": [
    "for ex_path, ex_captions in train_raw.take(1):\n",
    "  print(ex_path)\n",
    "  print(ex_captions)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-22T20:12:53.050662800Z",
     "start_time": "2023-12-22T20:12:50.870567400Z"
    }
   },
   "id": "dc256207712d49ea"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### image feature extractor\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fcbb588d3239242b"
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/mobilenet_v3/weights_mobilenet_v3_small_224_1.0_float_no_top_v2.h5\n",
      "4334752/4334752 [==============================] - 1s 0us/step\n"
     ]
    }
   ],
   "source": [
    "IMAGE_SHAPE=(224, 224, 3)\n",
    "mobilenet = tf.keras.applications.MobileNetV3Small(\n",
    "    input_shape=IMAGE_SHAPE,\n",
    "    include_top=False,\n",
    "    include_preprocessing=True)\n",
    "mobilenet.trainable=False"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-23T00:48:15.466008400Z",
     "start_time": "2023-12-23T00:48:09.648114900Z"
    }
   },
   "id": "33aabddf26c5830f"
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "outputs": [],
   "source": [
    "def load_image(image_path):\n",
    "    img = tf.io.read_file(image_path)\n",
    "    img = tf.io.decode_jpeg(img, channels=3)\n",
    "    img = tf.image.resize(img, IMAGE_SHAPE[:-1])\n",
    "    return img"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-23T00:48:29.443493300Z",
     "start_time": "2023-12-23T00:48:29.375238900Z"
    }
   },
   "id": "49804c9199ab574e"
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 224, 224, 3)\n",
      "(1, 7, 7, 576)\n"
     ]
    }
   ],
   "source": [
    "test_img_batch = load_image(ex_path)[tf.newaxis, :]\n",
    "\n",
    "print(test_img_batch.shape)\n",
    "print(mobilenet(test_img_batch).shape)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-23T00:48:39.468665100Z",
     "start_time": "2023-12-23T00:48:38.938285400Z"
    }
   },
   "id": "99c76b2c14e163c4"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### setup text tokenizer/vectorizer"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c07acf92780c0e30"
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "outputs": [],
   "source": [
    "def standardize(s):\n",
    "  s = tf.strings.lower(s)\n",
    "  s = tf.strings.regex_replace(s, f'[{re.escape(string.punctuation)}]', '')\n",
    "  s = tf.strings.join(['[START]', s, '[END]'], separator=' ')\n",
    "  return s"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-23T00:49:18.655122700Z",
     "start_time": "2023-12-23T00:49:18.639044300Z"
    }
   },
   "id": "3fc82b400fbb986e"
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "outputs": [],
   "source": [
    "# Use the top 5000 words for a vocabulary.\n",
    "vocabulary_size = 5000\n",
    "tokenizer = tf.keras.layers.TextVectorization(\n",
    "    max_tokens=vocabulary_size,\n",
    "    standardize=standardize,\n",
    "    ragged=True)\n",
    "# Learn the vocabulary from the caption data."
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-23T00:49:21.371736800Z",
     "start_time": "2023-12-23T00:49:21.279134900Z"
    }
   },
   "id": "6ff8a9109a925d5c"
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "outputs": [],
   "source": [
    "tokenizer.adapt(train_raw.map(lambda fp,txt: txt).unbatch().batch(1024))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-23T00:49:34.289790600Z",
     "start_time": "2023-12-23T00:49:31.267795400Z"
    }
   },
   "id": "512631b8b31afa70"
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "outputs": [
    {
     "data": {
      "text/plain": "['', '[UNK]', 'a', '[START]', '[END]', 'in', 'the', 'on', 'is', 'and']"
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.get_vocabulary()[:10]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-23T00:49:42.530690500Z",
     "start_time": "2023-12-23T00:49:42.437664600Z"
    }
   },
   "id": "339550362f7e1e17"
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "outputs": [
    {
     "data": {
      "text/plain": "<tf.RaggedTensor [[3, 2, 655, 5, 2, 97, 4], [3, 2, 1937, 10, 4]]>"
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = tokenizer([['a cat in a hat'], ['a robot dog']])\n",
    "t"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-23T00:49:54.992360300Z",
     "start_time": "2023-12-23T00:49:54.870674900Z"
    }
   },
   "id": "8901312d3f264591"
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "outputs": [],
   "source": [
    "# Create mappings for words to indices and indices to words.\n",
    "word_to_index = tf.keras.layers.StringLookup(\n",
    "    mask_token=\"\",\n",
    "    vocabulary=tokenizer.get_vocabulary())\n",
    "index_to_word = tf.keras.layers.StringLookup(\n",
    "    mask_token=\"\",\n",
    "    vocabulary=tokenizer.get_vocabulary(),\n",
    "    invert=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-23T00:50:12.521244400Z",
     "start_time": "2023-12-23T00:50:12.407024500Z"
    }
   },
   "id": "f06e74845e75f31c"
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "outputs": [
    {
     "data": {
      "text/plain": "[[b'[START]', b'a', b'cat', b'in', b'a', b'hat', b'[END]'],\n [b'[START]', b'a', b'robot', b'dog', b'[END]']]"
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w = index_to_word(t)\n",
    "w.to_list()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-23T00:50:21.007901400Z",
     "start_time": "2023-12-23T00:50:20.861910500Z"
    }
   },
   "id": "7f8511e7202bfc5"
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "outputs": [
    {
     "data": {
      "text/plain": "array([b'[START] a cat in a hat [END]', b'[START] a robot dog [END]'],\n      dtype=object)"
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.strings.reduce_join(w, separator=' ', axis=-1).numpy()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-23T00:50:38.640215500Z",
     "start_time": "2023-12-23T00:50:38.508379100Z"
    }
   },
   "id": "95ede1b288c3a206"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### prepare datasets\n",
    "train_raw and test_raw contain 1:many (image, captions) pairs\n",
    "following function replicates the images so there are 1:1 images/captions"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "613a00631f6b9b49"
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "outputs": [],
   "source": [
    "def match_shapes(images, captions):\n",
    "  caption_shape = einops.parse_shape(captions, 'b c')\n",
    "  captions = einops.rearrange(captions, 'b c -> (b c)')\n",
    "  images = einops.repeat(\n",
    "      images, 'b ... -> (b c) ...',\n",
    "      c = caption_shape['c'])\n",
    "  return images, captions"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-23T00:52:17.803945200Z",
     "start_time": "2023-12-23T00:52:17.786189400Z"
    }
   },
   "id": "afc715cbddc8b988"
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image paths: (32,)\n",
      "captions: (32, 5)\n",
      "\n",
      "image_paths: (160,)\n",
      "captions: (160,)\n"
     ]
    }
   ],
   "source": [
    "for ex_paths, ex_captions in train_raw.batch(32).take(1):\n",
    "  break\n",
    "\n",
    "print('image paths:', ex_paths.shape)\n",
    "print('captions:', ex_captions.shape)\n",
    "print()\n",
    "\n",
    "ex_paths, ex_captions = match_shapes(images=ex_paths, captions=ex_captions)\n",
    "\n",
    "print('image_paths:', ex_paths.shape)\n",
    "print('captions:', ex_captions.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-23T00:52:26.359899400Z",
     "start_time": "2023-12-23T00:52:24.216965300Z"
    }
   },
   "id": "83f9793bc8e028b1"
  },
  {
   "cell_type": "markdown",
   "source": [
    "dataset should contain (inputs, labels) pairs.  For text generation the tokens are both an input and the labels shifted ibe step\n",
    "following function converts (image, text) pair to ((image, input tokens), label tokens) pair"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9e5aa6df48de54d7"
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "outputs": [],
   "source": [
    "def prepare_txt(imgs, txts):\n",
    "  tokens = tokenizer(txts)\n",
    "\n",
    "  input_tokens = tokens[..., :-1]\n",
    "  label_tokens = tokens[..., 1:]\n",
    "  return (imgs, input_tokens), label_tokens"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-23T00:52:52.641090900Z",
     "start_time": "2023-12-23T00:52:52.628030900Z"
    }
   },
   "id": "de6fa3b7fb7e6657"
  },
  {
   "cell_type": "markdown",
   "source": [
    "function adds operations to a dataset:\n",
    "1. load images (ignore those that don't load)\n",
    "2. replicate images to match number of captions\n",
    "3. shuffle/rebatch image, caption pairs\n",
    "4. tokenize the text, shift tokens and add label tokens\n",
    "5. convert text from raggedTensor to padded dense Tensor representations"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1df7b48fbddecae"
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "outputs": [],
   "source": [
    "def prepare_dataset(ds, tokenizer, batch_size=32, shuffle_buffer=1000):\n",
    "  # Load the images and make batches.\n",
    "  ds = (ds\n",
    "        .shuffle(10000)\n",
    "        .map(lambda path, caption: (load_image(path), caption))\n",
    "        .apply(tf.data.experimental.ignore_errors())\n",
    "        .batch(batch_size))\n",
    "\n",
    "  def to_tensor(inputs, labels):\n",
    "    (images, in_tok), out_tok = inputs, labels\n",
    "    return (images, in_tok.to_tensor()), out_tok.to_tensor()\n",
    "\n",
    "  return (ds\n",
    "          .map(match_shapes, tf.data.AUTOTUNE)\n",
    "          .unbatch()\n",
    "          .shuffle(shuffle_buffer)\n",
    "          .batch(batch_size)\n",
    "          .map(prepare_txt, tf.data.AUTOTUNE)\n",
    "          .map(to_tensor, tf.data.AUTOTUNE)\n",
    "          )"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-23T00:56:14.043737500Z",
     "start_time": "2023-12-23T00:56:14.031162500Z"
    }
   },
   "id": "e24d1a9552c9808f"
  },
  {
   "cell_type": "markdown",
   "source": [
    "install feature extractor and train on the datasets:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "48355d937d11a2c4"
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "outputs": [
    {
     "data": {
      "text/plain": "((TensorSpec(shape=(None, 224, 224, 3), dtype=tf.float32, name=None),\n  TensorSpec(shape=(None, None), dtype=tf.int64, name=None)),\n TensorSpec(shape=(None, None), dtype=tf.int64, name=None))"
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds = prepare_dataset(train_raw, tokenizer)\n",
    "train_ds.element_spec"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-23T00:57:22.826350300Z",
     "start_time": "2023-12-23T00:57:20.126168400Z"
    }
   },
   "id": "f0a0561787fe75dd"
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "outputs": [
    {
     "data": {
      "text/plain": "((TensorSpec(shape=(None, 224, 224, 3), dtype=tf.float32, name=None),\n  TensorSpec(shape=(None, None), dtype=tf.int64, name=None)),\n TensorSpec(shape=(None, None), dtype=tf.int64, name=None))"
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_ds = prepare_dataset(test_raw, tokenizer)\n",
    "test_ds.element_spec"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-23T00:57:31.679622600Z",
     "start_time": "2023-12-23T00:57:31.531557600Z"
    }
   },
   "id": "383a3bc8086d3523"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### cache the image features"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "553413ff8c23e009"
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "outputs": [],
   "source": [
    "def save_dataset(ds, save_path, image_model, tokenizer, shards=10, batch_size=32):\n",
    "  # Load the images and make batches.\n",
    "  ds = (ds\n",
    "        .map(lambda path, caption: (load_image(path), caption))\n",
    "        .apply(tf.data.experimental.ignore_errors())\n",
    "        .batch(batch_size))\n",
    "\n",
    "  # Run the feature extractor on each batch\n",
    "  # Don't do this in a .map, because tf.data runs on the CPU. \n",
    "  def gen():\n",
    "    for (images, captions) in tqdm.tqdm(ds): \n",
    "      feature_maps = image_model(images)\n",
    "\n",
    "      feature_maps, captions = match_shapes(feature_maps, captions)\n",
    "      yield feature_maps, captions\n",
    "\n",
    "  # Wrap the generator in a new tf.data.Dataset.\n",
    "  new_ds = tf.data.Dataset.from_generator(\n",
    "      gen,\n",
    "      output_signature=(\n",
    "          tf.TensorSpec(shape=image_model.output_shape),\n",
    "          tf.TensorSpec(shape=(None,), dtype=tf.string)))\n",
    "\n",
    "  # Apply the tokenization \n",
    "  new_ds = (new_ds\n",
    "            .map(prepare_txt, tf.data.AUTOTUNE)\n",
    "            .unbatch()\n",
    "            .shuffle(1000))\n",
    "\n",
    "  # Save the dataset into shard files.\n",
    "  def shard_func(i, item):\n",
    "    return i % shards\n",
    "  new_ds.enumerate().save(save_path, shard_func=shard_func)\n",
    "\n",
    "def load_dataset(save_path, batch_size=32, shuffle=1000, cycle_length=2):\n",
    "  def custom_reader_func(datasets):\n",
    "    datasets = datasets.shuffle(1000)\n",
    "    return datasets.interleave(lambda x: x, cycle_length=cycle_length)\n",
    "\n",
    "  ds = tf.data.Dataset.load(save_path, reader_func=custom_reader_func)\n",
    "\n",
    "  def drop_index(i, x):\n",
    "    return x\n",
    "\n",
    "  ds = (ds\n",
    "        .map(drop_index, tf.data.AUTOTUNE)\n",
    "        .shuffle(shuffle)\n",
    "        .padded_batch(batch_size)\n",
    "        .prefetch(tf.data.AUTOTUNE))\n",
    "  return ds"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-23T00:58:06.790661500Z",
     "start_time": "2023-12-23T00:58:06.767374700Z"
    }
   },
   "id": "637cbb4af640a602"
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "188it [02:14,  1.39it/s]\n",
      "32it [00:21,  1.47it/s]\n"
     ]
    }
   ],
   "source": [
    "save_dataset(train_raw, local_data_cache_path+'train_cache', mobilenet, tokenizer)\n",
    "save_dataset(test_raw, local_data_cache_path+'test_cache', mobilenet, tokenizer)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-23T01:03:51.279605200Z",
     "start_time": "2023-12-23T01:01:11.398809300Z"
    }
   },
   "id": "ac1371acdb4c294a"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Step 2 Data ready for training"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "dec562a3d52c88c0"
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "outputs": [],
   "source": [
    "train_ds = load_dataset(local_data_cache_path+'train_cache')\n",
    "test_ds = load_dataset(local_data_cache_path+'test_cache')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-23T01:08:46.510764300Z",
     "start_time": "2023-12-23T01:08:46.342956400Z"
    }
   },
   "id": "c2874e124b47a19c"
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "outputs": [
    {
     "data": {
      "text/plain": "((TensorSpec(shape=(None, 7, 7, 576), dtype=tf.float32, name=None),\n  TensorSpec(shape=(None, None), dtype=tf.int64, name=None)),\n TensorSpec(shape=(None, None), dtype=tf.int64, name=None))"
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds.element_spec"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-23T01:09:22.550553700Z",
     "start_time": "2023-12-23T01:09:22.306727800Z"
    }
   },
   "id": "81610b87293620cc"
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 7, 7, 576)\n",
      "(32, 20)\n",
      "(32, 20)\n"
     ]
    }
   ],
   "source": [
    "for (inputs, ex_labels) in train_ds.take(1):\n",
    "  (ex_img, ex_in_tok) = inputs\n",
    "\n",
    "print(ex_img.shape)\n",
    "print(ex_in_tok.shape)\n",
    "print(ex_labels.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-23T01:09:57.542284900Z",
     "start_time": "2023-12-23T01:09:55.960827200Z"
    }
   },
   "id": "debc4c5e65ec0320"
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  3   6 101 162  82   7   6 336 461   9 389   0   0   0   0   0   0   0\n",
      "   0   0]\n",
      "[  6 101 162  82   7   6 336 461   9 389   4   0   0   0   0   0   0   0\n",
      "   0   0]\n"
     ]
    }
   ],
   "source": [
    "print(ex_in_tok[0].numpy())\n",
    "print(ex_labels[0].numpy())"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-23T01:10:22.310653600Z",
     "start_time": "2023-12-23T01:10:22.049006200Z"
    }
   },
   "id": "eefbbda18574b831"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Step 3 A Transformer Decoder Model\n",
    "\n",
    "utilizes the pretrained input image encoder\n",
    "implements a 2-layer transformation decoder similar to that in the transformer notebook \n",
    "\n",
    "1. input - token embedding and positional encoding\n",
    "2. decoder - stack of decoder layers that contain:\n",
    "    - causal self-attention layer\n",
    "    - cross attention layer\n",
    "    - feed forward network\n",
    "3. output - multiclass-classification over output vocabulary\n",
    "\n",
    "### input\n",
    "unlike a CNN/RNN, the atttention layers are invariant to the sequence order - needs positional embedding  "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7a98e28380d4bddb"
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "outputs": [],
   "source": [
    "class SeqEmbedding(tf.keras.layers.Layer):\n",
    "  def __init__(self, vocab_size, max_length, depth):\n",
    "    super().__init__()\n",
    "    self.pos_embedding = tf.keras.layers.Embedding(input_dim=max_length, output_dim=depth)\n",
    "\n",
    "    self.token_embedding = tf.keras.layers.Embedding(\n",
    "        input_dim=vocab_size,\n",
    "        output_dim=depth,\n",
    "        mask_zero=True)\n",
    "\n",
    "    self.add = tf.keras.layers.Add()\n",
    "\n",
    "  def call(self, seq):\n",
    "    seq = self.token_embedding(seq) # (batch, seq, depth)\n",
    "\n",
    "    x = tf.range(tf.shape(seq)[1])  # (seq)\n",
    "    x = x[tf.newaxis, :]  # (1, seq)\n",
    "    x = self.pos_embedding(x)  # (1, seq, depth)\n",
    "\n",
    "    return self.add([seq,x])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-23T01:14:52.941751400Z",
     "start_time": "2023-12-23T01:14:52.709365400Z"
    }
   },
   "id": "fb177d3c8259ebfe"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### decoder\n",
    "as in the transformer notebook"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cb64c39be2dcb586"
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "outputs": [],
   "source": [
    "class CausalSelfAttention(tf.keras.layers.Layer):\n",
    "  def __init__(self, **kwargs):\n",
    "    super().__init__()\n",
    "    self.mha = tf.keras.layers.MultiHeadAttention(**kwargs)\n",
    "    # Use Add instead of + so the keras mask propagates through.\n",
    "    self.add = tf.keras.layers.Add() \n",
    "    self.layernorm = tf.keras.layers.LayerNormalization()\n",
    "\n",
    "  def call(self, x):\n",
    "    attn = self.mha(query=x, value=x,\n",
    "                    use_causal_mask=True)\n",
    "    x = self.add([x, attn])\n",
    "    return self.layernorm(x)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-23T01:15:59.284283600Z",
     "start_time": "2023-12-23T01:15:59.204343900Z"
    }
   },
   "id": "6c7693d06b55bb57"
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "outputs": [],
   "source": [
    "class CrossAttention(tf.keras.layers.Layer):\n",
    "  def __init__(self,**kwargs):\n",
    "    super().__init__()\n",
    "    self.mha = tf.keras.layers.MultiHeadAttention(**kwargs)\n",
    "    self.add = tf.keras.layers.Add() \n",
    "    self.layernorm = tf.keras.layers.LayerNormalization()\n",
    "\n",
    "  def call(self, x, y, **kwargs):\n",
    "    attn, attention_scores = self.mha(\n",
    "             query=x, value=y,\n",
    "             return_attention_scores=True)\n",
    "\n",
    "    self.last_attention_scores = attention_scores\n",
    "\n",
    "    x = self.add([x, attn])\n",
    "    return self.layernorm(x)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-23T01:16:08.749619Z",
     "start_time": "2023-12-23T01:16:08.731467800Z"
    }
   },
   "id": "ed35aeeb74b6f8e3"
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "outputs": [],
   "source": [
    "class FeedForward(tf.keras.layers.Layer):\n",
    "  def __init__(self, units, dropout_rate=0.1):\n",
    "    super().__init__()\n",
    "    self.seq = tf.keras.Sequential([\n",
    "        tf.keras.layers.Dense(units=2*units, activation='relu'),\n",
    "        tf.keras.layers.Dense(units=units),\n",
    "        tf.keras.layers.Dropout(rate=dropout_rate),\n",
    "    ])\n",
    "\n",
    "    self.layernorm = tf.keras.layers.LayerNormalization()\n",
    "\n",
    "  def call(self, x):\n",
    "    x = x + self.seq(x)\n",
    "    return self.layernorm(x)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-23T01:16:19.896666600Z",
     "start_time": "2023-12-23T01:16:19.734340Z"
    }
   },
   "id": "83db969d7c7ab66f"
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "outputs": [],
   "source": [
    "class DecoderLayer(tf.keras.layers.Layer):\n",
    "  def __init__(self, units, num_heads=1, dropout_rate=0.1):\n",
    "    super().__init__()\n",
    "\n",
    "    self.self_attention = CausalSelfAttention(num_heads=num_heads,\n",
    "                                              key_dim=units,\n",
    "                                              dropout=dropout_rate)\n",
    "    self.cross_attention = CrossAttention(num_heads=num_heads,\n",
    "                                          key_dim=units,\n",
    "                                          dropout=dropout_rate)\n",
    "    self.ff = FeedForward(units=units, dropout_rate=dropout_rate)\n",
    "\n",
    "\n",
    "  def call(self, inputs, training=False):\n",
    "    in_seq, out_seq = inputs\n",
    "\n",
    "    # Text input\n",
    "    out_seq = self.self_attention(out_seq)\n",
    "\n",
    "    out_seq = self.cross_attention(out_seq, in_seq)\n",
    "\n",
    "    self.last_attention_scores = self.cross_attention.last_attention_scores\n",
    "\n",
    "    out_seq = self.ff(out_seq)\n",
    "\n",
    "    return out_seq"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-23T01:16:30.950188700Z",
     "start_time": "2023-12-23T01:16:30.923968100Z"
    }
   },
   "id": "bbfc91206f6b4208"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### output"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8209f7e07bee49ee"
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "outputs": [],
   "source": [
    "class TokenOutput(tf.keras.layers.Layer):\n",
    "  def __init__(self, tokenizer, banned_tokens=('', '[UNK]', '[START]'), **kwargs):\n",
    "    super().__init__()\n",
    "\n",
    "    self.dense = tf.keras.layers.Dense(\n",
    "        units=tokenizer.vocabulary_size(), **kwargs)\n",
    "    self.tokenizer = tokenizer\n",
    "    self.banned_tokens = banned_tokens\n",
    "\n",
    "    self.bias = None\n",
    "\n",
    "  def adapt(self, ds):\n",
    "    counts = collections.Counter()\n",
    "    vocab_dict = {name: id \n",
    "                  for id, name in enumerate(self.tokenizer.get_vocabulary())}\n",
    "\n",
    "    for tokens in tqdm.tqdm(ds):\n",
    "      counts.update(tokens.numpy().flatten())\n",
    "\n",
    "    counts_arr = np.zeros(shape=(self.tokenizer.vocabulary_size(),))\n",
    "    counts_arr[np.array(list(counts.keys()), dtype=np.int32)] = list(counts.values())\n",
    "\n",
    "    counts_arr = counts_arr[:]\n",
    "    for token in self.banned_tokens:\n",
    "      counts_arr[vocab_dict[token]] = 0\n",
    "\n",
    "    total = counts_arr.sum()\n",
    "    p = counts_arr/total\n",
    "    p[counts_arr==0] = 1.0\n",
    "    log_p = np.log(p)  # log(1) == 0\n",
    "\n",
    "    entropy = -(log_p*p).sum()\n",
    "\n",
    "    print()\n",
    "    print(f\"Uniform entropy: {np.log(self.tokenizer.vocabulary_size()):0.2f}\")\n",
    "    print(f\"Marginal entropy: {entropy:0.2f}\")\n",
    "\n",
    "    self.bias = log_p\n",
    "    self.bias[counts_arr==0] = -1e9\n",
    "\n",
    "  def call(self, x):\n",
    "    x = self.dense(x)\n",
    "    # TODO(b/250038731): Fix this.\n",
    "    # An Add layer doesn't work because of the different shapes.\n",
    "    # This clears the mask, that's okay because it prevents keras from rescaling\n",
    "    # the losses.\n",
    "    return x + self.bias"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-23T01:18:01.470327Z",
     "start_time": "2023-12-23T01:18:01.444321700Z"
    }
   },
   "id": "b343381375cfee20"
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 938/938 [00:42<00:00, 22.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Uniform entropy: 8.52\n",
      "Marginal entropy: 5.29\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "output_layer = TokenOutput(tokenizer, banned_tokens=('', '[UNK]', '[START]'))\n",
    "# This might run a little faster if the dataset didn't also have to load the image data.\n",
    "output_layer.adapt(train_ds.map(lambda inputs, labels: labels))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-23T01:19:20.817943800Z",
     "start_time": "2023-12-23T01:18:38.196876400Z"
    }
   },
   "id": "bc2194f6394cf0d8"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### build the model\n",
    "\n",
    "1. feature extractor and text tokenizer\n",
    "2. sequence embedding layer to convert token IDs to vectors (batch, sequence, channels)\n",
    "3. stack of decoder layers\n",
    "4. output layer - pointwise prediction of next word"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4959dcb046bb07f"
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "outputs": [],
   "source": [
    "class Captioner(tf.keras.Model):\n",
    "  @classmethod\n",
    "  def add_method(cls, fun):\n",
    "    setattr(cls, fun.__name__, fun)\n",
    "    return fun\n",
    "\n",
    "  def __init__(self, tokenizer, feature_extractor, output_layer, num_layers=1,\n",
    "               units=256, max_length=50, num_heads=1, dropout_rate=0.1):\n",
    "    super().__init__()\n",
    "    self.feature_extractor = feature_extractor\n",
    "    self.tokenizer = tokenizer\n",
    "    self.word_to_index = tf.keras.layers.StringLookup(\n",
    "        mask_token=\"\",\n",
    "        vocabulary=tokenizer.get_vocabulary())\n",
    "    self.index_to_word = tf.keras.layers.StringLookup(\n",
    "        mask_token=\"\",\n",
    "        vocabulary=tokenizer.get_vocabulary(),\n",
    "        invert=True) \n",
    "\n",
    "    self.seq_embedding = SeqEmbedding(\n",
    "        vocab_size=tokenizer.vocabulary_size(),\n",
    "        depth=units,\n",
    "        max_length=max_length)\n",
    "\n",
    "    self.decoder_layers = [\n",
    "        DecoderLayer(units, num_heads=num_heads, dropout_rate=dropout_rate)\n",
    "        for n in range(num_layers)]\n",
    "\n",
    "    self.output_layer = output_layer"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-23T01:20:29.066306100Z",
     "start_time": "2023-12-23T01:20:28.910741800Z"
    }
   },
   "id": "4615740f4c8b9fbc"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
