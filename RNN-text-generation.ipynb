{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2113b7af313f668e",
   "metadata": {},
   "source": [
    "# Text Generation using Recurrent Neural Networks\n",
    " \n",
    "Based on Tensorflow tutorial [Text generation with an RNN](https://www.tensorflow.org/text/tutorials/text_generation)\n",
    "\n",
    "Text generation using Shakespeare dataset from [The Unreasonable Effectiveness of Recurrent Neural Networks](http://karpathy.github.io/2015/05/21/rnn-effectiveness/)\n",
    "\n",
    "With modifications described along the way...\n",
    "this runs on the GPU if available - about 55 min to train on CPU - <2 min on GPU\n",
    "\n",
    "## Step 0 - Environment Setup \n",
    "\n",
    "conda yaml for the environment compatible with all of these notebooks is included in the root of this repo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bb007a00562a6500",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-27T19:21:18.687105800Z",
     "start_time": "2023-12-27T19:21:13.398239400Z"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Physical GPUs, 1 Logical GPUs\n"
     ]
    }
   ],
   "source": [
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    # Restrict TensorFlow to only allocate 1GB of memory on the first GPU\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "            #tf.config.set_logical_device_configuration(\n",
    "        #    gpus[0],\n",
    "        #    [tf.config.LogicalDeviceConfiguration(memory_limit=512)])\n",
    "        logical_gpus = tf.config.list_logical_devices('GPU')\n",
    "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "    except RuntimeError as e:\n",
    "        # Virtual devices must be set before GPUs have been initialized\n",
    "        print(e)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-27T19:21:19.554910800Z",
     "start_time": "2023-12-27T19:21:19.536903400Z"
    }
   },
   "id": "69a203f23b1bdf8d"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fa1e278a6bb2ff21",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-27T19:21:19.591920800Z",
     "start_time": "2023-12-27T19:21:19.557907200Z"
    }
   },
   "outputs": [],
   "source": [
    "local_data_path_root = \"C:/LocalResearch/JPD-Research/translationWork\"\n",
    "local_data_path = local_data_path_root+ \"/data\"\n",
    "local_vocab_path = local_data_path_root+ \"/vocab/\"\n",
    "local_model_path = local_data_path_root+ \"/models/\"\n",
    "\n",
    "# Directory where the checkpoints will be saved\n",
    "checkpoint_dir = local_data_path_root + '/training_checkpoints'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1927d862267744c0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-27T19:21:19.663429200Z",
     "start_time": "2023-12-27T19:21:19.571922300Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of text: 1115394 characters\n",
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you know Caius Marcius is chief enemy to the people.\n",
      "\n",
      "65 unique characters\n"
     ]
    }
   ],
   "source": [
    "path_to_file = tf.keras.utils.get_file('shakespeare.txt', 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt', cache_subdir=local_data_path)\n",
    "\n",
    "# Read, then decode \n",
    "text = open(path_to_file, 'rb').read().decode(encoding='utf-8')\n",
    "# length of text is the number of characters in it\n",
    "print(f'Length of text: {len(text)} characters')\n",
    "print(text[:250])\n",
    "\n",
    "vocab = sorted(set(text))\n",
    "print(f'{len(vocab)} unique characters')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e30eac3991c93dbd",
   "metadata": {},
   "source": [
    "get dataset and validate:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ded04fc3f32bf47e",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Step 1 - Preprocess the data \n",
    "with the dataset loaded, we need to vectorize it\n",
    "characters can be turned into numeric IDs, once the text is split into tokens\n",
    "\n",
    "and the tokens are turned into character IDs using a StringLookup layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e04a1da10eed1caf",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-27T19:21:19.744946100Z",
     "start_time": "2023-12-27T19:21:19.666927900Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.RaggedTensor [[b'a', b'b', b'c', b'd', b'e', b'f', b'g'], [b'x', b'y', b'z']]>\n",
      "<tf.RaggedTensor [[40, 41, 42, 43, 44, 45, 46], [63, 64, 65]]>\n"
     ]
    }
   ],
   "source": [
    "example_texts = ['abcdefg', 'xyz']\n",
    "\n",
    "chars = tf.strings.unicode_split(example_texts, input_encoding='UTF-8')\n",
    "print(chars)\n",
    "\n",
    "ids_from_chars = tf.keras.layers.StringLookup(\n",
    "    vocabulary=list(vocab), mask_token=None)\n",
    "\n",
    "ids = ids_from_chars(chars)\n",
    "print(ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "379f715481536887",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "for generation, this needs to be able to be inverted, recovering strings from IDs, using the same LookupLayer\n",
    "and these can be joined back into strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5dd54f052683de10",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-27T19:21:19.834470500Z",
     "start_time": "2023-12-27T19:21:19.744946100Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.RaggedTensor [[b'a', b'b', b'c', b'd', b'e', b'f', b'g'], [b'x', b'y', b'z']]>\n",
      "[b'abcdefg' b'xyz']\n",
      "tf.Tensor([b'abcdefg' b'xyz'], shape=(2,), dtype=string)\n",
      "[b'abcdefg' b'xyz']\n"
     ]
    }
   ],
   "source": [
    "chars_from_ids = tf.keras.layers.StringLookup(\n",
    "    vocabulary=ids_from_chars.get_vocabulary(), invert=True, mask_token=None)\n",
    "chars = chars_from_ids(ids)\n",
    "print(chars)\n",
    "\n",
    "print(tf.strings.reduce_join(chars, axis=-1).numpy())\n",
    "\n",
    "def text_from_ids(ids):\n",
    "  return tf.strings.reduce_join(chars_from_ids(ids), axis=-1)\n",
    "\n",
    "print( text_from_ids(ids))\n",
    "print( text_from_ids(ids).numpy() )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48756f5780bd892d",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Task definition - to determine the next likely character, given a character or sequence\n",
    "RNNs maintain state dependent on prior seen elements, use that state to predict the next character\n",
    "\n",
    "Split set into training example sequences, each of seq_length length\n",
    "each sequence predicts the seq_length+1 character"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ce577295113ca2df",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-27T19:21:20.164946500Z",
     "start_time": "2023-12-27T19:21:19.836977600Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([19 48 57 ... 46  9  1], shape=(1115394,), dtype=int64)\n",
      "F\n",
      "i\n",
      "r\n",
      "s\n",
      "t\n",
      " \n",
      "C\n",
      "i\n",
      "t\n",
      "i\n"
     ]
    }
   ],
   "source": [
    "all_ids = ids_from_chars(tf.strings.unicode_split(text, 'UTF-8'))\n",
    "print(all_ids)\n",
    "ids_dataset = tf.data.Dataset.from_tensor_slices(all_ids)\n",
    "for ids in ids_dataset.take(10):\n",
    "    print(chars_from_ids(ids).numpy().decode('utf-8'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6ffd09490941b024",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-27T19:21:20.176446300Z",
     "start_time": "2023-12-27T19:21:20.162444400Z"
    }
   },
   "outputs": [],
   "source": [
    "seq_length = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b6fc3dfab2f770f",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "use batch to generate appropriate sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f813ebe9440fbe47",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-27T19:21:20.222449500Z",
     "start_time": "2023-12-27T19:21:20.180444800Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[b'F' b'i' b'r' b's' b't' b' ' b'C' b'i' b't' b'i' b'z' b'e' b'n' b':'\n",
      " b'\\n' b'B' b'e' b'f' b'o' b'r' b'e' b' ' b'w' b'e' b' ' b'p' b'r' b'o'\n",
      " b'c' b'e' b'e' b'd' b' ' b'a' b'n' b'y' b' ' b'f' b'u' b'r' b't' b'h'\n",
      " b'e' b'r' b',' b' ' b'h' b'e' b'a' b'r' b' ' b'm' b'e' b' ' b's' b'p'\n",
      " b'e' b'a' b'k' b'.' b'\\n' b'\\n' b'A' b'l' b'l' b':' b'\\n' b'S' b'p' b'e'\n",
      " b'a' b'k' b',' b' ' b's' b'p' b'e' b'a' b'k' b'.' b'\\n' b'\\n' b'F' b'i'\n",
      " b'r' b's' b't' b' ' b'C' b'i' b't' b'i' b'z' b'e' b'n' b':' b'\\n' b'Y'\n",
      " b'o' b'u' b' '], shape=(101,), dtype=string)\n",
      "tf.Tensor(b'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou ', shape=(), dtype=string)\n",
      "tf.Tensor(b'are all resolved rather to die than to famish?\\n\\nAll:\\nResolved. resolved.\\n\\nFirst Citizen:\\nFirst, you k', shape=(), dtype=string)\n",
      "tf.Tensor(b\"now Caius Marcius is chief enemy to the people.\\n\\nAll:\\nWe know't, we know't.\\n\\nFirst Citizen:\\nLet us ki\", shape=(), dtype=string)\n",
      "tf.Tensor(b\"ll him, and we'll have corn at our own price.\\nIs't a verdict?\\n\\nAll:\\nNo more talking on't; let it be d\", shape=(), dtype=string)\n",
      "tf.Tensor(b'one: away, away!\\n\\nSecond Citizen:\\nOne word, good citizens.\\n\\nFirst Citizen:\\nWe are accounted poor citi', shape=(), dtype=string)\n"
     ]
    }
   ],
   "source": [
    "sequences = ids_dataset.batch(seq_length+1, drop_remainder=True)\n",
    "\n",
    "for seq in sequences.take(1):\n",
    "  print(chars_from_ids(seq))  \n",
    "for seq in sequences.take(5):\n",
    "  print(text_from_ids(seq))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63f29587c8c80c07",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "these sequences need to be turned into input/label sets\n",
    "for each step, the input is the current character, and the label is the next character"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e1ba20d5db37af4a",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-27T19:21:20.273995500Z",
     "start_time": "2023-12-27T19:21:20.224944900Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(['t', 'e', 'n', 's', 'o', 'r', 'f', 'l', 'o'], ['e', 'n', 's', 'o', 'r', 'f', 'l', 'o', 'w'])\n"
     ]
    }
   ],
   "source": [
    "def split_input_target(sequence):\n",
    "    input_text = sequence[:-1]\n",
    "    target_text = sequence[1:]\n",
    "    return input_text, target_text\n",
    "\n",
    "print( split_input_target(list(\"tensorflow\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "32cd8928f818c438",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-27T19:21:20.322965500Z",
     "start_time": "2023-12-27T19:21:20.241943900Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input : b'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou'\n",
      "Target: b'irst Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n"
     ]
    }
   ],
   "source": [
    "dataset = sequences.map(split_input_target)\n",
    "\n",
    "for input_example, target_example in dataset.take(1):\n",
    "    print(\"Input :\", text_from_ids(input_example).numpy())\n",
    "    print(\"Target:\", text_from_ids(target_example).numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5af8884960cb0015",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "with these sequences, we need to pack these into training batches we can use with the model\n",
    "note that not all data is pulled into memory at once using the batch and buffer size to manage this transition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "93feefea03f62c09",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-27T19:21:20.385670300Z",
     "start_time": "2023-12-27T19:21:20.318965700Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<PrefetchDataset element_spec=(TensorSpec(shape=(64, 100), dtype=tf.int64, name=None), TensorSpec(shape=(64, 100), dtype=tf.int64, name=None))>\n"
     ]
    }
   ],
   "source": [
    "# Batch size\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "# Buffer size to shuffle the dataset\n",
    "# (TF data is designed to work with possibly infinite sequences,\n",
    "# so it doesn't attempt to shuffle the entire sequence in memory. Instead,\n",
    "# it maintains a buffer in which it shuffles elements).\n",
    "BUFFER_SIZE = 10000\n",
    "\n",
    "dataset = (\n",
    "    dataset\n",
    "    .shuffle(BUFFER_SIZE)\n",
    "    .batch(BATCH_SIZE, drop_remainder=True)\n",
    "    .prefetch(tf.data.experimental.AUTOTUNE))\n",
    "\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8713c78a6632479",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Step 3 Model generation\n",
    "\n",
    "This Keras.Model implementation has three layers\n",
    "\n",
    "| layer name | function                             |\n",
    "|------------|--------------------------------------|\n",
    "| Embedding  | input layer                          |\n",
    "| GRU        | RNN with input size units=rnn_units  |\n",
    "| Dense      | output layer with vocab_size outputs |\n",
    " outputs are the log-likelihood of each character in the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7b219313337c3c7d",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-27T19:21:20.387170100Z",
     "start_time": "2023-12-27T19:21:20.350086400Z"
    }
   },
   "outputs": [],
   "source": [
    "# Length of the vocabulary in StringLookup Layer\n",
    "vocab_size = len(ids_from_chars.get_vocabulary())\n",
    "\n",
    "# The embedding dimension\n",
    "embedding_dim = 256\n",
    "\n",
    "# Number of RNN units\n",
    "rnn_units = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b7b6f7029d1dd3ac",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-27T19:21:20.414171100Z",
     "start_time": "2023-12-27T19:21:20.369142400Z"
    }
   },
   "outputs": [],
   "source": [
    "class MyModel(tf.keras.Model):\n",
    "  def __init__(self, vocab_size, embedding_dim, rnn_units):\n",
    "    super().__init__(self)\n",
    "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "    self.gru = tf.keras.layers.GRU(rnn_units,\n",
    "                                   return_sequences=True,\n",
    "                                   return_state=True)\n",
    "    self.dense = tf.keras.layers.Dense(vocab_size)\n",
    "\n",
    "  def call(self, inputs, states=None, return_state=False, training=False):\n",
    "    x = inputs\n",
    "    x = self.embedding(x, training=training)\n",
    "    if states is None:\n",
    "      states = self.gru.get_initial_state(x)\n",
    "    x, states = self.gru(x, initial_state=states, training=training)\n",
    "    x = self.dense(x, training=training)\n",
    "\n",
    "    if return_state:\n",
    "      return x, states\n",
    "    else:\n",
    "      return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "10bbdceb577233a5",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-27T19:21:20.418169800Z",
     "start_time": "2023-12-27T19:21:20.381677200Z"
    }
   },
   "outputs": [],
   "source": [
    "model = MyModel(\n",
    "    vocab_size=vocab_size,\n",
    "    embedding_dim=embedding_dim,\n",
    "    rnn_units=rnn_units)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdb7e64aad89531b",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Testing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e7171fd6993fb1",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-27T19:21:27.851010500Z",
     "start_time": "2023-12-27T19:21:20.411171900Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 100, 66) # (batch_size, sequence_length, vocab_size)\n",
      "Model: \"my_model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       multiple                  16896     \n",
      "                                                                 \n",
      " gru (GRU)                   multiple                  3938304   \n",
      "                                                                 \n",
      " dense (Dense)               multiple                  67650     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4,022,850\n",
      "Trainable params: 4,022,850\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "for input_example_batch, target_example_batch in dataset.take(1):\n",
    "    example_batch_predictions = model(input_example_batch)\n",
    "    print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaa784675224a1f4",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "to actually predict, you need to process the logits returned over the character vocabulary\n",
    "for the first example (encoded and decoded):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ef0efa1f97edd01d",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-27T19:21:27.890072600Z",
     "start_time": "2023-12-27T19:21:27.850510500Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[50 54 42 37 30 26  7 58 64 63 18 33  2 37  4 31 37 35 59 52 58 55 60  0\n",
      " 46 12 42 16 45  6 43 28  9 22 39 24 57 19  5 36 45 34 19 40 44 35 54 60\n",
      " 19 10 36 48  6 45 64 39 23 65 34 37 36 21 62 49 24 18  7 21 36 65 49 39\n",
      " 35  3 55 12 60 56 28 13 34 38  0 53 28  0  4 62  7 47 55 64 31 38 39 22\n",
      " 17 43 26 55]\n",
      "Input:\n",
      " b\"ength, and make us think\\nRather our state's defective for requital\\nThan we to stretch it out.\\nMaster\"\n",
      "\n",
      "Next Char Predictions:\n",
      " b\"kocXQM,syxET X$RXVtmspu[UNK]g;cCf'dO.IZKrF&WfUFaeVouF3Wi'fyZJzUXWHwjKE,HWzjZV!p;uqO?UY[UNK]nO[UNK]$w,hpyRYZIDdMp\"\n"
     ]
    }
   ],
   "source": [
    "sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)\n",
    "sampled_indices = tf.squeeze(sampled_indices, axis=-1).numpy()\n",
    "print(sampled_indices)\n",
    "print(\"Input:\\n\", text_from_ids(input_example_batch[0]).numpy())\n",
    "print()\n",
    "print(\"Next Char Predictions:\\n\", text_from_ids(sampled_indices).numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e979f764cb424f57",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Step 4 - Model Training\n",
    "\n",
    "We've turned prediction into a simple classification problem - given the prior state, predict the class of the next character\n",
    "We need an optimizer and loss function\n",
    "- the crossentropy loss function is reasonable in this case?\n",
    "- use the 'Adam' optimizer\n",
    "- use Compile to comfigure the training process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4b97396b8948378f",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-27T19:21:27.938056700Z",
     "start_time": "2023-12-27T19:21:27.877537600Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction shape:  (64, 100, 66)  # (batch_size, sequence_length, vocab_size)\n",
      "Mean loss:         tf.Tensor(4.1899037, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "loss = tf.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "example_batch_mean_loss = loss(target_example_batch, example_batch_predictions)\n",
    "print(\"Prediction shape: \", example_batch_predictions.shape, \" # (batch_size, sequence_length, vocab_size)\")\n",
    "print(\"Mean loss:        \", example_batch_mean_loss)\n",
    "\n",
    "model.compile(optimizer='adam', loss=loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ead04c6b21128780",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "prior to training, the mean loss should be equal to the vocabulary size - even higher values means the model us certain of its wrong answers..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "da245f657adda1ec",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-27T19:21:27.991632900Z",
     "start_time": "2023-12-27T19:21:27.924533800Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66.01643\n"
     ]
    }
   ],
   "source": [
    "print(tf.exp(example_batch_mean_loss).numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0ece1323f350fc1",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "configure checkpoints, and train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "45f9f66163cab996",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-27T19:21:27.993600300Z",
     "start_time": "2023-12-27T19:21:27.956045600Z"
    }
   },
   "outputs": [],
   "source": [
    "# Name of the checkpoint files\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
    "\n",
    "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_prefix,\n",
    "    save_weights_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5ba8df5f55990c0",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-27T19:23:14.484173900Z",
     "start_time": "2023-12-27T19:21:27.970594500Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "172/172 [==============================] - 7s 24ms/step - loss: 2.7258\n",
      "Epoch 2/20\n",
      "172/172 [==============================] - 5s 25ms/step - loss: 1.9877\n",
      "Epoch 3/20\n",
      "172/172 [==============================] - 5s 26ms/step - loss: 1.7109\n",
      "Epoch 4/20\n",
      "172/172 [==============================] - 5s 24ms/step - loss: 1.5509\n",
      "Epoch 5/20\n",
      "172/172 [==============================] - 5s 24ms/step - loss: 1.4530\n",
      "Epoch 6/20\n",
      "172/172 [==============================] - 5s 24ms/step - loss: 1.3844\n",
      "Epoch 7/20\n",
      "172/172 [==============================] - 5s 25ms/step - loss: 1.3320\n",
      "Epoch 8/20\n",
      "172/172 [==============================] - 6s 25ms/step - loss: 1.2866\n",
      "Epoch 9/20\n",
      "172/172 [==============================] - 5s 25ms/step - loss: 1.2462\n",
      "Epoch 10/20\n",
      "172/172 [==============================] - 5s 25ms/step - loss: 1.2072\n",
      "Epoch 11/20\n",
      "172/172 [==============================] - 5s 26ms/step - loss: 1.1668\n",
      "Epoch 12/20\n",
      "172/172 [==============================] - 5s 25ms/step - loss: 1.1260\n",
      "Epoch 13/20\n",
      "172/172 [==============================] - 5s 25ms/step - loss: 1.0826\n",
      "Epoch 14/20\n",
      "172/172 [==============================] - 5s 25ms/step - loss: 1.0375\n",
      "Epoch 15/20\n",
      "172/172 [==============================] - 5s 25ms/step - loss: 0.9889\n",
      "Epoch 16/20\n",
      "172/172 [==============================] - 5s 25ms/step - loss: 0.9372\n",
      "Epoch 17/20\n",
      "172/172 [==============================] - 5s 25ms/step - loss: 0.8851\n",
      "Epoch 18/20\n",
      "172/172 [==============================] - 5s 26ms/step - loss: 0.8332\n",
      "Epoch 19/20\n",
      "172/172 [==============================] - 5s 25ms/step - loss: 0.7820\n",
      "Epoch 20/20\n",
      "172/172 [==============================] - 5s 25ms/step - loss: 0.7330\n",
      "<keras.callbacks.History object at 0x000001E71227EE60>\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 20\n",
    "history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])\n",
    "print(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca34575165428d3e",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Step 5 Text Generation\n",
    "\n",
    "can use a loop that passes in a propmpt, adds the generated output and passes that back in\n",
    "\n",
    "We'll define a class that takes the model and char <--> id functions as inputs\n",
    "- this class masks the input such that [UNK] won't be generated \n",
    "- then the function that \n",
    "    - tokenizees the current inputs\n",
    "    - runs the model on the input\n",
    "    - gets the last prediction\n",
    "    - samples the last prediction\n",
    "    - char-izes the resulting output\n",
    "    - returns the updated result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "129ec09c044c857a",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-27T19:23:14.504673700Z",
     "start_time": "2023-12-27T19:23:14.491673600Z"
    }
   },
   "outputs": [],
   "source": [
    "class OneStep(tf.keras.Model):\n",
    "  def __init__(self, model, chars_from_ids, ids_from_chars, temperature=1.0):\n",
    "    super().__init__()\n",
    "    self.temperature = temperature\n",
    "    self.model = model\n",
    "    self.chars_from_ids = chars_from_ids\n",
    "    self.ids_from_chars = ids_from_chars\n",
    "\n",
    "    # Create a mask to prevent \"[UNK]\" from being generated.\n",
    "    skip_ids = self.ids_from_chars(['[UNK]'])[:, None]\n",
    "    sparse_mask = tf.SparseTensor(\n",
    "        # Put a -inf at each bad index.\n",
    "        values=[-float('inf')]*len(skip_ids),\n",
    "        indices=skip_ids,\n",
    "        # Match the shape to the vocabulary\n",
    "        dense_shape=[len(ids_from_chars.get_vocabulary())])\n",
    "    self.prediction_mask = tf.sparse.to_dense(sparse_mask)\n",
    "\n",
    "  @tf.function\n",
    "  def generate_one_step(self, inputs, states=None):\n",
    "    # Convert strings to token IDs.\n",
    "    input_chars = tf.strings.unicode_split(inputs, 'UTF-8')\n",
    "    input_ids = self.ids_from_chars(input_chars).to_tensor()\n",
    "\n",
    "    # Run the model.\n",
    "    # predicted_logits.shape is [batch, char, next_char_logits]\n",
    "    predicted_logits, states = self.model(inputs=input_ids, states=states,\n",
    "                                          return_state=True)\n",
    "    # Only use the last prediction.\n",
    "    predicted_logits = predicted_logits[:, -1, :]\n",
    "    predicted_logits = predicted_logits/self.temperature\n",
    "    # Apply the prediction mask: prevent \"[UNK]\" from being generated.\n",
    "    predicted_logits = predicted_logits + self.prediction_mask\n",
    "\n",
    "    # Sample the output logits to generate token IDs.\n",
    "    predicted_ids = tf.random.categorical(predicted_logits, num_samples=1)\n",
    "    predicted_ids = tf.squeeze(predicted_ids, axis=-1)\n",
    "\n",
    "    # Convert from token ids to characters\n",
    "    predicted_chars = self.chars_from_ids(predicted_ids)\n",
    "\n",
    "    # Return the characters and model state.\n",
    "    return predicted_chars, states"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "616ccba8734af7e3",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "now define an instance of our generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b9c65170b9cae4e0",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-27T19:23:14.535185100Z",
     "start_time": "2023-12-27T19:23:14.502674Z"
    }
   },
   "outputs": [],
   "source": [
    "one_step_model = OneStep(model, chars_from_ids, ids_from_chars)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ebc75c68a1635eb",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "the generation loop:\n",
    "- defines a constant text of \"ROMEO:\" \n",
    "- initialize our result (state) to this constant \n",
    "- iterates 1000 times\n",
    "    - calling our function to get the next char (and states)\n",
    "    - appends the next char to \"result\"\n",
    "- finally, the output result characters are joined together\n",
    "- and the result (and runtime) are printed out  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fe91809392697f1c",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-27T19:23:17.816131800Z",
     "start_time": "2023-12-27T19:23:14.534188700Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "________________________________________________________________________________\n",
      "\n",
      "ROMEO:\n",
      "I'll lay what may be made to execute, I cannot\n",
      "I will watch you Ever tame not a dozen yet\n",
      "To raise their fill assure to you: prithee, wert\n",
      "they not.\n",
      "\n",
      "DUKE VINCENTIO:\n",
      "\n",
      "ANGELO:\n",
      "You did I to take allowing must fall on: the\n",
      "retting new-pleasant cesses and enter\n",
      "And take our side.\n",
      "\n",
      "Sets, God pregnany,\n",
      "Icspaid to be talked on their pipes, upon the hell,\n",
      "For dot thy king by Borimous lordship?\n",
      "She shall not knew not whether to decrow afact,\n",
      "I parmit you in Valona, liop us well.\n",
      "But what said Berketiona, the King Pronsey?\n",
      "\n",
      "ABHORSOL:\n",
      "Cousin, no; good sir God hate your cats, '\n",
      "you.\n",
      "\n",
      "\n",
      "AMIELO:\n",
      "Mistress Open made in Corioli Aught way.\n",
      "\n",
      "LUCENTIO:\n",
      "Apollo's absence, of you. My lord, your war\n",
      "Unto from all.\n",
      "\n",
      "MENENIUS:\n",
      "I hear our furlt: you, let it would forget.\n",
      "Nay, most widow, not ungearsh their affects my husband-I\n",
      "Have look'd it, and for green many pacester;\n",
      "And Barnardine i' the valour with honesty.\n",
      "\n",
      "CAMILLO:\n",
      "It was the stronger pathers.\n",
      "\n",
      "GONZALO:\n",
      "Where is Buckingham? Let's not answer to.\n",
      "\n",
      "EMER:\n",
      "O  \n",
      "\n",
      "________________________________________________________________________________\n",
      "\n",
      "Run time: 3.2719810009002686\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "states = None\n",
    "next_char = tf.constant(['ROMEO:'])\n",
    "result = [next_char]\n",
    "\n",
    "for n in range(1000):\n",
    "  next_char, states = one_step_model.generate_one_step(next_char, states=states)\n",
    "  result.append(next_char)\n",
    "\n",
    "result = tf.strings.join(result)\n",
    "end = time.time()\n",
    "print('_'*80 + '\\n\\n' + result[0].numpy().decode('utf-8'), '\\n\\n' + '_'*80)\n",
    "print('\\nRun time:', end - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c89dd896a5f29a7",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### things to note:\n",
    "- this model is character-based, so it doesn't really know about words, \n",
    "- it does know when to capitalize, insert paragraph breaks, emulate the Shakespearean vocab from the vocabulary\n",
    "- it has not yet learned to generate coherent sentences\n",
    "    - larger number of epochs may improve this\n",
    "\n",
    "### things to experiment with\n",
    "- the \"temperature\" parameter can be used to make more/less random predictions\n",
    "- starting with a different seed string will change the output\n",
    "- adding another RNN layer can improve its overall accuracy\n",
    "- generation speed can be improved by batching (below code runs in similar time to above code) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7bae3e60975dd545",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-27T19:23:21.351237400Z",
     "start_time": "2023-12-27T19:23:17.823639900Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[b\"ROMEO:\\nI give your lord-she comes\\nBut when Lucentio still lives.\\n\\nPETRUCHIO:\\nHarry, my lord, let love is done,\\nAnd to deny is harfful against his armeluse,\\nSweet priestome want: but one that issue out of your\\ndiscover back that so if but rans' entire,\\nForting all husband: and tretcheding us all:\\nSo further pleasure hit a thief should be door\\ncrown'd, uneven in their chims,\\nLike to a best to quench it, by some levies more than\\nUnstoward's rights, that from thy lights for her:\\nMyself no less than vain would since for solf your spirit,\\nWhich, by despite of us, and leave out of\\nthe love.\\n\\nISABELLA:\\nI say should I had virtuous match should pins\\nOur backs; and to say that I think,\\nHave thus the heart to do it every pass\\nOf peeping bodd, to see the wind.\\n\\nSecond Citizen:\\nWithdraw thee, my good lord? She will not bring him.\\nGo to London with usurping back!\\nStanl your own desert she shall not persuade,\\nAnd nothing of your fifther kingness: ports\\nOf much up rusher to encounter man:\\nPerhaps you may don\"\n",
      " b\"ROMEO:\\nI'll have no joys, machers! cannot thit, be curtain,\\nThat England's thy free speechlass, look to his bed,\\nAnd samisf confers for you to be one of it,\\nBreathe up in passion made thy embracement.\\nShe is your will?\\n\\nISABELLA:\\nI do learn afford the torturers, late,\\nhere's that shall shed me But in the end:\\nHaving death did, put to death, upon your company.\\n\\nKING RICHARD III:\\nRatake is full of her you soughty:\\nAll bid them lords, take leave to knowledge. Gear war,\\nThou wast true, you now, sir, an your voices; neither\\nsince the turned staff afaments of our love army:\\nTill thou hilts myself a forward\\nhouse to London match; and therefore be deposed\\nYour woman's grace am about to keep a good disgrace.\\n\\nKING RICHARD III:\\nYou are torned to thee.\\n\\nROMEO:\\nO, that, since it is, I have pierced to lie alike\\nAs it your holy hundsmen forbid;\\nAnd say, what men, you should to hear it?\\nHarry of Hereford! what dog thee read?\\n\\nDUCHESS OF YORK:\\nI'll go alone to my lord; 'tis a matter\\nBy her became a bloson's\"\n",
      " b\"ROMEO:\\nI say, he doth in spite of all sheep;\\nAnd she, together, for I do fetch the king\\nIf pathemption: let him be very so.\\n\\nMANIA:\\nOf all this way. What's within, change it is, Iralts to flatter'd you I wish you.\\n\\nANGELO:\\nIn wooned thought I had slain in death,\\n'Romeo, buy, that hath a party from the current.\\n\\nMISTRESS OVERDONE:\\nWas I from thy heaving this my ungle seat,\\nAnd bring you gain in peace, the empty hope,\\nMy faith is going. To her once more too late, and to Lond\\nThat you shouldst take the palace may proceed.\\n\\nPERDITA:\\nI pray thee, masters,\\nFie: you shall speak for me the lord poor king,\\nAnd each interrupts her this day.\\n\\nBALTHA:\\nAre they then less in actiff ald!\\n\\nSecond Citizen:\\nWhat ill in this ready strides him swell sure, why they late\\nIn the sun upon't. But instant glory\\nThan canon'st town, or you rogue and so great,\\nAnd she shall not be amied; therefore be he.\\n\\nQUEEN ELIZABETH:\\nFlatter's head o'er-keep with which affection evil,\\nHis curse is dead, your father Gremie,\\nAre full\"\n",
      " b\"ROMEO:\\n\\nFirst Servingman:\\nWhat will you guilty?\\n\\nISABELLA:\\nI'll speak a while! O fortune will be done!\\nGood grief and sullent, gulls our packs,\\nyields did scare that thought their love and long\\nfaint, which, God prepare to meet the ground,\\nAnd lord of York, who should here have and post;\\nAnd yet, to speak, I will take my death. But what\\ntalk we of that respect, husband and I do enjoy,\\nI cannot among by eyes and ears\\nWere stand betwixt us all enough afore his part.\\n\\nBRUTUS:\\nWhen very well for hate, Gried Lord Hastings, I am they\\nlack'd point to do us both.\\n\\nCLAUDIO:\\nBut 'tis no lamentatio.\\n\\nCATESBY:\\nHe for 'tis called Katharina. Come, Now, Much of it\\nis: but what's the news i' the law,\\nThe night-owes for Richard, Efery in your air\\nAnd brought do fall be suffer'd ruinhing stone:\\nFor though unhappy, when it doth he comes,\\nBoth sorrow for beiser means to cross the just.\\n\\nKATHARINA:\\nI prithee go slight. CAPULET:\\nFear me, my lord, we will do't\\nCutsourfeit to be it short, and strength of yours;\\nLet\"\n",
      " b\"ROMEO:\\nI prophesy myself, and reconciled high blessed\\nThe abbeg of uffeir maids: What counsel not?\\nO, be a brother mark thee hearing of honour,\\nThou colour'd to him, or to be contrain'd that I,\\nBehold, these woes were in unaccides' grey;\\nOr breaking of it at your disand,\\nBlush'd about your country's face, so sight may love,\\nFor it is well poor infressed with the earth,\\nLike to my cousin knows! Masters of his lamenta,\\nI shy the friends there in any what entreats,\\nLet me unkink about my fault, and that she is ours.\\n\\nKING EDWARD IV:\\nSweet sail! For then we looked for, that none of yout\\nhonour'd rash fill'd by humself, friends, with heavy mights,\\nSet in every man that shall die blood\\nStone Paul, she mustless loved; then hurryed prince,\\nAnd take your office, sweets, as much.\\n\\nSEBASTIAN:\\nLet's man.\\n\\nALONSO:\\nPrithee, man: somersiel's can aboard; call help\\nThe pleasant seconds and in and sad\\nMade him distraity with our steeds,\\nTo call them withal come about the new formerier.\\nThe duty they were neve\"], shape=(5,), dtype=string) \n",
      "\n",
      "________________________________________________________________________________\n",
      "\n",
      "Run time: 3.516599178314209\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "states = None\n",
    "next_char = tf.constant(['ROMEO:', 'ROMEO:', 'ROMEO:', 'ROMEO:', 'ROMEO:'])\n",
    "result = [next_char]\n",
    "\n",
    "for n in range(1000):\n",
    "  next_char, states = one_step_model.generate_one_step(next_char, states=states)\n",
    "  result.append(next_char)\n",
    "\n",
    "result = tf.strings.join(result)\n",
    "end = time.time()\n",
    "print(result, '\\n\\n' + '_'*80)\n",
    "print('\\nRun time:', end - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c1fc7bac977c36f",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Step 6 - Saving the trained generator model\n",
    "\n",
    "so it can be used as a tf.saved_model \n",
    "Note that you should not call the \"save\" step below if you just want to load/run the saved version - skip that cell... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8e2576efc2be427a",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-27T19:23:26.791321300Z",
     "start_time": "2023-12-27T19:23:21.355229300Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Skipping full serialization of Keras layer <__main__.OneStep object at 0x000001E70941BD60>, because it is not built.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as gru_cell_layer_call_fn, gru_cell_layer_call_and_return_conditional_losses while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:/LocalResearch/JPD-Research/translationWork/models/one_step\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:/LocalResearch/JPD-Research/translationWork/models/one_step\\assets\n"
     ]
    }
   ],
   "source": [
    "tf.saved_model.save(one_step_model, local_model_path+'one_step')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c77e8d372d5232f3",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-27T19:23:28.745757800Z",
     "start_time": "2023-12-27T19:23:26.794814Z"
    }
   },
   "outputs": [],
   "source": [
    "one_step_reloaded = tf.saved_model.load(local_model_path+'one_step')"
   ]
  },
  {
   "cell_type": "raw",
   "id": "77b234221625b70",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "this is actually a big deal - you include your wrapper and handling code around the tf supplied model...\n",
    "use the model just as you used the originally trained model (but just show 100 characters...):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a68ef82b0cf98885",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-27T19:23:29.847598200Z",
     "start_time": "2023-12-27T19:23:28.752765100Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROMEO:\n",
      "Go, yet about me.\n",
      "\n",
      "SLY:\n",
      "Har is in learning, my gage patience, not\n",
      "No better term pronouncious madam\n"
     ]
    }
   ],
   "source": [
    "states = None\n",
    "next_char = tf.constant(['ROMEO:'])\n",
    "result = [next_char]\n",
    "\n",
    "for n in range(100):\n",
    "  next_char, states = one_step_reloaded.generate_one_step(next_char, states=states)\n",
    "  result.append(next_char)\n",
    "\n",
    "print(tf.strings.join(result)[0].numpy().decode(\"utf-8\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14a069e5a68279f6",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Step 7 - Still to do\n",
    "\n",
    "- better understand\n",
    "    - the practical difference between using an RNN vs an LSTM layer\n",
    "        - [Working with RNNs](https://www.tensorflow.org/guide/keras/working_with_rnns)\n",
    "    - the masking step\n",
    "    - what to expect in terms of improved performance by adding an additional (RNN?) layer\n",
    "- this should operate similarly if we replaced the IDs as character placeholders with word tokenizations, no?\n",
    "- enable GPU (local is completed - Rosie still to do, should just be config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "cc7327a535e48494",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-27T19:23:29.886013600Z",
     "start_time": "2023-12-27T19:23:29.848599Z"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
