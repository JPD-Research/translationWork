{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2113b7af313f668e",
   "metadata": {},
   "source": [
    "# Text Generation using Recurrent Neural Networks\n",
    " \n",
    "Based on Tensorflow tutorial [Text generation with an RNN](https://www.tensorflow.org/text/tutorials/text_generation)\n",
    "\n",
    "Text generation using Shakespeare dataset from [The Unreasonable Effectiveness of Recurrent Neural Networks](http://karpathy.github.io/2015/05/21/rnn-effectiveness/)\n",
    "\n",
    "With modifications described along the way...this does not currently leverage any available GPU, but will by the time we're done...\n",
    "\n",
    "## Step 0 - Environment Setup \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bb007a00562a6500",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-19T06:51:34.069013700Z",
     "start_time": "2023-12-19T06:51:34.038075500Z"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [],
   "source": [
    "local_data_path_root = \"C:/LocalResearch/JPD-Research/translationWork\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-19T13:14:36.237199100Z",
     "start_time": "2023-12-19T13:14:36.212157500Z"
    }
   },
   "id": "fa1e278a6bb2ff21"
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of text: 1115394 characters\n",
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you know Caius Marcius is chief enemy to the people.\n",
      "\n",
      "65 unique characters\n"
     ]
    }
   ],
   "source": [
    "local_data_path = local_data_path_root+ \"/data\"\n",
    "path_to_file = tf.keras.utils.get_file('shakespeare.txt', 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt', cache_subdir=local_data_path)\n",
    "\n",
    "# Read, then decode \n",
    "text = open(path_to_file, 'rb').read().decode(encoding='utf-8')\n",
    "# length of text is the number of characters in it\n",
    "print(f'Length of text: {len(text)} characters')\n",
    "print(text[:250])\n",
    "\n",
    "vocab = sorted(set(text))\n",
    "print(f'{len(vocab)} unique characters')"
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-19T13:14:40.039812800Z",
     "start_time": "2023-12-19T13:14:39.963366Z"
    }
   },
   "id": "1927d862267744c0"
  },
  {
   "cell_type": "markdown",
   "source": [
    "get dataset and validate:"
   ],
   "metadata": {},
   "id": "e30eac3991c93dbd"
  },
  {
   "cell_type": "markdown",
   "id": "ded04fc3f32bf47e",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Step 1 - Preprocess the data \n",
    "with the dataset loaded, we need to vectorize it\n",
    "characters can be turned into numeric IDs, once the text is split into tokens\n",
    "\n",
    "and the tokens are turned into character IDs using a StringLookup layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e04a1da10eed1caf",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-19T06:51:34.723023100Z",
     "start_time": "2023-12-19T06:51:34.167460900Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.RaggedTensor [[b'a', b'b', b'c', b'd', b'e', b'f', b'g'], [b'x', b'y', b'z']]>\n",
      "WARNING:tensorflow:From c:\\LocalResearch\\CondaEnvs\\xfmr_tf\\Lib\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "WARNING:tensorflow:From c:\\LocalResearch\\CondaEnvs\\xfmr_tf\\Lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "<tf.RaggedTensor [[40, 41, 42, 43, 44, 45, 46], [63, 64, 65]]>\n"
     ]
    }
   ],
   "source": [
    "example_texts = ['abcdefg', 'xyz']\n",
    "\n",
    "chars = tf.strings.unicode_split(example_texts, input_encoding='UTF-8')\n",
    "print(chars)\n",
    "\n",
    "ids_from_chars = tf.keras.layers.StringLookup(\n",
    "    vocabulary=list(vocab), mask_token=None)\n",
    "\n",
    "ids = ids_from_chars(chars)\n",
    "print(ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "379f715481536887",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "for generation, this needs to be able to be inverted, recovering strings from IDs, using the same LookupLayer\n",
    "and these can be joined back into strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5dd54f052683de10",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-19T06:51:34.814368700Z",
     "start_time": "2023-12-19T06:51:34.720524800Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.RaggedTensor [[b'a', b'b', b'c', b'd', b'e', b'f', b'g'], [b'x', b'y', b'z']]>\n",
      "[b'abcdefg' b'xyz']\n",
      "tf.Tensor([b'abcdefg' b'xyz'], shape=(2,), dtype=string)\n",
      "[b'abcdefg' b'xyz']\n"
     ]
    }
   ],
   "source": [
    "chars_from_ids = tf.keras.layers.StringLookup(\n",
    "    vocabulary=ids_from_chars.get_vocabulary(), invert=True, mask_token=None)\n",
    "chars = chars_from_ids(ids)\n",
    "print(chars)\n",
    "\n",
    "print(tf.strings.reduce_join(chars, axis=-1).numpy())\n",
    "\n",
    "def text_from_ids(ids):\n",
    "  return tf.strings.reduce_join(chars_from_ids(ids), axis=-1)\n",
    "\n",
    "print( text_from_ids(ids))\n",
    "print( text_from_ids(ids).numpy() )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48756f5780bd892d",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Task definition - to determine the next likely character, given a character or sequence\n",
    "RNNs maintain state dependent on prior seen elements, use that state to predict the next character\n",
    "\n",
    "Split set into training example sequences, each of seq_length length\n",
    "each sequence predicts the seq_length+1 character"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ce577295113ca2df",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-19T06:51:35.158973500Z",
     "start_time": "2023-12-19T06:51:34.817869300Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([19 48 57 ... 46  9  1], shape=(1115394,), dtype=int64)\n",
      "F\n",
      "i\n",
      "r\n",
      "s\n",
      "t\n",
      " \n",
      "C\n",
      "i\n",
      "t\n",
      "i\n"
     ]
    }
   ],
   "source": [
    "all_ids = ids_from_chars(tf.strings.unicode_split(text, 'UTF-8'))\n",
    "print(all_ids)\n",
    "ids_dataset = tf.data.Dataset.from_tensor_slices(all_ids)\n",
    "for ids in ids_dataset.take(10):\n",
    "    print(chars_from_ids(ids).numpy().decode('utf-8'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6ffd09490941b024",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-19T06:51:35.160441200Z",
     "start_time": "2023-12-19T06:51:35.156435300Z"
    }
   },
   "outputs": [],
   "source": [
    "seq_length = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b6fc3dfab2f770f",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "use batch to generate appropriate sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f813ebe9440fbe47",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-19T06:51:35.221526100Z",
     "start_time": "2023-12-19T06:51:35.161471600Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[b'F' b'i' b'r' b's' b't' b' ' b'C' b'i' b't' b'i' b'z' b'e' b'n' b':'\n",
      " b'\\n' b'B' b'e' b'f' b'o' b'r' b'e' b' ' b'w' b'e' b' ' b'p' b'r' b'o'\n",
      " b'c' b'e' b'e' b'd' b' ' b'a' b'n' b'y' b' ' b'f' b'u' b'r' b't' b'h'\n",
      " b'e' b'r' b',' b' ' b'h' b'e' b'a' b'r' b' ' b'm' b'e' b' ' b's' b'p'\n",
      " b'e' b'a' b'k' b'.' b'\\n' b'\\n' b'A' b'l' b'l' b':' b'\\n' b'S' b'p' b'e'\n",
      " b'a' b'k' b',' b' ' b's' b'p' b'e' b'a' b'k' b'.' b'\\n' b'\\n' b'F' b'i'\n",
      " b'r' b's' b't' b' ' b'C' b'i' b't' b'i' b'z' b'e' b'n' b':' b'\\n' b'Y'\n",
      " b'o' b'u' b' '], shape=(101,), dtype=string)\n",
      "tf.Tensor(b'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou ', shape=(), dtype=string)\n",
      "tf.Tensor(b'are all resolved rather to die than to famish?\\n\\nAll:\\nResolved. resolved.\\n\\nFirst Citizen:\\nFirst, you k', shape=(), dtype=string)\n",
      "tf.Tensor(b\"now Caius Marcius is chief enemy to the people.\\n\\nAll:\\nWe know't, we know't.\\n\\nFirst Citizen:\\nLet us ki\", shape=(), dtype=string)\n",
      "tf.Tensor(b\"ll him, and we'll have corn at our own price.\\nIs't a verdict?\\n\\nAll:\\nNo more talking on't; let it be d\", shape=(), dtype=string)\n",
      "tf.Tensor(b'one: away, away!\\n\\nSecond Citizen:\\nOne word, good citizens.\\n\\nFirst Citizen:\\nWe are accounted poor citi', shape=(), dtype=string)\n"
     ]
    }
   ],
   "source": [
    "sequences = ids_dataset.batch(seq_length+1, drop_remainder=True)\n",
    "\n",
    "for seq in sequences.take(1):\n",
    "  print(chars_from_ids(seq))  \n",
    "for seq in sequences.take(5):\n",
    "  print(text_from_ids(seq))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63f29587c8c80c07",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "these sequences need to be turned into input/label sets\n",
    "for each step, the input is the current character, and the label is the next character"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e1ba20d5db37af4a",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-19T06:51:35.222548300Z",
     "start_time": "2023-12-19T06:51:35.197972300Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(['t', 'e', 'n', 's', 'o', 'r', 'f', 'l', 'o'], ['e', 'n', 's', 'o', 'r', 'f', 'l', 'o', 'w'])\n"
     ]
    }
   ],
   "source": [
    "def split_input_target(sequence):\n",
    "    input_text = sequence[:-1]\n",
    "    target_text = sequence[1:]\n",
    "    return input_text, target_text\n",
    "\n",
    "print( split_input_target(list(\"tensorflow\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "32cd8928f818c438",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-19T06:51:35.287678Z",
     "start_time": "2023-12-19T06:51:35.202519600Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input : b'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou'\n",
      "Target: b'irst Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n"
     ]
    }
   ],
   "source": [
    "dataset = sequences.map(split_input_target)\n",
    "\n",
    "for input_example, target_example in dataset.take(1):\n",
    "    print(\"Input :\", text_from_ids(input_example).numpy())\n",
    "    print(\"Target:\", text_from_ids(target_example).numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5af8884960cb0015",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "with these sequences, we need to pack these into training batches we can use with the model\n",
    "note that not all data is pulled into memory at once using the batch and buffer size to manage this transition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "93feefea03f62c09",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-19T06:51:35.289168900Z",
     "start_time": "2023-12-19T06:51:35.270770300Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<_PrefetchDataset element_spec=(TensorSpec(shape=(64, 100), dtype=tf.int64, name=None), TensorSpec(shape=(64, 100), dtype=tf.int64, name=None))>\n"
     ]
    }
   ],
   "source": [
    "# Batch size\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "# Buffer size to shuffle the dataset\n",
    "# (TF data is designed to work with possibly infinite sequences,\n",
    "# so it doesn't attempt to shuffle the entire sequence in memory. Instead,\n",
    "# it maintains a buffer in which it shuffles elements).\n",
    "BUFFER_SIZE = 10000\n",
    "\n",
    "dataset = (\n",
    "    dataset\n",
    "    .shuffle(BUFFER_SIZE)\n",
    "    .batch(BATCH_SIZE, drop_remainder=True)\n",
    "    .prefetch(tf.data.experimental.AUTOTUNE))\n",
    "\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Step 3 Model generation\n",
    "\n",
    "This Keras.Model implementation has three layers\n",
    "\n",
    "| layer name | function                             |\n",
    "|------------|--------------------------------------|\n",
    "| Embedding  | input layer                          |\n",
    "| GRU        | RNN with input size units=rnn_units  |\n",
    "| Dense      | output layer with vocab_size outputs |\n",
    " outputs are the log-likelihood of each character in the model"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e8713c78a6632479"
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "# Length of the vocabulary in StringLookup Layer\n",
    "vocab_size = len(ids_from_chars.get_vocabulary())\n",
    "\n",
    "# The embedding dimension\n",
    "embedding_dim = 256\n",
    "\n",
    "# Number of RNN units\n",
    "rnn_units = 1024"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-19T06:51:35.309079400Z",
     "start_time": "2023-12-19T06:51:35.284192800Z"
    }
   },
   "id": "7b219313337c3c7d"
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "class MyModel(tf.keras.Model):\n",
    "  def __init__(self, vocab_size, embedding_dim, rnn_units):\n",
    "    super().__init__(self)\n",
    "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "    self.gru = tf.keras.layers.GRU(rnn_units,\n",
    "                                   return_sequences=True,\n",
    "                                   return_state=True)\n",
    "    self.dense = tf.keras.layers.Dense(vocab_size)\n",
    "\n",
    "  def call(self, inputs, states=None, return_state=False, training=False):\n",
    "    x = inputs\n",
    "    x = self.embedding(x, training=training)\n",
    "    if states is None:\n",
    "      states = self.gru.get_initial_state(x)\n",
    "    x, states = self.gru(x, initial_state=states, training=training)\n",
    "    x = self.dense(x, training=training)\n",
    "\n",
    "    if return_state:\n",
    "      return x, states\n",
    "    else:\n",
    "      return x"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-19T06:51:35.368605700Z",
     "start_time": "2023-12-19T06:51:35.292680600Z"
    }
   },
   "id": "b7b6f7029d1dd3ac"
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "model = MyModel(\n",
    "    vocab_size=vocab_size,\n",
    "    embedding_dim=embedding_dim,\n",
    "    rnn_units=rnn_units)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-19T06:51:35.371114100Z",
     "start_time": "2023-12-19T06:51:35.297590200Z"
    }
   },
   "id": "10bbdceb577233a5"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Testing the model"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cdb7e64aad89531b"
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 100, 66) # (batch_size, sequence_length, vocab_size)\n",
      "Model: \"my_model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       multiple                  16896     \n",
      "                                                                 \n",
      " gru (GRU)                   multiple                  3938304   \n",
      "                                                                 \n",
      " dense (Dense)               multiple                  67650     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4022850 (15.35 MB)\n",
      "Trainable params: 4022850 (15.35 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "for input_example_batch, target_example_batch in dataset.take(1):\n",
    "    example_batch_predictions = model(input_example_batch)\n",
    "    print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")\n",
    "\n",
    "print(model.summary())"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-19T06:51:37.977263200Z",
     "start_time": "2023-12-19T06:51:35.338122400Z"
    }
   },
   "id": "e7171fd6993fb1"
  },
  {
   "cell_type": "markdown",
   "source": [
    "to actually predict, you need to process the logits returned over the character vocabulary\n",
    "for the first example (encoded and decoded):"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "eaa784675224a1f4"
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[31  5 37 36 35 44 22 12 49 28 56 64 51 29 13 46 65 58 56 22 61 47 45 10\n",
      " 29 55 33 22 65 41  8 12 61 42 61 59 42 62 30  9 44 61 65 56 23 41 47 45\n",
      " 19 26  5 63 15 33 25 17 14 13 15 13 44 37 60 56 28 10  5 26 12 38 48 55\n",
      " 47 65  0 54 49 18 45 46 20 22 21 50 40 21 65 46 25  4 54 35 16 15  4 29\n",
      " 51 49 30 26]\n",
      "Input:\n",
      " b's true; I heard a senator speak it.\\nThus it is: the Volsces have an army forth; against\\nwhom Cominiu'\n",
      "\n",
      "Next Char Predictions:\n",
      " b'R&XWVeI;jOqylP?gzsqIvhf3PpTIzb-;vcvtcwQ.evzqJbhfFM&xBTLDA?B?eXuqO3&M;Yiphz[UNK]ojEfgGIHkaHzgL$oVCB$PljQM'\n"
     ]
    }
   ],
   "source": [
    "sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)\n",
    "sampled_indices = tf.squeeze(sampled_indices, axis=-1).numpy()\n",
    "print(sampled_indices)\n",
    "print(\"Input:\\n\", text_from_ids(input_example_batch[0]).numpy())\n",
    "print()\n",
    "print(\"Next Char Predictions:\\n\", text_from_ids(sampled_indices).numpy())"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-19T06:51:38.000775Z",
     "start_time": "2023-12-19T06:51:37.967362900Z"
    }
   },
   "id": "ef0efa1f97edd01d"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Step 4 - Model Training\n",
    "\n",
    "We've turned prediction into a simple classification problem - given the prior state, predict the class of the next character\n",
    "We need an optimizer and loss function\n",
    "- the crossentropy loss function is reasonable in this case?\n",
    "- use the 'Adam' optimizer\n",
    "- use Compile to comfigure the training process"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e979f764cb424f57"
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction shape:  (64, 100, 66)  # (batch_size, sequence_length, vocab_size)\n",
      "Mean loss:         tf.Tensor(4.190152, shape=(), dtype=float32)\n",
      "WARNING:tensorflow:From c:\\LocalResearch\\CondaEnvs\\xfmr_tf\\Lib\\site-packages\\keras\\src\\optimizers\\__init__.py:309: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n"
     ]
    }
   ],
   "source": [
    "loss = tf.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "example_batch_mean_loss = loss(target_example_batch, example_batch_predictions)\n",
    "print(\"Prediction shape: \", example_batch_predictions.shape, \" # (batch_size, sequence_length, vocab_size)\")\n",
    "print(\"Mean loss:        \", example_batch_mean_loss)\n",
    "\n",
    "model.compile(optimizer='adam', loss=loss)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-19T06:51:38.068554300Z",
     "start_time": "2023-12-19T06:51:37.979264700Z"
    }
   },
   "id": "4b97396b8948378f"
  },
  {
   "cell_type": "markdown",
   "source": [
    "prior to training, the mean loss should be equal to the vocabulary size - even higher values means the model us certain of its wrong answers..."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ead04c6b21128780"
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66.03284\n"
     ]
    }
   ],
   "source": [
    "print(tf.exp(example_batch_mean_loss).numpy())"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-19T06:51:38.082088400Z",
     "start_time": "2023-12-19T06:51:38.066534500Z"
    }
   },
   "id": "da245f657adda1ec"
  },
  {
   "cell_type": "markdown",
   "source": [
    "configure checkpoints, and train the model"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e0ece1323f350fc1"
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [],
   "source": [
    "# Directory where the checkpoints will be saved\n",
    "checkpoint_dir = local_data_path_root + '/training_checkpoints'\n",
    "# Name of the checkpoint files\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
    "\n",
    "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_prefix,\n",
    "    save_weights_only=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-19T06:51:38.084591500Z",
     "start_time": "2023-12-19T06:51:38.075330400Z"
    }
   },
   "id": "45f9f66163cab996"
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "172/172 [==============================] - 172s 985ms/step - loss: 2.7049\n",
      "Epoch 2/20\n",
      "172/172 [==============================] - 164s 945ms/step - loss: 1.9786\n",
      "Epoch 3/20\n",
      "172/172 [==============================] - 157s 909ms/step - loss: 1.6968\n",
      "Epoch 4/20\n",
      "172/172 [==============================] - 160s 924ms/step - loss: 1.5377\n",
      "Epoch 5/20\n",
      "172/172 [==============================] - 158s 916ms/step - loss: 1.4405\n",
      "Epoch 6/20\n",
      "172/172 [==============================] - 158s 914ms/step - loss: 1.3734\n",
      "Epoch 7/20\n",
      "172/172 [==============================] - 157s 905ms/step - loss: 1.3224\n",
      "Epoch 8/20\n",
      "172/172 [==============================] - 155s 899ms/step - loss: 1.2777\n",
      "Epoch 9/20\n",
      "172/172 [==============================] - 154s 892ms/step - loss: 1.2361\n",
      "Epoch 10/20\n",
      "172/172 [==============================] - 154s 893ms/step - loss: 1.1962\n",
      "Epoch 11/20\n",
      "172/172 [==============================] - 154s 890ms/step - loss: 1.1557\n",
      "Epoch 12/20\n",
      "172/172 [==============================] - 155s 895ms/step - loss: 1.1139\n",
      "Epoch 13/20\n",
      "172/172 [==============================] - 155s 898ms/step - loss: 1.0692\n",
      "Epoch 14/20\n",
      "172/172 [==============================] - 155s 898ms/step - loss: 1.0231\n",
      "Epoch 15/20\n",
      "172/172 [==============================] - 156s 905ms/step - loss: 0.9723\n",
      "Epoch 16/20\n",
      "172/172 [==============================] - 154s 893ms/step - loss: 0.9216\n",
      "Epoch 17/20\n",
      "172/172 [==============================] - 156s 899ms/step - loss: 0.8692\n",
      "Epoch 18/20\n",
      "172/172 [==============================] - 155s 896ms/step - loss: 0.8173\n",
      "Epoch 19/20\n",
      "172/172 [==============================] - 155s 897ms/step - loss: 0.7657\n",
      "Epoch 20/20\n",
      "172/172 [==============================] - 156s 904ms/step - loss: 0.7185\n",
      "<keras.src.callbacks.History object at 0x000002BAC1EBBC90>\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 20\n",
    "history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])\n",
    "print(history)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-19T07:44:00.831354Z",
     "start_time": "2023-12-19T06:51:38.083589800Z"
    }
   },
   "id": "5ba8df5f55990c0"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Step 5 Text Generation\n",
    "\n",
    "can use a loop that passes in a propmpt, adds the generated output and passes that back in\n",
    "\n",
    "We'll define a class that takes the model and char <--> id functions as inputs\n",
    "- this class masks the input such that [UNK] won't be generated \n",
    "- then the function that \n",
    "    - tokenizees the current inputs\n",
    "    - runs the model on the input\n",
    "    - gets the last prediction\n",
    "    - samples the last prediction\n",
    "    - char-izes the resulting output\n",
    "    - returns the updated result"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ca34575165428d3e"
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [],
   "source": [
    "class OneStep(tf.keras.Model):\n",
    "  def __init__(self, model, chars_from_ids, ids_from_chars, temperature=1.0):\n",
    "    super().__init__()\n",
    "    self.temperature = temperature\n",
    "    self.model = model\n",
    "    self.chars_from_ids = chars_from_ids\n",
    "    self.ids_from_chars = ids_from_chars\n",
    "\n",
    "    # Create a mask to prevent \"[UNK]\" from being generated.\n",
    "    skip_ids = self.ids_from_chars(['[UNK]'])[:, None]\n",
    "    sparse_mask = tf.SparseTensor(\n",
    "        # Put a -inf at each bad index.\n",
    "        values=[-float('inf')]*len(skip_ids),\n",
    "        indices=skip_ids,\n",
    "        # Match the shape to the vocabulary\n",
    "        dense_shape=[len(ids_from_chars.get_vocabulary())])\n",
    "    self.prediction_mask = tf.sparse.to_dense(sparse_mask)\n",
    "\n",
    "  @tf.function\n",
    "  def generate_one_step(self, inputs, states=None):\n",
    "    # Convert strings to token IDs.\n",
    "    input_chars = tf.strings.unicode_split(inputs, 'UTF-8')\n",
    "    input_ids = self.ids_from_chars(input_chars).to_tensor()\n",
    "\n",
    "    # Run the model.\n",
    "    # predicted_logits.shape is [batch, char, next_char_logits]\n",
    "    predicted_logits, states = self.model(inputs=input_ids, states=states,\n",
    "                                          return_state=True)\n",
    "    # Only use the last prediction.\n",
    "    predicted_logits = predicted_logits[:, -1, :]\n",
    "    predicted_logits = predicted_logits/self.temperature\n",
    "    # Apply the prediction mask: prevent \"[UNK]\" from being generated.\n",
    "    predicted_logits = predicted_logits + self.prediction_mask\n",
    "\n",
    "    # Sample the output logits to generate token IDs.\n",
    "    predicted_ids = tf.random.categorical(predicted_logits, num_samples=1)\n",
    "    predicted_ids = tf.squeeze(predicted_ids, axis=-1)\n",
    "\n",
    "    # Convert from token ids to characters\n",
    "    predicted_chars = self.chars_from_ids(predicted_ids)\n",
    "\n",
    "    # Return the characters and model state.\n",
    "    return predicted_chars, states"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-19T12:57:18.222538Z",
     "start_time": "2023-12-19T12:57:18.200033100Z"
    }
   },
   "id": "129ec09c044c857a"
  },
  {
   "cell_type": "markdown",
   "source": [
    "now define an instance of our generator"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "616ccba8734af7e3"
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [],
   "source": [
    "one_step_model = OneStep(model, chars_from_ids, ids_from_chars)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-19T12:57:23.760189800Z",
     "start_time": "2023-12-19T12:57:23.727251Z"
    }
   },
   "id": "b9c65170b9cae4e0"
  },
  {
   "cell_type": "markdown",
   "source": [
    "the generation loop:\n",
    "- defines a constant text of \"ROMEO:\" \n",
    "- initialize our result (state) to this constant \n",
    "- iterates 1000 times\n",
    "    - calling our function to get the next char (and states)\n",
    "    - appends the next char to \"result\"\n",
    "- finally, the output result characters are joined together\n",
    "- and the result (and runtime) are printed out  "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9ebc75c68a1635eb"
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "________________________________________________________________________________\n",
      "\n",
      "ROMEO:\n",
      "This came from that which calls me luty learn of him;\n",
      "Thou shalt be satisfied; nor the jour is three Day:\n",
      "No, take your years: bethink you both! I have served--\n",
      "\n",
      "KING RICHARD II:\n",
      "True, most mercy too. The arm is off by black\n",
      "To go aboar it; bear him go:\n",
      "And I could weep which I should see this writon.\n",
      "\n",
      "Provost:\n",
      "His voices shall run a cup of hand:\n",
      "In hangmand said we stay twine conference!\n",
      "That instantally thrustly, madam:\n",
      "'Hought upon your honour'd lord:\n",
      "That would I live, in love her brothers like,\n",
      "As if I loved him instantly, and so die,\n",
      "Upon a barben so their ancient,\n",
      "There instantly execute, talk can make goss\n",
      "Hasting that heaven sinc: what say you this? Boy!\n",
      "\n",
      "Clown:\n",
      "God--judges I in judgment all on her;\n",
      "And to be brief, it is excellent: let it go\n",
      "on Richard's soldier's man.\n",
      "\n",
      "MERCUTIO:\n",
      "Consul! what's thy not? There's attime to do me,\n",
      "Here comes a gentler spirit.\n",
      "\n",
      "Provost:\n",
      "None, sir; I hear, most conscience, I beseech you\n",
      "\n",
      "Your grace spoke was he.\n",
      "\n",
      "GLOUCESTER:\n",
      "As shriftless sister  \n",
      "\n",
      "________________________________________________________________________________\n",
      "\n",
      "Run time: 1.5428550243377686\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "states = None\n",
    "next_char = tf.constant(['ROMEO:'])\n",
    "result = [next_char]\n",
    "\n",
    "for n in range(1000):\n",
    "  next_char, states = one_step_model.generate_one_step(next_char, states=states)\n",
    "  result.append(next_char)\n",
    "\n",
    "result = tf.strings.join(result)\n",
    "end = time.time()\n",
    "print('_'*80 + '\\n\\n' + result[0].numpy().decode('utf-8'), '\\n\\n' + '_'*80)\n",
    "print('\\nRun time:', end - start)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-19T13:03:07.104091800Z",
     "start_time": "2023-12-19T13:03:05.556654800Z"
    }
   },
   "id": "fe91809392697f1c"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### things to note:\n",
    "- this model is character-based, so it doesn't really know about words, \n",
    "- it does know when to capitalize, insert paragraph breaks, emulate the Shakespearean vocab from the vocabulary\n",
    "- it has not yet learned to generate coherent sentences\n",
    "    - larger number of epochs may improve this\n",
    "\n",
    "### things to experiment with\n",
    "- the \"temperature\" parameter can be used to make more/less random predictions\n",
    "- starting with a different seed string will change the output\n",
    "- adding another RNN layer can improve its overall accuracy\n",
    "- generation speed can be improved by batching (below code runs in similar time to above code) "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7c89dd896a5f29a7"
  },
  {
   "cell_type": "markdown",
   "source": [
    "start = time.time()\n",
    "states = None\n",
    "next_char = tf.constant(['ROMEO:', 'ROMEO:', 'ROMEO:', 'ROMEO:', 'ROMEO:'])\n",
    "result = [next_char]\n",
    "\n",
    "for n in range(1000):\n",
    "  next_char, states = one_step_model.generate_one_step(next_char, states=states)\n",
    "  result.append(next_char)\n",
    "\n",
    "result = tf.strings.join(result)\n",
    "end = time.time()\n",
    "print(result, '\\n\\n' + '_'*80)\n",
    "print('\\nRun time:', end - start)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "82a89b2fab46a48d"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Step 6 - Saving the trained generator model\n",
    "\n",
    "so it can be used as a tf.saved_model "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7c1fc7bac977c36f"
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\LocalResearch\\CondaEnvs\\xfmr_tf\\Lib\\site-packages\\keras\\src\\saving\\legacy\\saved_model\\save_impl.py:66: The name tf.logging.warning is deprecated. Please use tf.compat.v1.logging.warning instead.\n",
      "\n",
      "WARNING:tensorflow:Skipping full serialization of Keras layer <__main__.OneStep object at 0x000002BAE3CEE250>, because it is not built.\n",
      "WARNING:tensorflow:Model's `__init__()` arguments contain non-serializable objects. Please implement a `get_config()` method in the subclassed Model for proper saving and loading. Defaulting to empty config.\n",
      "WARNING:tensorflow:Model's `__init__()` arguments contain non-serializable objects. Please implement a `get_config()` method in the subclassed Model for proper saving and loading. Defaulting to empty config.\n",
      "INFO:tensorflow:Assets written to: C:/LocalResearch/JPD-Research/translationWork/models/one_step\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:/LocalResearch/JPD-Research/translationWork/models/one_step\\assets\n"
     ]
    }
   ],
   "source": [
    "local_model_path = local_data_path_root+ \"/models/\"\n",
    "tf.saved_model.save(one_step_model, local_model_path+'one_step')\n",
    "one_step_reloaded = tf.saved_model.load(local_model_path+'one_step')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-19T13:22:17.700653500Z",
     "start_time": "2023-12-19T13:22:11.338370800Z"
    }
   },
   "id": "61803446d7a80b60"
  },
  {
   "cell_type": "raw",
   "source": [
    "this is actually a big deal - you include your wrapper and handling code around the tf supplied model...\n",
    "use the model just as you used the originally trained model (but just show 100 characters...):"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "77b234221625b70"
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROMEO:\n",
      "What do you hear, ma? this is still your care?\n",
      "\n",
      "WARWICK:\n",
      "Suppose that, and desirds them on his knee\n"
     ]
    }
   ],
   "source": [
    "states = None\n",
    "next_char = tf.constant(['ROMEO:'])\n",
    "result = [next_char]\n",
    "\n",
    "for n in range(100):\n",
    "  next_char, states = one_step_reloaded.generate_one_step(next_char, states=states)\n",
    "  result.append(next_char)\n",
    "\n",
    "print(tf.strings.join(result)[0].numpy().decode(\"utf-8\"))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-19T13:27:56.689022200Z",
     "start_time": "2023-12-19T13:27:56.509910400Z"
    }
   },
   "id": "a68ef82b0cf98885"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "9e353204cf87d9f7"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
