{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2113b7af313f668e",
   "metadata": {},
   "source": [
    "# Text Generation using Recurrent Neural Networks\n",
    " \n",
    "Based on Tensorflow tutorial [Text generation with an RNN](https://www.tensorflow.org/text/tutorials/text_generation)\n",
    "\n",
    "Text generation using Shakespeare dataset from [The Unreasonable Effectiveness of Recurrent Neural Networks](http://karpathy.github.io/2015/05/21/rnn-effectiveness/)\n",
    "\n",
    "With modifications described along the way...this does not currently leverage any available GPU, but will by the time we're done...\n",
    "\n",
    "## Step 0 - Environment Setup \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bb007a00562a6500",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-19T17:02:53.114169500Z",
     "start_time": "2023-12-19T17:02:42.420819500Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\LocalResearch\\CondaEnvs\\xfmr_tf\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fa1e278a6bb2ff21",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-19T17:02:25.117187100Z",
     "start_time": "2023-12-19T17:02:25.102341800Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "local_data_path_root = \"C:/LocalResearch/JPD-Research/translationWork\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1927d862267744c0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-19T13:14:40.039812800Z",
     "start_time": "2023-12-19T13:14:39.963366Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of text: 1115394 characters\n",
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you know Caius Marcius is chief enemy to the people.\n",
      "\n",
      "65 unique characters\n"
     ]
    }
   ],
   "source": [
    "local_data_path = local_data_path_root+ \"/data\"\n",
    "path_to_file = tf.keras.utils.get_file('shakespeare.txt', 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt', cache_subdir=local_data_path)\n",
    "\n",
    "# Read, then decode \n",
    "text = open(path_to_file, 'rb').read().decode(encoding='utf-8')\n",
    "# length of text is the number of characters in it\n",
    "print(f'Length of text: {len(text)} characters')\n",
    "print(text[:250])\n",
    "\n",
    "vocab = sorted(set(text))\n",
    "print(f'{len(vocab)} unique characters')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e30eac3991c93dbd",
   "metadata": {},
   "source": [
    "get dataset and validate:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ded04fc3f32bf47e",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Step 1 - Preprocess the data \n",
    "with the dataset loaded, we need to vectorize it\n",
    "characters can be turned into numeric IDs, once the text is split into tokens\n",
    "\n",
    "and the tokens are turned into character IDs using a StringLookup layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e04a1da10eed1caf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-19T06:51:34.723023100Z",
     "start_time": "2023-12-19T06:51:34.167460900Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.RaggedTensor [[b'a', b'b', b'c', b'd', b'e', b'f', b'g'], [b'x', b'y', b'z']]>\n",
      "WARNING:tensorflow:From c:\\LocalResearch\\CondaEnvs\\xfmr_tf\\Lib\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "WARNING:tensorflow:From c:\\LocalResearch\\CondaEnvs\\xfmr_tf\\Lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "<tf.RaggedTensor [[40, 41, 42, 43, 44, 45, 46], [63, 64, 65]]>\n"
     ]
    }
   ],
   "source": [
    "example_texts = ['abcdefg', 'xyz']\n",
    "\n",
    "chars = tf.strings.unicode_split(example_texts, input_encoding='UTF-8')\n",
    "print(chars)\n",
    "\n",
    "ids_from_chars = tf.keras.layers.StringLookup(\n",
    "    vocabulary=list(vocab), mask_token=None)\n",
    "\n",
    "ids = ids_from_chars(chars)\n",
    "print(ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "379f715481536887",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "for generation, this needs to be able to be inverted, recovering strings from IDs, using the same LookupLayer\n",
    "and these can be joined back into strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5dd54f052683de10",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-19T06:51:34.814368700Z",
     "start_time": "2023-12-19T06:51:34.720524800Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.RaggedTensor [[b'a', b'b', b'c', b'd', b'e', b'f', b'g'], [b'x', b'y', b'z']]>\n",
      "[b'abcdefg' b'xyz']\n",
      "tf.Tensor([b'abcdefg' b'xyz'], shape=(2,), dtype=string)\n",
      "[b'abcdefg' b'xyz']\n"
     ]
    }
   ],
   "source": [
    "chars_from_ids = tf.keras.layers.StringLookup(\n",
    "    vocabulary=ids_from_chars.get_vocabulary(), invert=True, mask_token=None)\n",
    "chars = chars_from_ids(ids)\n",
    "print(chars)\n",
    "\n",
    "print(tf.strings.reduce_join(chars, axis=-1).numpy())\n",
    "\n",
    "def text_from_ids(ids):\n",
    "  return tf.strings.reduce_join(chars_from_ids(ids), axis=-1)\n",
    "\n",
    "print( text_from_ids(ids))\n",
    "print( text_from_ids(ids).numpy() )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48756f5780bd892d",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Task definition - to determine the next likely character, given a character or sequence\n",
    "RNNs maintain state dependent on prior seen elements, use that state to predict the next character\n",
    "\n",
    "Split set into training example sequences, each of seq_length length\n",
    "each sequence predicts the seq_length+1 character"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ce577295113ca2df",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-19T06:51:35.158973500Z",
     "start_time": "2023-12-19T06:51:34.817869300Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([19 48 57 ... 46  9  1], shape=(1115394,), dtype=int64)\n",
      "F\n",
      "i\n",
      "r\n",
      "s\n",
      "t\n",
      " \n",
      "C\n",
      "i\n",
      "t\n",
      "i\n"
     ]
    }
   ],
   "source": [
    "all_ids = ids_from_chars(tf.strings.unicode_split(text, 'UTF-8'))\n",
    "print(all_ids)\n",
    "ids_dataset = tf.data.Dataset.from_tensor_slices(all_ids)\n",
    "for ids in ids_dataset.take(10):\n",
    "    print(chars_from_ids(ids).numpy().decode('utf-8'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6ffd09490941b024",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-19T06:51:35.160441200Z",
     "start_time": "2023-12-19T06:51:35.156435300Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "seq_length = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b6fc3dfab2f770f",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "use batch to generate appropriate sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f813ebe9440fbe47",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-19T06:51:35.221526100Z",
     "start_time": "2023-12-19T06:51:35.161471600Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[b'F' b'i' b'r' b's' b't' b' ' b'C' b'i' b't' b'i' b'z' b'e' b'n' b':'\n",
      " b'\\n' b'B' b'e' b'f' b'o' b'r' b'e' b' ' b'w' b'e' b' ' b'p' b'r' b'o'\n",
      " b'c' b'e' b'e' b'd' b' ' b'a' b'n' b'y' b' ' b'f' b'u' b'r' b't' b'h'\n",
      " b'e' b'r' b',' b' ' b'h' b'e' b'a' b'r' b' ' b'm' b'e' b' ' b's' b'p'\n",
      " b'e' b'a' b'k' b'.' b'\\n' b'\\n' b'A' b'l' b'l' b':' b'\\n' b'S' b'p' b'e'\n",
      " b'a' b'k' b',' b' ' b's' b'p' b'e' b'a' b'k' b'.' b'\\n' b'\\n' b'F' b'i'\n",
      " b'r' b's' b't' b' ' b'C' b'i' b't' b'i' b'z' b'e' b'n' b':' b'\\n' b'Y'\n",
      " b'o' b'u' b' '], shape=(101,), dtype=string)\n",
      "tf.Tensor(b'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou ', shape=(), dtype=string)\n",
      "tf.Tensor(b'are all resolved rather to die than to famish?\\n\\nAll:\\nResolved. resolved.\\n\\nFirst Citizen:\\nFirst, you k', shape=(), dtype=string)\n",
      "tf.Tensor(b\"now Caius Marcius is chief enemy to the people.\\n\\nAll:\\nWe know't, we know't.\\n\\nFirst Citizen:\\nLet us ki\", shape=(), dtype=string)\n",
      "tf.Tensor(b\"ll him, and we'll have corn at our own price.\\nIs't a verdict?\\n\\nAll:\\nNo more talking on't; let it be d\", shape=(), dtype=string)\n",
      "tf.Tensor(b'one: away, away!\\n\\nSecond Citizen:\\nOne word, good citizens.\\n\\nFirst Citizen:\\nWe are accounted poor citi', shape=(), dtype=string)\n"
     ]
    }
   ],
   "source": [
    "sequences = ids_dataset.batch(seq_length+1, drop_remainder=True)\n",
    "\n",
    "for seq in sequences.take(1):\n",
    "  print(chars_from_ids(seq))  \n",
    "for seq in sequences.take(5):\n",
    "  print(text_from_ids(seq))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63f29587c8c80c07",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "these sequences need to be turned into input/label sets\n",
    "for each step, the input is the current character, and the label is the next character"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e1ba20d5db37af4a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-19T06:51:35.222548300Z",
     "start_time": "2023-12-19T06:51:35.197972300Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(['t', 'e', 'n', 's', 'o', 'r', 'f', 'l', 'o'], ['e', 'n', 's', 'o', 'r', 'f', 'l', 'o', 'w'])\n"
     ]
    }
   ],
   "source": [
    "def split_input_target(sequence):\n",
    "    input_text = sequence[:-1]\n",
    "    target_text = sequence[1:]\n",
    "    return input_text, target_text\n",
    "\n",
    "print( split_input_target(list(\"tensorflow\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "32cd8928f818c438",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-19T06:51:35.287678Z",
     "start_time": "2023-12-19T06:51:35.202519600Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input : b'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou'\n",
      "Target: b'irst Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n"
     ]
    }
   ],
   "source": [
    "dataset = sequences.map(split_input_target)\n",
    "\n",
    "for input_example, target_example in dataset.take(1):\n",
    "    print(\"Input :\", text_from_ids(input_example).numpy())\n",
    "    print(\"Target:\", text_from_ids(target_example).numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5af8884960cb0015",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "with these sequences, we need to pack these into training batches we can use with the model\n",
    "note that not all data is pulled into memory at once using the batch and buffer size to manage this transition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "93feefea03f62c09",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-19T06:51:35.289168900Z",
     "start_time": "2023-12-19T06:51:35.270770300Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<_PrefetchDataset element_spec=(TensorSpec(shape=(64, 100), dtype=tf.int64, name=None), TensorSpec(shape=(64, 100), dtype=tf.int64, name=None))>\n"
     ]
    }
   ],
   "source": [
    "# Batch size\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "# Buffer size to shuffle the dataset\n",
    "# (TF data is designed to work with possibly infinite sequences,\n",
    "# so it doesn't attempt to shuffle the entire sequence in memory. Instead,\n",
    "# it maintains a buffer in which it shuffles elements).\n",
    "BUFFER_SIZE = 10000\n",
    "\n",
    "dataset = (\n",
    "    dataset\n",
    "    .shuffle(BUFFER_SIZE)\n",
    "    .batch(BATCH_SIZE, drop_remainder=True)\n",
    "    .prefetch(tf.data.experimental.AUTOTUNE))\n",
    "\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8713c78a6632479",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Step 3 Model generation\n",
    "\n",
    "This Keras.Model implementation has three layers\n",
    "\n",
    "| layer name | function                             |\n",
    "|------------|--------------------------------------|\n",
    "| Embedding  | input layer                          |\n",
    "| GRU        | RNN with input size units=rnn_units  |\n",
    "| Dense      | output layer with vocab_size outputs |\n",
    " outputs are the log-likelihood of each character in the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7b219313337c3c7d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-19T06:51:35.309079400Z",
     "start_time": "2023-12-19T06:51:35.284192800Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Length of the vocabulary in StringLookup Layer\n",
    "vocab_size = len(ids_from_chars.get_vocabulary())\n",
    "\n",
    "# The embedding dimension\n",
    "embedding_dim = 256\n",
    "\n",
    "# Number of RNN units\n",
    "rnn_units = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b7b6f7029d1dd3ac",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-19T06:51:35.368605700Z",
     "start_time": "2023-12-19T06:51:35.292680600Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "class MyModel(tf.keras.Model):\n",
    "  def __init__(self, vocab_size, embedding_dim, rnn_units):\n",
    "    super().__init__(self)\n",
    "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "    self.gru = tf.keras.layers.GRU(rnn_units,\n",
    "                                   return_sequences=True,\n",
    "                                   return_state=True)\n",
    "    self.dense = tf.keras.layers.Dense(vocab_size)\n",
    "\n",
    "  def call(self, inputs, states=None, return_state=False, training=False):\n",
    "    x = inputs\n",
    "    x = self.embedding(x, training=training)\n",
    "    if states is None:\n",
    "      states = self.gru.get_initial_state(x)\n",
    "    x, states = self.gru(x, initial_state=states, training=training)\n",
    "    x = self.dense(x, training=training)\n",
    "\n",
    "    if return_state:\n",
    "      return x, states\n",
    "    else:\n",
    "      return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "10bbdceb577233a5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-19T06:51:35.371114100Z",
     "start_time": "2023-12-19T06:51:35.297590200Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "model = MyModel(\n",
    "    vocab_size=vocab_size,\n",
    "    embedding_dim=embedding_dim,\n",
    "    rnn_units=rnn_units)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdb7e64aad89531b",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Testing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e7171fd6993fb1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-19T06:51:37.977263200Z",
     "start_time": "2023-12-19T06:51:35.338122400Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 100, 66) # (batch_size, sequence_length, vocab_size)\n",
      "Model: \"my_model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       multiple                  16896     \n",
      "                                                                 \n",
      " gru (GRU)                   multiple                  3938304   \n",
      "                                                                 \n",
      " dense (Dense)               multiple                  67650     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4022850 (15.35 MB)\n",
      "Trainable params: 4022850 (15.35 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "for input_example_batch, target_example_batch in dataset.take(1):\n",
    "    example_batch_predictions = model(input_example_batch)\n",
    "    print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaa784675224a1f4",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "to actually predict, you need to process the logits returned over the character vocabulary\n",
    "for the first example (encoded and decoded):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ef0efa1f97edd01d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-19T06:51:38.000775Z",
     "start_time": "2023-12-19T06:51:37.967362900Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[31  5 37 36 35 44 22 12 49 28 56 64 51 29 13 46 65 58 56 22 61 47 45 10\n",
      " 29 55 33 22 65 41  8 12 61 42 61 59 42 62 30  9 44 61 65 56 23 41 47 45\n",
      " 19 26  5 63 15 33 25 17 14 13 15 13 44 37 60 56 28 10  5 26 12 38 48 55\n",
      " 47 65  0 54 49 18 45 46 20 22 21 50 40 21 65 46 25  4 54 35 16 15  4 29\n",
      " 51 49 30 26]\n",
      "Input:\n",
      " b's true; I heard a senator speak it.\\nThus it is: the Volsces have an army forth; against\\nwhom Cominiu'\n",
      "\n",
      "Next Char Predictions:\n",
      " b'R&XWVeI;jOqylP?gzsqIvhf3PpTIzb-;vcvtcwQ.evzqJbhfFM&xBTLDA?B?eXuqO3&M;Yiphz[UNK]ojEfgGIHkaHzgL$oVCB$PljQM'\n"
     ]
    }
   ],
   "source": [
    "sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)\n",
    "sampled_indices = tf.squeeze(sampled_indices, axis=-1).numpy()\n",
    "print(sampled_indices)\n",
    "print(\"Input:\\n\", text_from_ids(input_example_batch[0]).numpy())\n",
    "print()\n",
    "print(\"Next Char Predictions:\\n\", text_from_ids(sampled_indices).numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e979f764cb424f57",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Step 4 - Model Training\n",
    "\n",
    "We've turned prediction into a simple classification problem - given the prior state, predict the class of the next character\n",
    "We need an optimizer and loss function\n",
    "- the crossentropy loss function is reasonable in this case?\n",
    "- use the 'Adam' optimizer\n",
    "- use Compile to comfigure the training process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4b97396b8948378f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-19T06:51:38.068554300Z",
     "start_time": "2023-12-19T06:51:37.979264700Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction shape:  (64, 100, 66)  # (batch_size, sequence_length, vocab_size)\n",
      "Mean loss:         tf.Tensor(4.190152, shape=(), dtype=float32)\n",
      "WARNING:tensorflow:From c:\\LocalResearch\\CondaEnvs\\xfmr_tf\\Lib\\site-packages\\keras\\src\\optimizers\\__init__.py:309: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n"
     ]
    }
   ],
   "source": [
    "loss = tf.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "example_batch_mean_loss = loss(target_example_batch, example_batch_predictions)\n",
    "print(\"Prediction shape: \", example_batch_predictions.shape, \" # (batch_size, sequence_length, vocab_size)\")\n",
    "print(\"Mean loss:        \", example_batch_mean_loss)\n",
    "\n",
    "model.compile(optimizer='adam', loss=loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ead04c6b21128780",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "prior to training, the mean loss should be equal to the vocabulary size - even higher values means the model us certain of its wrong answers..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "da245f657adda1ec",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-19T06:51:38.082088400Z",
     "start_time": "2023-12-19T06:51:38.066534500Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66.03284\n"
     ]
    }
   ],
   "source": [
    "print(tf.exp(example_batch_mean_loss).numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0ece1323f350fc1",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "configure checkpoints, and train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "45f9f66163cab996",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-19T06:51:38.084591500Z",
     "start_time": "2023-12-19T06:51:38.075330400Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Directory where the checkpoints will be saved\n",
    "checkpoint_dir = local_data_path_root + '/training_checkpoints'\n",
    "# Name of the checkpoint files\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
    "\n",
    "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_prefix,\n",
    "    save_weights_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5ba8df5f55990c0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-19T07:44:00.831354Z",
     "start_time": "2023-12-19T06:51:38.083589800Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "172/172 [==============================] - 172s 985ms/step - loss: 2.7049\n",
      "Epoch 2/20\n",
      "172/172 [==============================] - 164s 945ms/step - loss: 1.9786\n",
      "Epoch 3/20\n",
      "172/172 [==============================] - 157s 909ms/step - loss: 1.6968\n",
      "Epoch 4/20\n",
      "172/172 [==============================] - 160s 924ms/step - loss: 1.5377\n",
      "Epoch 5/20\n",
      "172/172 [==============================] - 158s 916ms/step - loss: 1.4405\n",
      "Epoch 6/20\n",
      "172/172 [==============================] - 158s 914ms/step - loss: 1.3734\n",
      "Epoch 7/20\n",
      "172/172 [==============================] - 157s 905ms/step - loss: 1.3224\n",
      "Epoch 8/20\n",
      "172/172 [==============================] - 155s 899ms/step - loss: 1.2777\n",
      "Epoch 9/20\n",
      "172/172 [==============================] - 154s 892ms/step - loss: 1.2361\n",
      "Epoch 10/20\n",
      "172/172 [==============================] - 154s 893ms/step - loss: 1.1962\n",
      "Epoch 11/20\n",
      "172/172 [==============================] - 154s 890ms/step - loss: 1.1557\n",
      "Epoch 12/20\n",
      "172/172 [==============================] - 155s 895ms/step - loss: 1.1139\n",
      "Epoch 13/20\n",
      "172/172 [==============================] - 155s 898ms/step - loss: 1.0692\n",
      "Epoch 14/20\n",
      "172/172 [==============================] - 155s 898ms/step - loss: 1.0231\n",
      "Epoch 15/20\n",
      "172/172 [==============================] - 156s 905ms/step - loss: 0.9723\n",
      "Epoch 16/20\n",
      "172/172 [==============================] - 154s 893ms/step - loss: 0.9216\n",
      "Epoch 17/20\n",
      "172/172 [==============================] - 156s 899ms/step - loss: 0.8692\n",
      "Epoch 18/20\n",
      "172/172 [==============================] - 155s 896ms/step - loss: 0.8173\n",
      "Epoch 19/20\n",
      "172/172 [==============================] - 155s 897ms/step - loss: 0.7657\n",
      "Epoch 20/20\n",
      "172/172 [==============================] - 156s 904ms/step - loss: 0.7185\n",
      "<keras.src.callbacks.History object at 0x000002BAC1EBBC90>\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 20\n",
    "history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])\n",
    "print(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca34575165428d3e",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Step 5 Text Generation\n",
    "\n",
    "can use a loop that passes in a propmpt, adds the generated output and passes that back in\n",
    "\n",
    "We'll define a class that takes the model and char <--> id functions as inputs\n",
    "- this class masks the input such that [UNK] won't be generated \n",
    "- then the function that \n",
    "    - tokenizees the current inputs\n",
    "    - runs the model on the input\n",
    "    - gets the last prediction\n",
    "    - samples the last prediction\n",
    "    - char-izes the resulting output\n",
    "    - returns the updated result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "129ec09c044c857a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-19T12:57:18.222538Z",
     "start_time": "2023-12-19T12:57:18.200033100Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "class OneStep(tf.keras.Model):\n",
    "  def __init__(self, model, chars_from_ids, ids_from_chars, temperature=1.0):\n",
    "    super().__init__()\n",
    "    self.temperature = temperature\n",
    "    self.model = model\n",
    "    self.chars_from_ids = chars_from_ids\n",
    "    self.ids_from_chars = ids_from_chars\n",
    "\n",
    "    # Create a mask to prevent \"[UNK]\" from being generated.\n",
    "    skip_ids = self.ids_from_chars(['[UNK]'])[:, None]\n",
    "    sparse_mask = tf.SparseTensor(\n",
    "        # Put a -inf at each bad index.\n",
    "        values=[-float('inf')]*len(skip_ids),\n",
    "        indices=skip_ids,\n",
    "        # Match the shape to the vocabulary\n",
    "        dense_shape=[len(ids_from_chars.get_vocabulary())])\n",
    "    self.prediction_mask = tf.sparse.to_dense(sparse_mask)\n",
    "\n",
    "  @tf.function\n",
    "  def generate_one_step(self, inputs, states=None):\n",
    "    # Convert strings to token IDs.\n",
    "    input_chars = tf.strings.unicode_split(inputs, 'UTF-8')\n",
    "    input_ids = self.ids_from_chars(input_chars).to_tensor()\n",
    "\n",
    "    # Run the model.\n",
    "    # predicted_logits.shape is [batch, char, next_char_logits]\n",
    "    predicted_logits, states = self.model(inputs=input_ids, states=states,\n",
    "                                          return_state=True)\n",
    "    # Only use the last prediction.\n",
    "    predicted_logits = predicted_logits[:, -1, :]\n",
    "    predicted_logits = predicted_logits/self.temperature\n",
    "    # Apply the prediction mask: prevent \"[UNK]\" from being generated.\n",
    "    predicted_logits = predicted_logits + self.prediction_mask\n",
    "\n",
    "    # Sample the output logits to generate token IDs.\n",
    "    predicted_ids = tf.random.categorical(predicted_logits, num_samples=1)\n",
    "    predicted_ids = tf.squeeze(predicted_ids, axis=-1)\n",
    "\n",
    "    # Convert from token ids to characters\n",
    "    predicted_chars = self.chars_from_ids(predicted_ids)\n",
    "\n",
    "    # Return the characters and model state.\n",
    "    return predicted_chars, states"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "616ccba8734af7e3",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "now define an instance of our generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b9c65170b9cae4e0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-19T12:57:23.760189800Z",
     "start_time": "2023-12-19T12:57:23.727251Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "one_step_model = OneStep(model, chars_from_ids, ids_from_chars)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ebc75c68a1635eb",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "the generation loop:\n",
    "- defines a constant text of \"ROMEO:\" \n",
    "- initialize our result (state) to this constant \n",
    "- iterates 1000 times\n",
    "    - calling our function to get the next char (and states)\n",
    "    - appends the next char to \"result\"\n",
    "- finally, the output result characters are joined together\n",
    "- and the result (and runtime) are printed out  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fe91809392697f1c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-19T17:09:07.214015600Z",
     "start_time": "2023-12-19T17:09:04.967563900Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "________________________________________________________________________________\n",
      "\n",
      "ROMEO:\n",
      "Ha's, condemns you.\n",
      "\n",
      "First Citizen:\n",
      "And indeed, sir; I have heard that Lucentio that have tumbled\n",
      "but then poor souls,--like honour, break a grand\n",
      "As every kept thing it: the king cry Cluth,'er\n",
      "In hand, and made thee well, indeed, I\n",
      "will attend you again; betedle the victory!\n",
      "\n",
      "KING RICHARD III:\n",
      "No; if I reaf corceing to her\n",
      "Accused by a man at an unlawful bed.\n",
      "\n",
      "TYBALT:\n",
      "What art thou, man? awake! thy trial with\n",
      "'Tis can yield be here? early up your haugn?\n",
      "But then a grandain state, out with a piece of banished.\n",
      "Threw you, last likeness, fair Duke of York,\n",
      "But Tybalt scolding here already? Romeo!\n",
      "Or bid him misselves twenty times to wake you: yea, my most conspiration!\n",
      "\n",
      "HASTINGS:\n",
      "Ay, my gracious lord;\n",
      "I come with bathal for that which, be not so advice\n",
      "My life contend; in AUng of such things as yours, as I am sorry,\n",
      "Will not her dear explority of this parand?\n",
      "\n",
      "HORTENSIO:\n",
      "Sir, I have these noble-garten looks on liberty.\n",
      "O cleanselves I will appear, unless\n",
      "The naturage of my brother's dau \n",
      "\n",
      "________________________________________________________________________________\n",
      "\n",
      "Run time: 2.2406930923461914\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "states = None\n",
    "next_char = tf.constant(['ROMEO:'])\n",
    "result = [next_char]\n",
    "\n",
    "for n in range(1000):\n",
    "  next_char, states = one_step_model.generate_one_step(next_char, states=states)\n",
    "  result.append(next_char)\n",
    "\n",
    "result = tf.strings.join(result)\n",
    "end = time.time()\n",
    "print('_'*80 + '\\n\\n' + result[0].numpy().decode('utf-8'), '\\n\\n' + '_'*80)\n",
    "print('\\nRun time:', end - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c89dd896a5f29a7",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### things to note:\n",
    "- this model is character-based, so it doesn't really know about words, \n",
    "- it does know when to capitalize, insert paragraph breaks, emulate the Shakespearean vocab from the vocabulary\n",
    "- it has not yet learned to generate coherent sentences\n",
    "    - larger number of epochs may improve this\n",
    "\n",
    "### things to experiment with\n",
    "- the \"temperature\" parameter can be used to make more/less random predictions\n",
    "- starting with a different seed string will change the output\n",
    "- adding another RNN layer can improve its overall accuracy\n",
    "- generation speed can be improved by batching (below code runs in similar time to above code) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7bae3e60975dd545",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-19T17:09:25.220451100Z",
     "start_time": "2023-12-19T17:09:22.577948300Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[b\"ROMEO:\\nThis gentleman bound of all the deed.\\n\\nSecond Citizen:\\nBut as I was your daughter's daughter shall.\\nShe start to that; if I warrant, you will not yet.\\nNo, no, sir, the medicience: against the king\\nRain on the Duke of Gloucester's death\\nTo grate my brother's limit; when I blame fortwwelle\\nIn carrion-bear with black spidices,\\nMakes her government to chole myself,\\nYea, atten at the least i' the world: these severe\\nOn you and you are well as gloats, they do\\nnot perft; methought I hear some noise of peace,\\nTear me with interchangeably cherishing\\nThat when he would have lately give,\\nBale praised infect the bight: pierce did I lie?\\nBe anmorm, your birth ign of a friend,\\nHow he goes but myself again by judges\\nThan so your highness' dry or twortune\\nTo shape me with it? O my dear'st trial! when\\nhe wakes.\\n\\nShepherd:\\nYou, Siciliab,\\nOld Gay arroa the Tormente. To me, look thee, then,\\nAs one distressed word before him, as we\\nrepeal'd the Duke of Bertague this point by him!\\nAnd those eyes do wrought\"\n",
      " b\"ROMEO:\\nThis is more ready? To see the contract by\\nmine cause, almost awnier meet by me,\\nMay ask a part in years.\\n\\nLEONTES:\\nHa!\\n\\nEDWARD:\\nSweet was I tought when tage of circums.\\nCome, sir, no hence did not; but we have show'd by\\nSomething me to this son-gentleman,\\nCan I changed thee that woe such capital,\\nVershaled a sun warn' merit.\\n\\nProvost:\\nCome, blessed, blexide, help the world with weeds,\\nThat this weak prince for you to die, because Baptista.\\n\\nHENRY BOLINGBROKE:\\nThen Ludy'S gentleman:\\nHere, Angelo double and cunstrance again!\\nI would you be so honour'd him and old superflia;\\nBelieve it.\\n\\nCAMILLO:\\nI have a note design.\\n\\nKING RICHARD III:\\nThe thunder--hinds must be my child? but we\\nhave spent a boice, thou shalt stily rain:\\nunentious thine in blood, the shoulder and his kneel\\nThat made thy revengeful soferies.\\nWere he my brother, And, Lady Allard!\\nnot I will show your honour to sens with embrace there,\\nOr by that rabed by his kind as you!\\n\\nISABELLA:\\nO, tell the state, a mad\\nAs trusty Boli\"\n",
      " b\"ROMEO:\\nHave you so fond to smile upon your head?\\nOr hews that how sound I have spoke, thou canst,\\nWhat prick-conceused has is, if you can.\\n\\nPARIS:\\nPoor meaning, make less in the mindow's\\nwhice, because 'I whisted an ambush; to the law bids to death,\\nwere true despite doth much great liberity or Richard's right:\\nAy, man! for ladies, younger:\\nNo, no, my lord.\\n\\nKING RICHARD III:\\nMy crown may, we'll warmer thee with thee\\nAnd these they were no nerefor means:\\nThis pretty biods, lords, confess which none still\\nhave not you hare.\\nThe wrinkles came unthankfully grief?\\n\\nYORK:\\nWhose art your loving liege, that doth nette behold,\\n'Ty, since it is well arrived for fortune.\\n\\nGREEN:\\nNeither, farewell; I beseech you, year say ye,\\nThese names together.\\n\\nKING RICHARD III:\\nCanterning the dark days of Clarence have your bonds\\nNo other tried before this brave power:\\nIndeed, I have not deserr to me,\\nThe Earl of Properb's broke and as fiery for it.\\n\\nLUCIO:\\nO heavy sins! or what you are, comining Lamencess of Seck\"\n",
      " b\"ROMEO:\\nIs eyes nor any other grief! O heavyn!\\nGood king by us, my vanity and say no;\\n'fore noble enough, no letters; for the law,\\nDestroy'd by Bolingbroke: thou migst unwineness\\nI' head she was born price, interpt to effes\\nI shall not go.\\n\\nDUKE VINCENTIO:\\nhold you, uncle.\\n\\nVOLUMNIA:\\nShe's mad;\\nBear her the court, the head, the bright-down.\\n\\nKING RICHARD III:\\nSoft, stars, that in war faith, though she hath got\\nwhich burst must be abulbo'rroo.\\n\\nSICINIUS:\\nGo, defend,\\nAnd made by mourn you are only 'gaid,\\nNot heard it: if my drie, my directions are\\nMercy to unsa's from Dercyan Marknigh's death.\\nNay, beat your own behflike store that canques\\nThe real of one: I, breathe, I come!\\nWhere do you Romeo?\\n\\nJULIET:\\nI have no caught of; but we one's word;\\nWe'll show thee in her cheek upon the wisest.\\n\\nPETRUCE:\\nWho dares not to be? not\\nTill the hour records and true as earlial?\\nWhat stays had and Gaunt amber, wedless Warwick's right:\\nClarence will take this kindred by the Duke of York\\nUsurp her, in the dust\"\n",
      " b\"ROMEO:\\nLike an unswer watery thrown drinking upon,\\nWhich once can I was his doil'd wend,\\nOr if your brother's life, speaking, my best\\nTyrant, captain, one regard,--\\nBe not so disman you, indeed.\\n\\nDUKE VINCENTIO:\\nBy god and trust the market-place; take it your doubt:\\nBut thus small seen in wass, and fasting.\\nMost wrong this darce, I fear me not,\\nThoughts did I give your liberty murder me.\\n\\nCLAUDIO:\\nThus stands the order of the aim of eleven:\\nSlandered, is gone to Romeo: where thou liest,\\nThou migst-you invetted, eleven power, twenty times,\\nTo instruct her sounds, to report one hour\\nMargaret like engrossing that which you\\nNad, been but only Lord Tallers on.\\n\\nPAULINA:\\nThank me your body\\nTo be on your to command deliver him.\\n\\nKING EDWARD IV:\\nAy, my reason: all our wits are honest.\\nPerhancio, go so; betimes, I hold it see,\\nThou wast the other fly.\\n\\nSecond Servingman:\\nAre you are more!\\n\\nShepherd:\\nAre you consented man, that hath alwasses him,\\nAnd spare my weary trimp that thou hadst saff;\\nAnd, Rom\"], shape=(5,), dtype=string) \n",
      "\n",
      "________________________________________________________________________________\n",
      "\n",
      "Run time: 2.6390936374664307\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "states = None\n",
    "next_char = tf.constant(['ROMEO:', 'ROMEO:', 'ROMEO:', 'ROMEO:', 'ROMEO:'])\n",
    "result = [next_char]\n",
    "\n",
    "for n in range(1000):\n",
    "  next_char, states = one_step_model.generate_one_step(next_char, states=states)\n",
    "  result.append(next_char)\n",
    "\n",
    "result = tf.strings.join(result)\n",
    "end = time.time()\n",
    "print(result, '\\n\\n' + '_'*80)\n",
    "print('\\nRun time:', end - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c1fc7bac977c36f",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Step 6 - Saving the trained generator model\n",
    "\n",
    "so it can be used as a tf.saved_model \n",
    "Note that you should not call the \"save\" step below if you just want to load/run the saved version - skip that cell... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "61803446d7a80b60",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-19T17:05:26.264686800Z",
     "start_time": "2023-12-19T17:05:26.248618400Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "local_model_path = local_data_path_root+ \"/models/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e2576efc2be427a",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "tf.saved_model.save(one_step_model, local_model_path+'one_step')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c77e8d372d5232f3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-19T17:05:32.032897Z",
     "start_time": "2023-12-19T17:05:29.956763Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "one_step_reloaded = tf.saved_model.load(local_model_path+'one_step')"
   ]
  },
  {
   "cell_type": "raw",
   "id": "77b234221625b70",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "this is actually a big deal - you include your wrapper and handling code around the tf supplied model...\n",
    "use the model just as you used the originally trained model (but just show 100 characters...):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a68ef82b0cf98885",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-19T17:05:33.484134500Z",
     "start_time": "2023-12-19T17:05:32.410953100Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROMEO:\n",
      "So strike along. Poor Margaret!\n",
      "My master markless appetle and fly.\n",
      "\n",
      "WARWICK:\n",
      "Why, that's our guilt\n"
     ]
    }
   ],
   "source": [
    "states = None\n",
    "next_char = tf.constant(['ROMEO:'])\n",
    "result = [next_char]\n",
    "\n",
    "for n in range(100):\n",
    "  next_char, states = one_step_reloaded.generate_one_step(next_char, states=states)\n",
    "  result.append(next_char)\n",
    "\n",
    "print(tf.strings.join(result)[0].numpy().decode(\"utf-8\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14a069e5a68279f6",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Step 7 - Still to do\n",
    "\n",
    "- better understand\n",
    "    - the practical difference between using an RNN vs an LSTM layer\n",
    "    - the making step\n",
    "    - what to expect in terms of improved performance by adding an additional (RNN?) layer\n",
    "- this should operate similarly if we replaced the IDs as character placeholders with word tokenizations, no?     "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
