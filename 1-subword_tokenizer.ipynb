{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JPD-Research/translationWork/blob/master/1-subword_tokenizer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Subword Tokenizers\n",
        "\n",
        "Based on Tensorflow tutorial [Subword tokenizers](https://www.tensorflow.org/text/guide/subwords_tokenizer)\n",
        "\n",
        "exploration of tokenizers - building a subword vocabulary from a dataset and build a Bert tokenizer from that vocabulary\n",
        "\n",
        "advangates of subword over whole word tokenizers\n",
        "- common words still get a slot, but can use word pieces / characters to tokenize unknown words\n",
        "\n",
        "this runs on the GPU if available\n",
        "\n",
        "## Step 0 - Environment Setup\n",
        "\n",
        "requires tf-text and tf-datasets\n",
        "conda yaml for the environment compatible with all of these notebooks is included in the root of this repo"
      ],
      "metadata": {
        "collapsed": false,
        "id": "de4bf5f4624f5820"
      },
      "id": "de4bf5f4624f5820"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow_text in /usr/local/lib/python3.10/dist-packages (2.16.1)\n",
            "Requirement already satisfied: tensorflow<2.17,>=2.16.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow_text) (2.16.2)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.17,>=2.16.1->tensorflow_text) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.17,>=2.16.1->tensorflow_text) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.17,>=2.16.1->tensorflow_text) (24.3.25)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.17,>=2.16.1->tensorflow_text) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.17,>=2.16.1->tensorflow_text) (0.2.0)\n",
            "Requirement already satisfied: h5py>=3.10.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.17,>=2.16.1->tensorflow_text) (3.11.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.17,>=2.16.1->tensorflow_text) (18.1.1)\n",
            "Requirement already satisfied: ml-dtypes~=0.3.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.17,>=2.16.1->tensorflow_text) (0.3.2)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.17,>=2.16.1->tensorflow_text) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.17,>=2.16.1->tensorflow_text) (24.1)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.17,>=2.16.1->tensorflow_text) (3.20.3)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.17,>=2.16.1->tensorflow_text) (2.31.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.17,>=2.16.1->tensorflow_text) (67.7.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.17,>=2.16.1->tensorflow_text) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.17,>=2.16.1->tensorflow_text) (2.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.17,>=2.16.1->tensorflow_text) (4.12.2)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.17,>=2.16.1->tensorflow_text) (1.14.1)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.17,>=2.16.1->tensorflow_text) (1.64.1)\n",
            "Requirement already satisfied: tensorboard<2.17,>=2.16 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.17,>=2.16.1->tensorflow_text) (2.16.2)\n",
            "Requirement already satisfied: keras>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.17,>=2.16.1->tensorflow_text) (3.4.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.17,>=2.16.1->tensorflow_text) (0.37.0)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.17,>=2.16.1->tensorflow_text) (1.25.2)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow<2.17,>=2.16.1->tensorflow_text) (0.43.0)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from keras>=3.0.0->tensorflow<2.17,>=2.16.1->tensorflow_text) (13.7.1)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.10/dist-packages (from keras>=3.0.0->tensorflow<2.17,>=2.16.1->tensorflow_text) (0.0.8)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.10/dist-packages (from keras>=3.0.0->tensorflow<2.17,>=2.16.1->tensorflow_text) (0.12.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow<2.17,>=2.16.1->tensorflow_text) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow<2.17,>=2.16.1->tensorflow_text) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow<2.17,>=2.16.1->tensorflow_text) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow<2.17,>=2.16.1->tensorflow_text) (2024.6.2)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.17,>=2.16->tensorflow<2.17,>=2.16.1->tensorflow_text) (3.6)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.17,>=2.16->tensorflow<2.17,>=2.16.1->tensorflow_text) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.17,>=2.16->tensorflow<2.17,>=2.16.1->tensorflow_text) (3.0.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.17,>=2.16->tensorflow<2.17,>=2.16.1->tensorflow_text) (2.1.5)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.0.0->tensorflow<2.17,>=2.16.1->tensorflow_text) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.0.0->tensorflow<2.17,>=2.16.1->tensorflow_text) (2.16.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.0.0->tensorflow<2.17,>=2.16.1->tensorflow_text) (0.1.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install tensorflow_text\n",
        "import collections\n",
        "import os\n",
        "import pathlib\n",
        "import re\n",
        "import string\n",
        "import sys\n",
        "import tempfile\n",
        "import time\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import tensorflow_datasets as tfds\n",
        "import tensorflow_text as text\n",
        "import tensorflow as tf"
      ],
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-12-27T19:10:56.944975Z",
          "start_time": "2023-12-27T19:10:39.160307600Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ddcef3626577b2c5",
        "outputId": "3b53c803-a70c-4495-ea59-e012d62c25da"
      },
      "id": "ddcef3626577b2c5"
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1 Physical GPUs, 1 Logical GPUs\n"
          ]
        }
      ],
      "source": [
        "gpus = tf.config.list_physical_devices('GPU')\n",
        "if gpus:\n",
        "    # Restrict TensorFlow to only allocate 1GB of memory on the first GPU\n",
        "    try:\n",
        "        for gpu in gpus:\n",
        "            tf.config.experimental.set_memory_growth(gpu, True)\n",
        "            #tf.config.set_logical_device_configuration(\n",
        "        #    gpus[0],\n",
        "        #    [tf.config.LogicalDeviceConfiguration(memory_limit=512)])\n",
        "        logical_gpus = tf.config.list_logical_devices('GPU')\n",
        "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
        "    except RuntimeError as e:\n",
        "        # Virtual devices must be set before GPUs have been initialized\n",
        "        print(e)"
      ],
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-12-27T19:11:16.108892Z",
          "start_time": "2023-12-27T19:10:57.363574400Z"
        },
        "id": "2209e082a45015c9",
        "outputId": "147dc918-aecc-4573-a1fc-e06b23d5a565",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "2209e082a45015c9"
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "outputs": [],
      "source": [
        "tf.get_logger().setLevel('ERROR')\n",
        "pwd = pathlib.Path.cwd()"
      ],
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-12-27T19:11:16.156881300Z",
          "start_time": "2023-12-27T19:11:16.110381400Z"
        },
        "id": "8a740ff3dc97ebd3"
      },
      "id": "8a740ff3dc97ebd3"
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "outputs": [],
      "source": [
        "local_data_path_root = \".\"\n",
        "local_data_path = local_data_path_root+ \"/data\"\n",
        "local_vocab_path = local_data_path_root+ \"/vocab/\"\n",
        "local_model_path = local_data_path_root+ \"/models/\"\n",
        "\n",
        "# Directory where the checkpoints will be saved\n",
        "checkpoint_dir = local_data_path_root + '/training_checkpoints'"
      ],
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-12-27T19:11:16.991761900Z",
          "start_time": "2023-12-27T19:11:16.127382300Z"
        },
        "id": "dc218c7af88f0de4"
      },
      "id": "dc218c7af88f0de4"
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "outputs": [],
      "source": [
        "examples, metadata = tfds.load('ted_hrlr_translate/pt_to_en', with_info=True,\n",
        "                               as_supervised=True, data_dir=local_data_path)\n",
        "train_examples, val_examples = examples['train'], examples['validation']"
      ],
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-12-27T19:11:21.109424200Z",
          "start_time": "2023-12-27T19:11:16.989264600Z"
        },
        "id": "f2c5fadb1eca1203"
      },
      "id": "f2c5fadb1eca1203"
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Portuguese:  e quando melhoramos a procura , tiramos a única vantagem da impressão , que é a serendipidade .\n",
            "English:    and when you improve searchability , you actually take away the one advantage of print , which is serendipity .\n"
          ]
        }
      ],
      "source": [
        "for pt, en in train_examples.take(1):\n",
        "  print(\"Portuguese: \", pt.numpy().decode('utf-8'))\n",
        "  print(\"English:   \", en.numpy().decode('utf-8'))"
      ],
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-12-27T19:11:22.986546400Z",
          "start_time": "2023-12-27T19:11:21.112422900Z"
        },
        "id": "5fda587c573b8db9",
        "outputId": "22ebe093-38e9-4302-ff27-544d28ca9a12",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "5fda587c573b8db9"
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "outputs": [],
      "source": [
        "train_en = train_examples.map(lambda pt, en: en)\n",
        "train_pt = train_examples.map(lambda pt, en: pt)"
      ],
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-12-27T19:11:24.522315Z",
          "start_time": "2023-12-27T19:11:22.990534500Z"
        },
        "id": "9d814b594d29019a"
      },
      "id": "9d814b594d29019a"
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "outputs": [],
      "source": [
        "from tensorflow_text.tools.wordpiece_vocab import bert_vocab_from_dataset as bert_vocab"
      ],
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-12-27T19:11:24.538316500Z",
          "start_time": "2023-12-27T19:11:24.524817200Z"
        },
        "id": "a8de64e961f19331"
      },
      "id": "a8de64e961f19331"
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "outputs": [],
      "source": [
        "bert_tokenizer_params=dict(lower_case=True)\n",
        "reserved_tokens=[\"[PAD]\", \"[UNK]\", \"[START]\", \"[END]\"]\n",
        "\n",
        "bert_vocab_args = dict(\n",
        "    # The target vocabulary size\n",
        "    vocab_size = 8000,\n",
        "    # Reserved tokens that must be included in the vocabulary\n",
        "    reserved_tokens=reserved_tokens,\n",
        "    # Arguments for `text.BertTokenizer`\n",
        "    bert_tokenizer_params=bert_tokenizer_params,\n",
        "    # Arguments for `wordpiece_vocab.wordpiece_tokenizer_learner_lib.learn`\n",
        "    learn_params={},\n",
        ")"
      ],
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-12-27T19:11:24.721519700Z",
          "start_time": "2023-12-27T19:11:24.562814400Z"
        },
        "id": "18d5c1bc3c635108"
      },
      "id": "18d5c1bc3c635108"
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 1min 51s, sys: 1.84 s, total: 1min 52s\n",
            "Wall time: 2min 12s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "pt_vocab = bert_vocab.bert_vocab_from_dataset(\n",
        "    train_pt.batch(1000).prefetch(2),\n",
        "    **bert_vocab_args\n",
        ")"
      ],
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-12-27T19:13:18.892424100Z",
          "start_time": "2023-12-27T19:11:24.723522300Z"
        },
        "id": "e3d240c9d7b44ff3",
        "outputId": "51903cca-6142-45bf-9deb-da911295e7bb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "e3d240c9d7b44ff3"
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['[PAD]', '[UNK]', '[START]', '[END]', '!', '#', '$', '%', '&', \"'\"]\n",
            "['no', 'por', 'mais', 'na', 'eu', 'esta', 'muito', 'isso', 'isto', 'sao']\n",
            "['90', 'desse', 'efeito', 'malaria', 'normalmente', 'palestra', 'recentemente', '##nca', 'bons', 'chave']\n",
            "['##–', '##—', '##‘', '##’', '##“', '##”', '##⁄', '##€', '##♪', '##♫']\n"
          ]
        }
      ],
      "source": [
        "print(pt_vocab[:10])\n",
        "print(pt_vocab[100:110])\n",
        "print(pt_vocab[1000:1010])\n",
        "print(pt_vocab[-10:])"
      ],
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-12-27T19:13:18.927924400Z",
          "start_time": "2023-12-27T19:13:18.912424700Z"
        },
        "id": "75a5ba8612bdb9b9",
        "outputId": "ad986969-a4b6-49e5-e32c-9772c6ce9fb0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "75a5ba8612bdb9b9"
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "outputs": [],
      "source": [
        "def write_vocab_file(filepath, filename, vocab):\n",
        "  if not os.path.exists(filepath):\n",
        "    os.makedirs(filepath)\n",
        "  with open(filepath+filename, 'w', encoding='utf-8') as f:\n",
        "    for token in vocab:\n",
        "      print(token, file=f)"
      ],
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-12-27T19:13:18.928947900Z",
          "start_time": "2023-12-27T19:13:18.916954700Z"
        },
        "id": "9d8c2358c20d7484"
      },
      "id": "9d8c2358c20d7484"
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "outputs": [],
      "source": [
        "write_vocab_file(local_vocab_path,'pt_vocab.txt', pt_vocab)"
      ],
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-12-27T19:13:19.020440800Z",
          "start_time": "2023-12-27T19:13:18.924924500Z"
        },
        "id": "f698131f785f62fb"
      },
      "id": "f698131f785f62fb"
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 1min 9s, sys: 903 ms, total: 1min 10s\n",
            "Wall time: 1min 11s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "en_vocab = bert_vocab.bert_vocab_from_dataset(\n",
        "    train_en.batch(1000).prefetch(2),\n",
        "    **bert_vocab_args\n",
        ")"
      ],
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-12-27T19:14:37.158084900Z",
          "start_time": "2023-12-27T19:13:18.944927300Z"
        },
        "id": "b2efc916ebed9a8e",
        "outputId": "b625f63f-110c-429a-e62e-630a39f8763c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "b2efc916ebed9a8e"
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['[PAD]', '[UNK]', '[START]', '[END]', '!', '#', '$', '%', '&', \"'\"]\n",
            "['as', 'all', 'at', 'one', 'people', 're', 'like', 'if', 'our', 'from']\n",
            "['choose', 'consider', 'extraordinary', 'focus', 'generation', 'killed', 'patterns', 'putting', 'scientific', 'wait']\n",
            "['##_', '##`', '##ย', '##ร', '##อ', '##–', '##—', '##’', '##♪', '##♫']\n"
          ]
        }
      ],
      "source": [
        "print(en_vocab[:10])\n",
        "print(en_vocab[100:110])\n",
        "print(en_vocab[1000:1010])\n",
        "print(en_vocab[-10:])"
      ],
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-12-27T19:14:37.369667800Z",
          "start_time": "2023-12-27T19:14:37.161091700Z"
        },
        "id": "52c8c0177dd8e976",
        "outputId": "c56ba003-41bc-450e-a2ae-0c19cdd77b00",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "52c8c0177dd8e976"
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "outputs": [],
      "source": [
        "write_vocab_file(local_vocab_path,'en_vocab.txt', en_vocab)"
      ],
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-12-27T19:14:37.373110200Z",
          "start_time": "2023-12-27T19:14:37.177585200Z"
        },
        "id": "6a00e12b70556125"
      },
      "id": "6a00e12b70556125"
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "outputs": [],
      "source": [
        "pt_tokenizer = text.BertTokenizer(local_vocab_path+'pt_vocab.txt', **bert_tokenizer_params)\n",
        "en_tokenizer = text.BertTokenizer(local_vocab_path+'en_vocab.txt', **bert_tokenizer_params)"
      ],
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-12-27T19:14:37.669493800Z",
          "start_time": "2023-12-27T19:14:37.204618400Z"
        },
        "id": "4f02cb87118fea89"
      },
      "id": "4f02cb87118fea89"
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "b'and when you improve searchability , you actually take away the one advantage of print , which is serendipity .'\n",
            "b'but what if it were active ?'\n",
            "b\"but they did n't test for curiosity .\"\n"
          ]
        }
      ],
      "source": [
        "for pt_examples, en_examples in train_examples.batch(3).take(1):\n",
        "  for ex in en_examples:\n",
        "    print(ex.numpy())"
      ],
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-12-27T19:14:37.714527500Z",
          "start_time": "2023-12-27T19:14:37.671995800Z"
        },
        "id": "3d9f97896c79ded",
        "outputId": "90396929-e46d-474b-ad3c-63178915acb0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "3d9f97896c79ded"
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[72, 117, 79, 1259, 1491, 2362, 13, 79, 150, 184, 311, 71, 103, 2308, 74, 2679, 13, 148, 80, 55, 4840, 1434, 2423, 540, 15]\n",
            "[87, 90, 107, 76, 129, 1852, 30]\n",
            "[87, 83, 149, 50, 9, 56, 664, 85, 2512, 15]\n"
          ]
        }
      ],
      "source": [
        "# Tokenize the examples -> (batch, word, word-piece)\n",
        "token_batch = en_tokenizer.tokenize(en_examples)\n",
        "# Merge the word and word-piece axes -> (batch, tokens)\n",
        "token_batch = token_batch.merge_dims(-2,-1)\n",
        "\n",
        "for ex in token_batch.to_list():\n",
        "  print(ex)"
      ],
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-12-27T19:14:38.039160800Z",
          "start_time": "2023-12-27T19:14:37.719036300Z"
        },
        "id": "73bf93c18d943a38",
        "outputId": "bd9798d7-1a12-4955-ffb6-70f43ac43b59",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "73bf93c18d943a38"
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(3,), dtype=string, numpy=\n",
              "array([b'and when you improve search ##ability , you actually take away the one advantage of print , which is s ##ere ##nd ##ip ##ity .',\n",
              "       b'but what if it were active ?',\n",
              "       b\"but they did n ' t test for curiosity .\"], dtype=object)>"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ],
      "source": [
        "# Lookup each token id in the vocabulary.\n",
        "txt_tokens = tf.gather(en_vocab, token_batch)\n",
        "# Join with spaces.\n",
        "tf.strings.reduce_join(txt_tokens, separator=' ', axis=-1)\n"
      ],
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-12-27T19:14:38.150345500Z",
          "start_time": "2023-12-27T19:14:38.046161700Z"
        },
        "id": "6dce9e00c9c5b968",
        "outputId": "1a829e47-2a08-4733-865a-880d472bf992",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "6dce9e00c9c5b968"
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(3,), dtype=string, numpy=\n",
              "array([b'and when you improve searchability , you actually take away the one advantage of print , which is serendipity .',\n",
              "       b'but what if it were active ?',\n",
              "       b\"but they did n ' t test for curiosity .\"], dtype=object)>"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "source": [
        "words = en_tokenizer.detokenize(token_batch)\n",
        "tf.strings.reduce_join(words, separator=' ', axis=-1)"
      ],
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-12-27T19:14:38.336897700Z",
          "start_time": "2023-12-27T19:14:38.152353700Z"
        },
        "id": "d8b4683a82f3e92a",
        "outputId": "9f9d6617-10d1-46e8-c159-2b03ca3ae59e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "d8b4683a82f3e92a"
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "outputs": [],
      "source": [
        "START = tf.argmax(tf.constant(reserved_tokens) == \"[START]\")\n",
        "END = tf.argmax(tf.constant(reserved_tokens) == \"[END]\")\n",
        "\n",
        "def add_start_end(ragged):\n",
        "  count = ragged.bounding_shape()[0]\n",
        "  starts = tf.fill([count,1], START)\n",
        "  ends = tf.fill([count,1], END)\n",
        "  return tf.concat([starts, ragged, ends], axis=1)"
      ],
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-12-27T19:14:38.371998200Z",
          "start_time": "2023-12-27T19:14:38.337400200Z"
        },
        "id": "314eb3565701292"
      },
      "id": "314eb3565701292"
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(3,), dtype=string, numpy=\n",
              "array([b'[START] and when you improve searchability , you actually take away the one advantage of print , which is serendipity . [END]',\n",
              "       b'[START] but what if it were active ? [END]',\n",
              "       b\"[START] but they did n ' t test for curiosity . [END]\"],\n",
              "      dtype=object)>"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ],
      "source": [
        "words = en_tokenizer.detokenize(add_start_end(token_batch))\n",
        "tf.strings.reduce_join(words, separator=' ', axis=-1)"
      ],
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-12-27T19:14:38.493045800Z",
          "start_time": "2023-12-27T19:14:38.367498600Z"
        },
        "id": "b795e7e1a1126fa2",
        "outputId": "9d4efbfb-194f-4a14-f33d-37824cdc5128",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "b795e7e1a1126fa2"
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "outputs": [],
      "source": [
        "def cleanup_text(reserved_tokens, token_txt):\n",
        "  # Drop the reserved tokens, except for \"[UNK]\".\n",
        "  bad_tokens = [re.escape(tok) for tok in reserved_tokens if tok != \"[UNK]\"]\n",
        "  bad_token_re = \"|\".join(bad_tokens)\n",
        "\n",
        "  bad_cells = tf.strings.regex_full_match(token_txt, bad_token_re)\n",
        "  result = tf.ragged.boolean_mask(token_txt, ~bad_cells)\n",
        "\n",
        "  # Join them into strings.\n",
        "  result = tf.strings.reduce_join(result, separator=' ', axis=-1)\n",
        "\n",
        "  return result"
      ],
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-12-27T19:14:38.503621400Z",
          "start_time": "2023-12-27T19:14:38.492575200Z"
        },
        "id": "38099f778ac8c669"
      },
      "id": "38099f778ac8c669"
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([b'and when you improve searchability , you actually take away the one advantage of print , which is serendipity .',\n",
              "       b'but what if it were active ?',\n",
              "       b\"but they did n't test for curiosity .\"], dtype=object)"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ],
      "source": [
        "en_examples.numpy()"
      ],
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-12-27T19:14:38.543613700Z",
          "start_time": "2023-12-27T19:14:38.505614400Z"
        },
        "id": "c42762fd33bb4d98",
        "outputId": "f31686fe-8be7-4ab0-d97b-89acde922a01",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "c42762fd33bb4d98"
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.RaggedTensor [[b'and', b'when', b'you', b'improve', b'searchability', b',', b'you',\n",
              "  b'actually', b'take', b'away', b'the', b'one', b'advantage', b'of',\n",
              "  b'print', b',', b'which', b'is', b'serendipity', b'.']              ,\n",
              " [b'but', b'what', b'if', b'it', b'were', b'active', b'?'],\n",
              " [b'but', b'they', b'did', b'n', b\"'\", b't', b'test', b'for', b'curiosity',\n",
              "  b'.']                                                                    ]>"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ],
      "source": [
        "token_batch = en_tokenizer.tokenize(en_examples).merge_dims(-2,-1)\n",
        "words = en_tokenizer.detokenize(token_batch)\n",
        "words"
      ],
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-12-27T19:14:38.598619900Z",
          "start_time": "2023-12-27T19:14:38.520615300Z"
        },
        "id": "87766521416ab74c",
        "outputId": "037702ce-d797-45f7-f6b3-52fe54800da6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "87766521416ab74c"
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([b'and when you improve searchability , you actually take away the one advantage of print , which is serendipity .',\n",
              "       b'but what if it were active ?',\n",
              "       b\"but they did n ' t test for curiosity .\"], dtype=object)"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ],
      "source": [
        "cleanup_text(reserved_tokens, words).numpy()"
      ],
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-12-27T19:14:38.674720600Z",
          "start_time": "2023-12-27T19:14:38.600632900Z"
        },
        "id": "89ba2b0bda8b2105",
        "outputId": "76bd5493-523a-4303-efca-6b1fd33bee91",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "89ba2b0bda8b2105"
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "outputs": [],
      "source": [
        "class CustomTokenizer(tf.Module):\n",
        "  def __init__(self, reserved_tokens, vocab_path):\n",
        "    self.tokenizer = text.BertTokenizer(vocab_path, lower_case=True)\n",
        "    self._reserved_tokens = reserved_tokens\n",
        "    self._vocab_path = tf.saved_model.Asset(vocab_path)\n",
        "\n",
        "    vocab = pathlib.Path(vocab_path).read_text(encoding='utf-8').splitlines()\n",
        "    self.vocab = tf.Variable(vocab)\n",
        "\n",
        "    ## Create the signatures for export:\n",
        "\n",
        "    # Include a tokenize signature for a batch of strings.\n",
        "    self.tokenize.get_concrete_function(\n",
        "        tf.TensorSpec(shape=[None], dtype=tf.string))\n",
        "\n",
        "    # Include `detokenize` and `lookup` signatures for:\n",
        "    #   * `Tensors` with shapes [tokens] and [batch, tokens]\n",
        "    #   * `RaggedTensors` with shape [batch, tokens]\n",
        "    self.detokenize.get_concrete_function(\n",
        "        tf.TensorSpec(shape=[None, None], dtype=tf.int64))\n",
        "    self.detokenize.get_concrete_function(\n",
        "          tf.RaggedTensorSpec(shape=[None, None], dtype=tf.int64))\n",
        "\n",
        "    self.lookup.get_concrete_function(\n",
        "        tf.TensorSpec(shape=[None, None], dtype=tf.int64))\n",
        "    self.lookup.get_concrete_function(\n",
        "          tf.RaggedTensorSpec(shape=[None, None], dtype=tf.int64))\n",
        "\n",
        "    # These `get_*` methods take no arguments\n",
        "    self.get_vocab_size.get_concrete_function()\n",
        "    self.get_vocab_path.get_concrete_function()\n",
        "    self.get_reserved_tokens.get_concrete_function()\n",
        "\n",
        "  @tf.function\n",
        "  def tokenize(self, strings):\n",
        "    enc = self.tokenizer.tokenize(strings)\n",
        "    # Merge the `word` and `word-piece` axes.\n",
        "    enc = enc.merge_dims(-2,-1)\n",
        "    enc = add_start_end(enc)\n",
        "    return enc\n",
        "\n",
        "  @tf.function\n",
        "  def detokenize(self, tokenized):\n",
        "    words = self.tokenizer.detokenize(tokenized)\n",
        "    return cleanup_text(self._reserved_tokens, words)\n",
        "\n",
        "  @tf.function\n",
        "  def lookup(self, token_ids):\n",
        "    return tf.gather(self.vocab, token_ids)\n",
        "\n",
        "  @tf.function\n",
        "  def get_vocab_size(self):\n",
        "    return tf.shape(self.vocab)[0]\n",
        "\n",
        "  @tf.function\n",
        "  def get_vocab_path(self):\n",
        "    return self._vocab_path\n",
        "\n",
        "  @tf.function\n",
        "  def get_reserved_tokens(self):\n",
        "    return tf.constant(self._reserved_tokens)"
      ],
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-12-27T19:14:38.709712Z",
          "start_time": "2023-12-27T19:14:38.684735Z"
        },
        "id": "e69c2e48dac23efc"
      },
      "id": "e69c2e48dac23efc"
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "outputs": [],
      "source": [
        "tokenizers = tf.Module()\n",
        "tokenizers.pt = CustomTokenizer(reserved_tokens, local_vocab_path+'pt_vocab.txt')\n",
        "tokenizers.en = CustomTokenizer(reserved_tokens, local_vocab_path+'en_vocab.txt')"
      ],
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-12-27T19:14:41.943347500Z",
          "start_time": "2023-12-27T19:14:38.702236500Z"
        },
        "id": "9efc982cd029669c"
      },
      "id": "9efc982cd029669c"
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "outputs": [],
      "source": [
        "model_name = 'ted_hrlr_translate_pt_en_converter'\n",
        "tf.saved_model.save(tokenizers, local_model_path+model_name)"
      ],
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-12-27T19:14:44.366146800Z",
          "start_time": "2023-12-27T19:14:41.961340Z"
        },
        "id": "b9b3d9b6f49b1df2"
      },
      "id": "b9b3d9b6f49b1df2"
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "7010"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ],
      "source": [
        "reloaded_tokenizers = tf.saved_model.load(local_model_path+model_name)\n",
        "reloaded_tokenizers.en.get_vocab_size().numpy()"
      ],
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-12-27T19:14:45.651048400Z",
          "start_time": "2023-12-27T19:14:44.361236500Z"
        },
        "id": "cd072867024c79d1",
        "outputId": "be716f98-568b-438e-f64f-1fd4d29587ad",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "cd072867024c79d1"
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[   2, 4006, 2358,  687, 1192, 2365,    4,    3]])"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ],
      "source": [
        "tokens = reloaded_tokenizers.en.tokenize(['Hello TensorFlow!'])\n",
        "tokens.numpy()"
      ],
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-12-27T19:14:46.031815300Z",
          "start_time": "2023-12-27T19:14:45.654537800Z"
        },
        "id": "fb415f89f8f31517",
        "outputId": "01650b13-a164-46c4-f8fc-ed106e7fbbfc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "fb415f89f8f31517"
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.RaggedTensor [[b'[START]', b'hello', b'tens', b'##or', b'##f', b'##low', b'!',\n",
              "  b'[END]']]>"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ],
      "source": [
        "text_tokens = reloaded_tokenizers.en.lookup(tokens)\n",
        "text_tokens"
      ],
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-12-27T19:14:46.061347500Z",
          "start_time": "2023-12-27T19:14:46.028320700Z"
        },
        "id": "cd93524e9097dbd0",
        "outputId": "af74e395-3968-4593-8b12-cf0208a8a1e4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "cd93524e9097dbd0"
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hello tensorflow !\n"
          ]
        }
      ],
      "source": [
        "round_trip = reloaded_tokenizers.en.detokenize(tokens)\n",
        "\n",
        "print(round_trip.numpy()[0].decode('utf-8'))"
      ],
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-12-27T19:14:46.225250900Z",
          "start_time": "2023-12-27T19:14:46.057822700Z"
        },
        "id": "25a0339b20d4d36",
        "outputId": "39c32705-8981-4f19-92cb-3ba227d90a58",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "25a0339b20d4d36"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step n - Still to do\n",
        "\n",
        "- better understand alternatives and include in report\n",
        "- implement tf:lookup - build lookup table and pass to tokenizer\n",
        "- enable GPU\n",
        " - local is completed\n",
        " - Colab completed\n",
        " - Rosie still to do"
      ],
      "metadata": {
        "collapsed": false,
        "id": "93411b59dfbedb7"
      },
      "id": "93411b59dfbedb7"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.6"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}