{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Subword Tokenizers\n",
    " \n",
    "Based on Tensorflow tutorial [Subword tokenizers](https://www.tensorflow.org/text/guide/subwords_tokenizer)\n",
    "\n",
    "exploration of tokenizers - building a subword vocabulary from a dataset and build a Bert tokenizer from that vocabulary\n",
    "\n",
    "advangates of subword over whole word tokenizers\n",
    "- common words still get a slot, but can use word pieces / characters to tokenize unknown words\n",
    "\n",
    "## Step 0 - Environment Setup \n",
    "\n",
    "requires tf-text and tf-datasets\n",
    "conda yaml for the environment compatible with all of these notebooks is included in the root of this repo"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "de4bf5f4624f5820"
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "outputs": [],
   "source": [
    "import collections\n",
    "import os\n",
    "import pathlib\n",
    "import re\n",
    "import string\n",
    "import sys\n",
    "import tempfile\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow_datasets as tfds\n",
    "import tensorflow_text as text\n",
    "import tensorflow as tf"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-20T18:11:33.925042Z",
     "start_time": "2023-12-20T18:11:33.683958300Z"
    }
   },
   "id": "ddcef3626577b2c5"
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "outputs": [],
   "source": [
    "tf.get_logger().setLevel('ERROR')\n",
    "pwd = pathlib.Path.cwd()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-20T18:11:33.943051200Z",
     "start_time": "2023-12-20T18:11:33.926542Z"
    }
   },
   "id": "8a740ff3dc97ebd3"
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "outputs": [],
   "source": [
    "local_data_path_root = \"C:/LocalResearch/JPD-Research/translationWork\"\n",
    "local_data_path = local_data_path_root+ \"/data\"\n",
    "local_vocab_path = local_data_path_root+ \"/vocab/\"\n",
    "local_model_path = local_data_path_root+ \"/models/\"\n",
    "\n",
    "# Directory where the checkpoints will be saved\n",
    "checkpoint_dir = local_data_path_root + '/training_checkpoints'"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-20T18:11:33.971050800Z",
     "start_time": "2023-12-20T18:11:33.945051900Z"
    }
   },
   "id": "dc218c7af88f0de4"
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "outputs": [],
   "source": [
    "examples, metadata = tfds.load('ted_hrlr_translate/pt_to_en', with_info=True,\n",
    "                               as_supervised=True, data_dir=local_data_path)\n",
    "train_examples, val_examples = examples['train'], examples['validation']"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-20T18:11:34.110594900Z",
     "start_time": "2023-12-20T18:11:33.960067600Z"
    }
   },
   "id": "f2c5fadb1eca1203"
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Portuguese:  e quando melhoramos a procura , tiramos a única vantagem da impressão , que é a serendipidade .\n",
      "English:    and when you improve searchability , you actually take away the one advantage of print , which is serendipity .\n"
     ]
    }
   ],
   "source": [
    "for pt, en in train_examples.take(1):\n",
    "  print(\"Portuguese: \", pt.numpy().decode('utf-8'))\n",
    "  print(\"English:   \", en.numpy().decode('utf-8'))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-20T18:11:34.248211100Z",
     "start_time": "2023-12-20T18:11:34.094596500Z"
    }
   },
   "id": "5fda587c573b8db9"
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "outputs": [],
   "source": [
    "train_en = train_examples.map(lambda pt, en: en)\n",
    "train_pt = train_examples.map(lambda pt, en: pt)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-20T18:11:34.378247300Z",
     "start_time": "2023-12-20T18:11:34.240211400Z"
    }
   },
   "id": "9d814b594d29019a"
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "outputs": [],
   "source": [
    "from tensorflow_text.tools.wordpiece_vocab import bert_vocab_from_dataset as bert_vocab"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-20T18:11:34.379236Z",
     "start_time": "2023-12-20T18:11:34.335735100Z"
    }
   },
   "id": "a8de64e961f19331"
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "outputs": [],
   "source": [
    "bert_tokenizer_params=dict(lower_case=True)\n",
    "reserved_tokens=[\"[PAD]\", \"[UNK]\", \"[START]\", \"[END]\"]\n",
    "\n",
    "bert_vocab_args = dict(\n",
    "    # The target vocabulary size\n",
    "    vocab_size = 8000,\n",
    "    # Reserved tokens that must be included in the vocabulary\n",
    "    reserved_tokens=reserved_tokens,\n",
    "    # Arguments for `text.BertTokenizer`\n",
    "    bert_tokenizer_params=bert_tokenizer_params,\n",
    "    # Arguments for `wordpiece_vocab.wordpiece_tokenizer_learner_lib.learn`\n",
    "    learn_params={},\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-20T18:11:34.383747Z",
     "start_time": "2023-12-20T18:11:34.346737200Z"
    }
   },
   "id": "18d5c1bc3c635108"
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 50.5 s\n",
      "Wall time: 1min 51s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "pt_vocab = bert_vocab.bert_vocab_from_dataset(\n",
    "    train_pt.batch(1000).prefetch(2),\n",
    "    **bert_vocab_args\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-20T18:13:26.184731400Z",
     "start_time": "2023-12-20T18:11:34.352234800Z"
    }
   },
   "id": "e3d240c9d7b44ff3"
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[PAD]', '[UNK]', '[START]', '[END]', '!', '#', '$', '%', '&', \"'\"]\n",
      "['no', 'por', 'mais', 'na', 'eu', 'esta', 'muito', 'isso', 'isto', 'sao']\n",
      "['90', 'desse', 'efeito', 'malaria', 'normalmente', 'palestra', 'recentemente', '##nca', 'bons', 'chave']\n",
      "['##–', '##—', '##‘', '##’', '##“', '##”', '##⁄', '##€', '##♪', '##♫']\n"
     ]
    }
   ],
   "source": [
    "print(pt_vocab[:10])\n",
    "print(pt_vocab[100:110])\n",
    "print(pt_vocab[1000:1010])\n",
    "print(pt_vocab[-10:])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-20T18:13:26.185732100Z",
     "start_time": "2023-12-20T18:13:26.180719800Z"
    }
   },
   "id": "75a5ba8612bdb9b9"
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "outputs": [],
   "source": [
    "def write_vocab_file(filepath, vocab):\n",
    "  with open(filepath, 'w', encoding='utf-8') as f:\n",
    "    for token in vocab:\n",
    "      print(token, file=f)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-20T18:13:26.244262700Z",
     "start_time": "2023-12-20T18:13:26.186224700Z"
    }
   },
   "id": "9d8c2358c20d7484"
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "outputs": [],
   "source": [
    "write_vocab_file(local_vocab_path+'pt_vocab.txt', pt_vocab)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-20T18:13:26.245762Z",
     "start_time": "2023-12-20T18:13:26.200759400Z"
    }
   },
   "id": "f698131f785f62fb"
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 33.4 s\n",
      "Wall time: 1min 21s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "en_vocab = bert_vocab.bert_vocab_from_dataset(\n",
    "    train_en.batch(1000).prefetch(2),\n",
    "    **bert_vocab_args\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-20T18:14:47.769874600Z",
     "start_time": "2023-12-20T18:13:26.215263100Z"
    }
   },
   "id": "b2efc916ebed9a8e"
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[PAD]', '[UNK]', '[START]', '[END]', '!', '#', '$', '%', '&', \"'\"]\n",
      "['as', 'all', 'at', 'one', 'people', 're', 'like', 'if', 'our', 'from']\n",
      "['choose', 'consider', 'extraordinary', 'focus', 'generation', 'killed', 'patterns', 'putting', 'scientific', 'wait']\n",
      "['##_', '##`', '##ย', '##ร', '##อ', '##–', '##—', '##’', '##♪', '##♫']\n"
     ]
    }
   ],
   "source": [
    "print(en_vocab[:10])\n",
    "print(en_vocab[100:110])\n",
    "print(en_vocab[1000:1010])\n",
    "print(en_vocab[-10:])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-20T18:14:47.784374800Z",
     "start_time": "2023-12-20T18:14:47.763875300Z"
    }
   },
   "id": "52c8c0177dd8e976"
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "outputs": [],
   "source": [
    "write_vocab_file(local_vocab_path+'en_vocab.txt', en_vocab)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-20T18:14:47.816373900Z",
     "start_time": "2023-12-20T18:14:47.770374900Z"
    }
   },
   "id": "6a00e12b70556125"
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "outputs": [],
   "source": [
    "pt_tokenizer = text.BertTokenizer(local_vocab_path+'pt_vocab.txt', **bert_tokenizer_params)\n",
    "en_tokenizer = text.BertTokenizer(local_vocab_path+'en_vocab.txt', **bert_tokenizer_params)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-20T18:14:47.843894300Z",
     "start_time": "2023-12-20T18:14:47.787373800Z"
    }
   },
   "id": "4f02cb87118fea89"
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'and when you improve searchability , you actually take away the one advantage of print , which is serendipity .'\n",
      "b'but what if it were active ?'\n",
      "b\"but they did n't test for curiosity .\"\n"
     ]
    }
   ],
   "source": [
    "for pt_examples, en_examples in train_examples.batch(3).take(1):\n",
    "  for ex in en_examples:\n",
    "    print(ex.numpy())"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-20T18:14:47.872891100Z",
     "start_time": "2023-12-20T18:14:47.817874400Z"
    }
   },
   "id": "3d9f97896c79ded"
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[72, 117, 79, 1259, 1491, 2362, 13, 79, 150, 184, 311, 71, 103, 2308, 74, 2679, 13, 148, 80, 55, 4840, 1434, 2423, 540, 15]\n",
      "[87, 90, 107, 76, 129, 1852, 30]\n",
      "[87, 83, 149, 50, 9, 56, 664, 85, 2512, 15]\n"
     ]
    }
   ],
   "source": [
    "# Tokenize the examples -> (batch, word, word-piece)\n",
    "token_batch = en_tokenizer.tokenize(en_examples)\n",
    "# Merge the word and word-piece axes -> (batch, tokens)\n",
    "token_batch = token_batch.merge_dims(-2,-1)\n",
    "\n",
    "for ex in token_batch.to_list():\n",
    "  print(ex)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-20T18:14:47.902891300Z",
     "start_time": "2023-12-20T18:14:47.862894100Z"
    }
   },
   "id": "73bf93c18d943a38"
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "outputs": [
    {
     "data": {
      "text/plain": "<tf.Tensor: shape=(3,), dtype=string, numpy=\narray([b'and when you improve search ##ability , you actually take away the one advantage of print , which is s ##ere ##nd ##ip ##ity .',\n       b'but what if it were active ?',\n       b\"but they did n ' t test for curiosity .\"], dtype=object)>"
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lookup each token id in the vocabulary.\n",
    "txt_tokens = tf.gather(en_vocab, token_batch)\n",
    "# Join with spaces.\n",
    "tf.strings.reduce_join(txt_tokens, separator=' ', axis=-1)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-20T18:14:47.904389700Z",
     "start_time": "2023-12-20T18:14:47.884389300Z"
    }
   },
   "id": "6dce9e00c9c5b968"
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "outputs": [
    {
     "data": {
      "text/plain": "<tf.Tensor: shape=(3,), dtype=string, numpy=\narray([b'and when you improve searchability , you actually take away the one advantage of print , which is serendipity .',\n       b'but what if it were active ?',\n       b\"but they did n ' t test for curiosity .\"], dtype=object)>"
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = en_tokenizer.detokenize(token_batch)\n",
    "tf.strings.reduce_join(words, separator=' ', axis=-1)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-20T18:14:47.957948300Z",
     "start_time": "2023-12-20T18:14:47.901890400Z"
    }
   },
   "id": "d8b4683a82f3e92a"
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "outputs": [],
   "source": [
    "START = tf.argmax(tf.constant(reserved_tokens) == \"[START]\")\n",
    "END = tf.argmax(tf.constant(reserved_tokens) == \"[END]\")\n",
    "\n",
    "def add_start_end(ragged):\n",
    "  count = ragged.bounding_shape()[0]\n",
    "  starts = tf.fill([count,1], START)\n",
    "  ends = tf.fill([count,1], END)\n",
    "  return tf.concat([starts, ragged, ends], axis=1)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-20T18:14:47.959409100Z",
     "start_time": "2023-12-20T18:14:47.931894600Z"
    }
   },
   "id": "314eb3565701292"
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "outputs": [
    {
     "data": {
      "text/plain": "<tf.Tensor: shape=(3,), dtype=string, numpy=\narray([b'[START] and when you improve searchability , you actually take away the one advantage of print , which is serendipity . [END]',\n       b'[START] but what if it were active ? [END]',\n       b\"[START] but they did n ' t test for curiosity . [END]\"],\n      dtype=object)>"
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = en_tokenizer.detokenize(add_start_end(token_batch))\n",
    "tf.strings.reduce_join(words, separator=' ', axis=-1)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-20T18:14:47.994910300Z",
     "start_time": "2023-12-20T18:14:47.937895200Z"
    }
   },
   "id": "b795e7e1a1126fa2"
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "outputs": [],
   "source": [
    "def cleanup_text(reserved_tokens, token_txt):\n",
    "  # Drop the reserved tokens, except for \"[UNK]\".\n",
    "  bad_tokens = [re.escape(tok) for tok in reserved_tokens if tok != \"[UNK]\"]\n",
    "  bad_token_re = \"|\".join(bad_tokens)\n",
    "\n",
    "  bad_cells = tf.strings.regex_full_match(token_txt, bad_token_re)\n",
    "  result = tf.ragged.boolean_mask(token_txt, ~bad_cells)\n",
    "\n",
    "  # Join them into strings.\n",
    "  result = tf.strings.reduce_join(result, separator=' ', axis=-1)\n",
    "\n",
    "  return result"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-20T18:14:47.995908400Z",
     "start_time": "2023-12-20T18:14:47.977408400Z"
    }
   },
   "id": "38099f778ac8c669"
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "outputs": [
    {
     "data": {
      "text/plain": "array([b'and when you improve searchability , you actually take away the one advantage of print , which is serendipity .',\n       b'but what if it were active ?',\n       b\"but they did n't test for curiosity .\"], dtype=object)"
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en_examples.numpy()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-20T18:14:48.007912500Z",
     "start_time": "2023-12-20T18:14:47.981910400Z"
    }
   },
   "id": "c42762fd33bb4d98"
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "outputs": [
    {
     "data": {
      "text/plain": "<tf.RaggedTensor [[b'and', b'when', b'you', b'improve', b'searchability', b',', b'you',\n  b'actually', b'take', b'away', b'the', b'one', b'advantage', b'of',\n  b'print', b',', b'which', b'is', b'serendipity', b'.']              ,\n [b'but', b'what', b'if', b'it', b'were', b'active', b'?'],\n [b'but', b'they', b'did', b'n', b\"'\", b't', b'test', b'for', b'curiosity',\n  b'.']                                                                    ]>"
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_batch = en_tokenizer.tokenize(en_examples).merge_dims(-2,-1)\n",
    "words = en_tokenizer.detokenize(token_batch)\n",
    "words"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-20T18:14:48.052926400Z",
     "start_time": "2023-12-20T18:14:47.991907700Z"
    }
   },
   "id": "87766521416ab74c"
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "outputs": [
    {
     "data": {
      "text/plain": "array([b'and when you improve searchability , you actually take away the one advantage of print , which is serendipity .',\n       b'but what if it were active ?',\n       b\"but they did n ' t test for curiosity .\"], dtype=object)"
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleanup_text(reserved_tokens, words).numpy()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-20T18:14:48.053927200Z",
     "start_time": "2023-12-20T18:14:48.028913300Z"
    }
   },
   "id": "89ba2b0bda8b2105"
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "outputs": [],
   "source": [
    "class CustomTokenizer(tf.Module):\n",
    "  def __init__(self, reserved_tokens, vocab_path):\n",
    "    self.tokenizer = text.BertTokenizer(vocab_path, lower_case=True)\n",
    "    self._reserved_tokens = reserved_tokens\n",
    "    self._vocab_path = tf.saved_model.Asset(vocab_path)\n",
    "\n",
    "    vocab = pathlib.Path(vocab_path).read_text(encoding='utf-8').splitlines()\n",
    "    self.vocab = tf.Variable(vocab)\n",
    "\n",
    "    ## Create the signatures for export:   \n",
    "\n",
    "    # Include a tokenize signature for a batch of strings. \n",
    "    self.tokenize.get_concrete_function(\n",
    "        tf.TensorSpec(shape=[None], dtype=tf.string))\n",
    "\n",
    "    # Include `detokenize` and `lookup` signatures for:\n",
    "    #   * `Tensors` with shapes [tokens] and [batch, tokens]\n",
    "    #   * `RaggedTensors` with shape [batch, tokens]\n",
    "    self.detokenize.get_concrete_function(\n",
    "        tf.TensorSpec(shape=[None, None], dtype=tf.int64))\n",
    "    self.detokenize.get_concrete_function(\n",
    "          tf.RaggedTensorSpec(shape=[None, None], dtype=tf.int64))\n",
    "\n",
    "    self.lookup.get_concrete_function(\n",
    "        tf.TensorSpec(shape=[None, None], dtype=tf.int64))\n",
    "    self.lookup.get_concrete_function(\n",
    "          tf.RaggedTensorSpec(shape=[None, None], dtype=tf.int64))\n",
    "\n",
    "    # These `get_*` methods take no arguments\n",
    "    self.get_vocab_size.get_concrete_function()\n",
    "    self.get_vocab_path.get_concrete_function()\n",
    "    self.get_reserved_tokens.get_concrete_function()\n",
    "\n",
    "  @tf.function\n",
    "  def tokenize(self, strings):\n",
    "    enc = self.tokenizer.tokenize(strings)\n",
    "    # Merge the `word` and `word-piece` axes.\n",
    "    enc = enc.merge_dims(-2,-1)\n",
    "    enc = add_start_end(enc)\n",
    "    return enc\n",
    "\n",
    "  @tf.function\n",
    "  def detokenize(self, tokenized):\n",
    "    words = self.tokenizer.detokenize(tokenized)\n",
    "    return cleanup_text(self._reserved_tokens, words)\n",
    "\n",
    "  @tf.function\n",
    "  def lookup(self, token_ids):\n",
    "    return tf.gather(self.vocab, token_ids)\n",
    "\n",
    "  @tf.function\n",
    "  def get_vocab_size(self):\n",
    "    return tf.shape(self.vocab)[0]\n",
    "\n",
    "  @tf.function\n",
    "  def get_vocab_path(self):\n",
    "    return self._vocab_path\n",
    "\n",
    "  @tf.function\n",
    "  def get_reserved_tokens(self):\n",
    "    return tf.constant(self._reserved_tokens)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-20T18:14:48.091930400Z",
     "start_time": "2023-12-20T18:14:48.055927700Z"
    }
   },
   "id": "e69c2e48dac23efc"
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "outputs": [],
   "source": [
    "tokenizers = tf.Module()\n",
    "tokenizers.pt = CustomTokenizer(reserved_tokens, local_vocab_path+'pt_vocab.txt')\n",
    "tokenizers.en = CustomTokenizer(reserved_tokens, local_vocab_path+'en_vocab.txt')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-20T18:14:50.005589600Z",
     "start_time": "2023-12-20T18:14:48.062428700Z"
    }
   },
   "id": "9efc982cd029669c"
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "outputs": [],
   "source": [
    "model_name = 'ted_hrlr_translate_pt_en_converter'\n",
    "tf.saved_model.save(tokenizers, local_model_path+model_name)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-20T18:14:52.106840200Z",
     "start_time": "2023-12-20T18:14:50.013064500Z"
    }
   },
   "id": "b9b3d9b6f49b1df2"
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "outputs": [
    {
     "data": {
      "text/plain": "7010"
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reloaded_tokenizers = tf.saved_model.load(local_model_path+model_name)\n",
    "reloaded_tokenizers.en.get_vocab_size().numpy()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-20T18:14:53.145222200Z",
     "start_time": "2023-12-20T18:14:52.108842500Z"
    }
   },
   "id": "cd072867024c79d1"
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[   2, 4006, 2358,  687, 1192, 2365,    4,    3]], dtype=int64)"
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = reloaded_tokenizers.en.tokenize(['Hello TensorFlow!'])\n",
    "tokens.numpy()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-20T18:14:53.475270300Z",
     "start_time": "2023-12-20T18:14:53.149219400Z"
    }
   },
   "id": "fb415f89f8f31517"
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "outputs": [
    {
     "data": {
      "text/plain": "<tf.RaggedTensor [[b'[START]', b'hello', b'tens', b'##or', b'##f', b'##low', b'!',\n  b'[END]']]>"
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_tokens = reloaded_tokenizers.en.lookup(tokens)\n",
    "text_tokens"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-20T18:14:53.509770300Z",
     "start_time": "2023-12-20T18:14:53.471271600Z"
    }
   },
   "id": "cd93524e9097dbd0"
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello tensorflow !\n"
     ]
    }
   ],
   "source": [
    "round_trip = reloaded_tokenizers.en.detokenize(tokens)\n",
    "\n",
    "print(round_trip.numpy()[0].decode('utf-8'))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-20T18:14:53.642792Z",
     "start_time": "2023-12-20T18:14:53.498769900Z"
    }
   },
   "id": "25a0339b20d4d36"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Step n - Still to do\n",
    "\n",
    "- better understand\n",
    "    - the algorithm alternatives (in tutorial)\n",
    "- implement tf:lookup - build lookup table and pass to tokenizer\n",
    "- enable GPU (local and on Rosie)     "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "93411b59dfbedb7"
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-20T18:14:53.645311500Z",
     "start_time": "2023-12-20T18:14:53.637796500Z"
    }
   },
   "id": "d9e620bde3084354"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
