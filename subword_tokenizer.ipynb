{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JPD-Research/translationWork/blob/master/subword_tokenizer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Subword Tokenizers\n",
        "\n",
        "Based on Tensorflow tutorial [Subword tokenizers](https://www.tensorflow.org/text/guide/subwords_tokenizer)\n",
        "\n",
        "exploration of tokenizers - building a subword vocabulary from a dataset and build a Bert tokenizer from that vocabulary\n",
        "\n",
        "advangates of subword over whole word tokenizers\n",
        "- common words still get a slot, but can use word pieces / characters to tokenize unknown words\n",
        "\n",
        "this runs on the GPU if available\n",
        "\n",
        "## Step 0 - Environment Setup\n",
        "\n",
        "requires tf-text and tf-datasets\n",
        "conda yaml for the environment compatible with all of these notebooks is included in the root of this repo"
      ],
      "metadata": {
        "collapsed": false,
        "id": "de4bf5f4624f5820"
      },
      "id": "de4bf5f4624f5820"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "import collections\n",
        "import os\n",
        "import pathlib\n",
        "import re\n",
        "import string\n",
        "import sys\n",
        "import tempfile\n",
        "import time\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import tensorflow_datasets as tfds\n",
        "import tensorflow_text as text\n",
        "import tensorflow as tf"
      ],
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-12-27T19:10:56.944975Z",
          "start_time": "2023-12-27T19:10:39.160307600Z"
        },
        "id": "ddcef3626577b2c5"
      },
      "id": "ddcef3626577b2c5"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1 Physical GPUs, 1 Logical GPUs\n"
          ]
        }
      ],
      "source": [
        "gpus = tf.config.list_physical_devices('GPU')\n",
        "if gpus:\n",
        "    # Restrict TensorFlow to only allocate 1GB of memory on the first GPU\n",
        "    try:\n",
        "        for gpu in gpus:\n",
        "            tf.config.experimental.set_memory_growth(gpu, True)\n",
        "            #tf.config.set_logical_device_configuration(\n",
        "        #    gpus[0],\n",
        "        #    [tf.config.LogicalDeviceConfiguration(memory_limit=512)])\n",
        "        logical_gpus = tf.config.list_logical_devices('GPU')\n",
        "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
        "    except RuntimeError as e:\n",
        "        # Virtual devices must be set before GPUs have been initialized\n",
        "        print(e)"
      ],
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-12-27T19:11:16.108892Z",
          "start_time": "2023-12-27T19:10:57.363574400Z"
        },
        "id": "2209e082a45015c9",
        "outputId": "55057e69-1e0a-4f49-e808-982bee6ca4f7"
      },
      "id": "2209e082a45015c9"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "tf.get_logger().setLevel('ERROR')\n",
        "pwd = pathlib.Path.cwd()"
      ],
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-12-27T19:11:16.156881300Z",
          "start_time": "2023-12-27T19:11:16.110381400Z"
        },
        "id": "8a740ff3dc97ebd3"
      },
      "id": "8a740ff3dc97ebd3"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "local_data_path_root = \"C:/LocalResearch/JPD-Research/translationWork\"\n",
        "local_data_path = local_data_path_root+ \"/data\"\n",
        "local_vocab_path = local_data_path_root+ \"/vocab/\"\n",
        "local_model_path = local_data_path_root+ \"/models/\"\n",
        "\n",
        "# Directory where the checkpoints will be saved\n",
        "checkpoint_dir = local_data_path_root + '/training_checkpoints'"
      ],
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-12-27T19:11:16.991761900Z",
          "start_time": "2023-12-27T19:11:16.127382300Z"
        },
        "id": "dc218c7af88f0de4"
      },
      "id": "dc218c7af88f0de4"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "examples, metadata = tfds.load('ted_hrlr_translate/pt_to_en', with_info=True,\n",
        "                               as_supervised=True, data_dir=local_data_path)\n",
        "train_examples, val_examples = examples['train'], examples['validation']"
      ],
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-12-27T19:11:21.109424200Z",
          "start_time": "2023-12-27T19:11:16.989264600Z"
        },
        "id": "f2c5fadb1eca1203"
      },
      "id": "f2c5fadb1eca1203"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Portuguese:  e quando melhoramos a procura , tiramos a única vantagem da impressão , que é a serendipidade .\n",
            "English:    and when you improve searchability , you actually take away the one advantage of print , which is serendipity .\n"
          ]
        }
      ],
      "source": [
        "for pt, en in train_examples.take(1):\n",
        "  print(\"Portuguese: \", pt.numpy().decode('utf-8'))\n",
        "  print(\"English:   \", en.numpy().decode('utf-8'))"
      ],
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-12-27T19:11:22.986546400Z",
          "start_time": "2023-12-27T19:11:21.112422900Z"
        },
        "id": "5fda587c573b8db9",
        "outputId": "b6d587ba-79e1-4238-d1e5-5d38a600b1f7"
      },
      "id": "5fda587c573b8db9"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "train_en = train_examples.map(lambda pt, en: en)\n",
        "train_pt = train_examples.map(lambda pt, en: pt)"
      ],
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-12-27T19:11:24.522315Z",
          "start_time": "2023-12-27T19:11:22.990534500Z"
        },
        "id": "9d814b594d29019a"
      },
      "id": "9d814b594d29019a"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "from tensorflow_text.tools.wordpiece_vocab import bert_vocab_from_dataset as bert_vocab"
      ],
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-12-27T19:11:24.538316500Z",
          "start_time": "2023-12-27T19:11:24.524817200Z"
        },
        "id": "a8de64e961f19331"
      },
      "id": "a8de64e961f19331"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "bert_tokenizer_params=dict(lower_case=True)\n",
        "reserved_tokens=[\"[PAD]\", \"[UNK]\", \"[START]\", \"[END]\"]\n",
        "\n",
        "bert_vocab_args = dict(\n",
        "    # The target vocabulary size\n",
        "    vocab_size = 8000,\n",
        "    # Reserved tokens that must be included in the vocabulary\n",
        "    reserved_tokens=reserved_tokens,\n",
        "    # Arguments for `text.BertTokenizer`\n",
        "    bert_tokenizer_params=bert_tokenizer_params,\n",
        "    # Arguments for `wordpiece_vocab.wordpiece_tokenizer_learner_lib.learn`\n",
        "    learn_params={},\n",
        ")"
      ],
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-12-27T19:11:24.721519700Z",
          "start_time": "2023-12-27T19:11:24.562814400Z"
        },
        "id": "18d5c1bc3c635108"
      },
      "id": "18d5c1bc3c635108"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CPU times: total: 34.3 s\n",
            "Wall time: 1min 54s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "pt_vocab = bert_vocab.bert_vocab_from_dataset(\n",
        "    train_pt.batch(1000).prefetch(2),\n",
        "    **bert_vocab_args\n",
        ")"
      ],
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-12-27T19:13:18.892424100Z",
          "start_time": "2023-12-27T19:11:24.723522300Z"
        },
        "id": "e3d240c9d7b44ff3",
        "outputId": "6df0f7ec-beaf-43c8-ec9d-eb8731f61d4e"
      },
      "id": "e3d240c9d7b44ff3"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['[PAD]', '[UNK]', '[START]', '[END]', '!', '#', '$', '%', '&', \"'\"]\n",
            "['no', 'por', 'mais', 'na', 'eu', 'esta', 'muito', 'isso', 'isto', 'sao']\n",
            "['90', 'desse', 'efeito', 'malaria', 'normalmente', 'palestra', 'recentemente', '##nca', 'bons', 'chave']\n",
            "['##–', '##—', '##‘', '##’', '##“', '##”', '##⁄', '##€', '##♪', '##♫']\n"
          ]
        }
      ],
      "source": [
        "print(pt_vocab[:10])\n",
        "print(pt_vocab[100:110])\n",
        "print(pt_vocab[1000:1010])\n",
        "print(pt_vocab[-10:])"
      ],
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-12-27T19:13:18.927924400Z",
          "start_time": "2023-12-27T19:13:18.912424700Z"
        },
        "id": "75a5ba8612bdb9b9",
        "outputId": "b5855328-0c38-4411-bd58-0aa93de7b8a4"
      },
      "id": "75a5ba8612bdb9b9"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "def write_vocab_file(filepath, vocab):\n",
        "  with open(filepath, 'w', encoding='utf-8') as f:\n",
        "    for token in vocab:\n",
        "      print(token, file=f)"
      ],
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-12-27T19:13:18.928947900Z",
          "start_time": "2023-12-27T19:13:18.916954700Z"
        },
        "id": "9d8c2358c20d7484"
      },
      "id": "9d8c2358c20d7484"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "write_vocab_file(local_vocab_path+'pt_vocab.txt', pt_vocab)"
      ],
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-12-27T19:13:19.020440800Z",
          "start_time": "2023-12-27T19:13:18.924924500Z"
        },
        "id": "f698131f785f62fb"
      },
      "id": "f698131f785f62fb"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CPU times: total: 20.7 s\n",
            "Wall time: 1min 18s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "en_vocab = bert_vocab.bert_vocab_from_dataset(\n",
        "    train_en.batch(1000).prefetch(2),\n",
        "    **bert_vocab_args\n",
        ")"
      ],
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-12-27T19:14:37.158084900Z",
          "start_time": "2023-12-27T19:13:18.944927300Z"
        },
        "id": "b2efc916ebed9a8e",
        "outputId": "1380d76a-9560-4447-9aa9-cd07742f8caa"
      },
      "id": "b2efc916ebed9a8e"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['[PAD]', '[UNK]', '[START]', '[END]', '!', '#', '$', '%', '&', \"'\"]\n",
            "['as', 'all', 'at', 'one', 'people', 're', 'like', 'if', 'our', 'from']\n",
            "['choose', 'consider', 'extraordinary', 'focus', 'generation', 'killed', 'patterns', 'putting', 'scientific', 'wait']\n",
            "['##_', '##`', '##ย', '##ร', '##อ', '##–', '##—', '##’', '##♪', '##♫']\n"
          ]
        }
      ],
      "source": [
        "print(en_vocab[:10])\n",
        "print(en_vocab[100:110])\n",
        "print(en_vocab[1000:1010])\n",
        "print(en_vocab[-10:])"
      ],
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-12-27T19:14:37.369667800Z",
          "start_time": "2023-12-27T19:14:37.161091700Z"
        },
        "id": "52c8c0177dd8e976",
        "outputId": "9920b7fb-bdac-443e-af4d-826c7fdbd6ec"
      },
      "id": "52c8c0177dd8e976"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "write_vocab_file(local_vocab_path+'en_vocab.txt', en_vocab)"
      ],
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-12-27T19:14:37.373110200Z",
          "start_time": "2023-12-27T19:14:37.177585200Z"
        },
        "id": "6a00e12b70556125"
      },
      "id": "6a00e12b70556125"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "pt_tokenizer = text.BertTokenizer(local_vocab_path+'pt_vocab.txt', **bert_tokenizer_params)\n",
        "en_tokenizer = text.BertTokenizer(local_vocab_path+'en_vocab.txt', **bert_tokenizer_params)"
      ],
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-12-27T19:14:37.669493800Z",
          "start_time": "2023-12-27T19:14:37.204618400Z"
        },
        "id": "4f02cb87118fea89"
      },
      "id": "4f02cb87118fea89"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "b'and when you improve searchability , you actually take away the one advantage of print , which is serendipity .'\n",
            "b'but what if it were active ?'\n",
            "b\"but they did n't test for curiosity .\"\n"
          ]
        }
      ],
      "source": [
        "for pt_examples, en_examples in train_examples.batch(3).take(1):\n",
        "  for ex in en_examples:\n",
        "    print(ex.numpy())"
      ],
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-12-27T19:14:37.714527500Z",
          "start_time": "2023-12-27T19:14:37.671995800Z"
        },
        "id": "3d9f97896c79ded",
        "outputId": "f0390d59-ea18-4b8d-bcad-04e030966b6b"
      },
      "id": "3d9f97896c79ded"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[72, 117, 79, 1259, 1491, 2362, 13, 79, 150, 184, 311, 71, 103, 2308, 74, 2679, 13, 148, 80, 55, 4840, 1434, 2423, 540, 15]\n",
            "[87, 90, 107, 76, 129, 1852, 30]\n",
            "[87, 83, 149, 50, 9, 56, 664, 85, 2512, 15]\n"
          ]
        }
      ],
      "source": [
        "# Tokenize the examples -> (batch, word, word-piece)\n",
        "token_batch = en_tokenizer.tokenize(en_examples)\n",
        "# Merge the word and word-piece axes -> (batch, tokens)\n",
        "token_batch = token_batch.merge_dims(-2,-1)\n",
        "\n",
        "for ex in token_batch.to_list():\n",
        "  print(ex)"
      ],
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-12-27T19:14:38.039160800Z",
          "start_time": "2023-12-27T19:14:37.719036300Z"
        },
        "id": "73bf93c18d943a38",
        "outputId": "af6df72e-f794-4996-df8a-55b6aab71725"
      },
      "id": "73bf93c18d943a38"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": "<tf.Tensor: shape=(3,), dtype=string, numpy=\narray([b'and when you improve search ##ability , you actually take away the one advantage of print , which is s ##ere ##nd ##ip ##ity .',\n       b'but what if it were active ?',\n       b\"but they did n ' t test for curiosity .\"], dtype=object)>"
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Lookup each token id in the vocabulary.\n",
        "txt_tokens = tf.gather(en_vocab, token_batch)\n",
        "# Join with spaces.\n",
        "tf.strings.reduce_join(txt_tokens, separator=' ', axis=-1)\n"
      ],
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-12-27T19:14:38.150345500Z",
          "start_time": "2023-12-27T19:14:38.046161700Z"
        },
        "id": "6dce9e00c9c5b968",
        "outputId": "5a1cd784-e05c-407f-f9aa-48b9932fe76d"
      },
      "id": "6dce9e00c9c5b968"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": "<tf.Tensor: shape=(3,), dtype=string, numpy=\narray([b'and when you improve searchability , you actually take away the one advantage of print , which is serendipity .',\n       b'but what if it were active ?',\n       b\"but they did n ' t test for curiosity .\"], dtype=object)>"
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "words = en_tokenizer.detokenize(token_batch)\n",
        "tf.strings.reduce_join(words, separator=' ', axis=-1)"
      ],
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-12-27T19:14:38.336897700Z",
          "start_time": "2023-12-27T19:14:38.152353700Z"
        },
        "id": "d8b4683a82f3e92a",
        "outputId": "8cae0610-0824-4ec3-cf63-209331d42d91"
      },
      "id": "d8b4683a82f3e92a"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "START = tf.argmax(tf.constant(reserved_tokens) == \"[START]\")\n",
        "END = tf.argmax(tf.constant(reserved_tokens) == \"[END]\")\n",
        "\n",
        "def add_start_end(ragged):\n",
        "  count = ragged.bounding_shape()[0]\n",
        "  starts = tf.fill([count,1], START)\n",
        "  ends = tf.fill([count,1], END)\n",
        "  return tf.concat([starts, ragged, ends], axis=1)"
      ],
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-12-27T19:14:38.371998200Z",
          "start_time": "2023-12-27T19:14:38.337400200Z"
        },
        "id": "314eb3565701292"
      },
      "id": "314eb3565701292"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": "<tf.Tensor: shape=(3,), dtype=string, numpy=\narray([b'[START] and when you improve searchability , you actually take away the one advantage of print , which is serendipity . [END]',\n       b'[START] but what if it were active ? [END]',\n       b\"[START] but they did n ' t test for curiosity . [END]\"],\n      dtype=object)>"
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "words = en_tokenizer.detokenize(add_start_end(token_batch))\n",
        "tf.strings.reduce_join(words, separator=' ', axis=-1)"
      ],
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-12-27T19:14:38.493045800Z",
          "start_time": "2023-12-27T19:14:38.367498600Z"
        },
        "id": "b795e7e1a1126fa2",
        "outputId": "8dd2327d-e276-4fcc-db8a-f0fc28789976"
      },
      "id": "b795e7e1a1126fa2"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "def cleanup_text(reserved_tokens, token_txt):\n",
        "  # Drop the reserved tokens, except for \"[UNK]\".\n",
        "  bad_tokens = [re.escape(tok) for tok in reserved_tokens if tok != \"[UNK]\"]\n",
        "  bad_token_re = \"|\".join(bad_tokens)\n",
        "\n",
        "  bad_cells = tf.strings.regex_full_match(token_txt, bad_token_re)\n",
        "  result = tf.ragged.boolean_mask(token_txt, ~bad_cells)\n",
        "\n",
        "  # Join them into strings.\n",
        "  result = tf.strings.reduce_join(result, separator=' ', axis=-1)\n",
        "\n",
        "  return result"
      ],
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-12-27T19:14:38.503621400Z",
          "start_time": "2023-12-27T19:14:38.492575200Z"
        },
        "id": "38099f778ac8c669"
      },
      "id": "38099f778ac8c669"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": "array([b'and when you improve searchability , you actually take away the one advantage of print , which is serendipity .',\n       b'but what if it were active ?',\n       b\"but they did n't test for curiosity .\"], dtype=object)"
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "en_examples.numpy()"
      ],
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-12-27T19:14:38.543613700Z",
          "start_time": "2023-12-27T19:14:38.505614400Z"
        },
        "id": "c42762fd33bb4d98",
        "outputId": "20bb637d-bfa0-4649-df06-9b9fde347667"
      },
      "id": "c42762fd33bb4d98"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": "<tf.RaggedTensor [[b'and', b'when', b'you', b'improve', b'searchability', b',', b'you',\n  b'actually', b'take', b'away', b'the', b'one', b'advantage', b'of',\n  b'print', b',', b'which', b'is', b'serendipity', b'.']              ,\n [b'but', b'what', b'if', b'it', b'were', b'active', b'?'],\n [b'but', b'they', b'did', b'n', b\"'\", b't', b'test', b'for', b'curiosity',\n  b'.']                                                                    ]>"
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "token_batch = en_tokenizer.tokenize(en_examples).merge_dims(-2,-1)\n",
        "words = en_tokenizer.detokenize(token_batch)\n",
        "words"
      ],
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-12-27T19:14:38.598619900Z",
          "start_time": "2023-12-27T19:14:38.520615300Z"
        },
        "id": "87766521416ab74c",
        "outputId": "737d5992-8736-480b-eadc-cdebb7c2a3f8"
      },
      "id": "87766521416ab74c"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": "array([b'and when you improve searchability , you actually take away the one advantage of print , which is serendipity .',\n       b'but what if it were active ?',\n       b\"but they did n ' t test for curiosity .\"], dtype=object)"
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "cleanup_text(reserved_tokens, words).numpy()"
      ],
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-12-27T19:14:38.674720600Z",
          "start_time": "2023-12-27T19:14:38.600632900Z"
        },
        "id": "89ba2b0bda8b2105",
        "outputId": "2df4b2b1-5db6-4983-fb4c-2bc4c429658f"
      },
      "id": "89ba2b0bda8b2105"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "class CustomTokenizer(tf.Module):\n",
        "  def __init__(self, reserved_tokens, vocab_path):\n",
        "    self.tokenizer = text.BertTokenizer(vocab_path, lower_case=True)\n",
        "    self._reserved_tokens = reserved_tokens\n",
        "    self._vocab_path = tf.saved_model.Asset(vocab_path)\n",
        "\n",
        "    vocab = pathlib.Path(vocab_path).read_text(encoding='utf-8').splitlines()\n",
        "    self.vocab = tf.Variable(vocab)\n",
        "\n",
        "    ## Create the signatures for export:\n",
        "\n",
        "    # Include a tokenize signature for a batch of strings.\n",
        "    self.tokenize.get_concrete_function(\n",
        "        tf.TensorSpec(shape=[None], dtype=tf.string))\n",
        "\n",
        "    # Include `detokenize` and `lookup` signatures for:\n",
        "    #   * `Tensors` with shapes [tokens] and [batch, tokens]\n",
        "    #   * `RaggedTensors` with shape [batch, tokens]\n",
        "    self.detokenize.get_concrete_function(\n",
        "        tf.TensorSpec(shape=[None, None], dtype=tf.int64))\n",
        "    self.detokenize.get_concrete_function(\n",
        "          tf.RaggedTensorSpec(shape=[None, None], dtype=tf.int64))\n",
        "\n",
        "    self.lookup.get_concrete_function(\n",
        "        tf.TensorSpec(shape=[None, None], dtype=tf.int64))\n",
        "    self.lookup.get_concrete_function(\n",
        "          tf.RaggedTensorSpec(shape=[None, None], dtype=tf.int64))\n",
        "\n",
        "    # These `get_*` methods take no arguments\n",
        "    self.get_vocab_size.get_concrete_function()\n",
        "    self.get_vocab_path.get_concrete_function()\n",
        "    self.get_reserved_tokens.get_concrete_function()\n",
        "\n",
        "  @tf.function\n",
        "  def tokenize(self, strings):\n",
        "    enc = self.tokenizer.tokenize(strings)\n",
        "    # Merge the `word` and `word-piece` axes.\n",
        "    enc = enc.merge_dims(-2,-1)\n",
        "    enc = add_start_end(enc)\n",
        "    return enc\n",
        "\n",
        "  @tf.function\n",
        "  def detokenize(self, tokenized):\n",
        "    words = self.tokenizer.detokenize(tokenized)\n",
        "    return cleanup_text(self._reserved_tokens, words)\n",
        "\n",
        "  @tf.function\n",
        "  def lookup(self, token_ids):\n",
        "    return tf.gather(self.vocab, token_ids)\n",
        "\n",
        "  @tf.function\n",
        "  def get_vocab_size(self):\n",
        "    return tf.shape(self.vocab)[0]\n",
        "\n",
        "  @tf.function\n",
        "  def get_vocab_path(self):\n",
        "    return self._vocab_path\n",
        "\n",
        "  @tf.function\n",
        "  def get_reserved_tokens(self):\n",
        "    return tf.constant(self._reserved_tokens)"
      ],
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-12-27T19:14:38.709712Z",
          "start_time": "2023-12-27T19:14:38.684735Z"
        },
        "id": "e69c2e48dac23efc"
      },
      "id": "e69c2e48dac23efc"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "tokenizers = tf.Module()\n",
        "tokenizers.pt = CustomTokenizer(reserved_tokens, local_vocab_path+'pt_vocab.txt')\n",
        "tokenizers.en = CustomTokenizer(reserved_tokens, local_vocab_path+'en_vocab.txt')"
      ],
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-12-27T19:14:41.943347500Z",
          "start_time": "2023-12-27T19:14:38.702236500Z"
        },
        "id": "9efc982cd029669c"
      },
      "id": "9efc982cd029669c"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "model_name = 'ted_hrlr_translate_pt_en_converter'\n",
        "tf.saved_model.save(tokenizers, local_model_path+model_name)"
      ],
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-12-27T19:14:44.366146800Z",
          "start_time": "2023-12-27T19:14:41.961340Z"
        },
        "id": "b9b3d9b6f49b1df2"
      },
      "id": "b9b3d9b6f49b1df2"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": "7010"
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "reloaded_tokenizers = tf.saved_model.load(local_model_path+model_name)\n",
        "reloaded_tokenizers.en.get_vocab_size().numpy()"
      ],
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-12-27T19:14:45.651048400Z",
          "start_time": "2023-12-27T19:14:44.361236500Z"
        },
        "id": "cd072867024c79d1",
        "outputId": "622341ca-d14b-47df-e7ee-424496798f2b"
      },
      "id": "cd072867024c79d1"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": "array([[   2, 4006, 2358,  687, 1192, 2365,    4,    3]], dtype=int64)"
          },
          "execution_count": 32,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokens = reloaded_tokenizers.en.tokenize(['Hello TensorFlow!'])\n",
        "tokens.numpy()"
      ],
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-12-27T19:14:46.031815300Z",
          "start_time": "2023-12-27T19:14:45.654537800Z"
        },
        "id": "fb415f89f8f31517",
        "outputId": "3c6b9891-1ef8-46a2-d4d2-5371c89480d7"
      },
      "id": "fb415f89f8f31517"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": "<tf.RaggedTensor [[b'[START]', b'hello', b'tens', b'##or', b'##f', b'##low', b'!',\n  b'[END]']]>"
          },
          "execution_count": 33,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "text_tokens = reloaded_tokenizers.en.lookup(tokens)\n",
        "text_tokens"
      ],
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-12-27T19:14:46.061347500Z",
          "start_time": "2023-12-27T19:14:46.028320700Z"
        },
        "id": "cd93524e9097dbd0",
        "outputId": "f482c983-ce80-4cc0-e038-4142ca0bde0c"
      },
      "id": "cd93524e9097dbd0"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "hello tensorflow !\n"
          ]
        }
      ],
      "source": [
        "round_trip = reloaded_tokenizers.en.detokenize(tokens)\n",
        "\n",
        "print(round_trip.numpy()[0].decode('utf-8'))"
      ],
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-12-27T19:14:46.225250900Z",
          "start_time": "2023-12-27T19:14:46.057822700Z"
        },
        "id": "25a0339b20d4d36",
        "outputId": "47795d1c-7bc9-413f-951d-2a7fd0dd6e5e"
      },
      "id": "25a0339b20d4d36"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step n - Still to do\n",
        "\n",
        "- better understand\n",
        "    - the algorithm alternatives (in tutorial)\n",
        "- implement tf:lookup - build lookup table and pass to tokenizer\n",
        "- enable GPU (local is completed - Rosie still to do, should just be config)  "
      ],
      "metadata": {
        "collapsed": false,
        "id": "93411b59dfbedb7"
      },
      "id": "93411b59dfbedb7"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.6"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}